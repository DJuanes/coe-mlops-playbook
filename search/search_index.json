{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLOps Playbook Dise\u00f1o Producto Ingenier\u00eda Proyecto Datos Etiquetado Exploraci\u00f3n Preprocesamiento Splitting Aumento Modelado L\u00edneas base Evaluaci\u00f3n Seguimiento de experimentos Optimizaci\u00f3n Desarrollo Empaquetado Organizaci\u00f3n Logging Documentaci\u00f3n Estilismo Makefile Inferencia L\u00ednea de comando API RESTful Pruebas C\u00f3digo Datos Modelos Reproducibilidad Git Pre-commit Versionado Docker Producci\u00f3n Dashboard Monitoreo Dise\u00f1o de sistemas Ingenier\u00eda de datos Data stack Orquestaci\u00f3n Feature store","title":"Home"},{"location":"#mlops-playbook","text":"Dise\u00f1o Producto Ingenier\u00eda Proyecto Datos Etiquetado Exploraci\u00f3n Preprocesamiento Splitting Aumento Modelado L\u00edneas base Evaluaci\u00f3n Seguimiento de experimentos Optimizaci\u00f3n Desarrollo Empaquetado Organizaci\u00f3n Logging Documentaci\u00f3n Estilismo Makefile Inferencia L\u00ednea de comando API RESTful Pruebas C\u00f3digo Datos Modelos Reproducibilidad Git Pre-commit Versionado Docker Producci\u00f3n Dashboard Monitoreo Dise\u00f1o de sistemas Ingenier\u00eda de datos Data stack Orquestaci\u00f3n Feature store","title":"MLOps Playbook"},{"location":"Datos/Aumento/","text":"","title":"Aumento"},{"location":"Datos/Etiquetado/","text":"","title":"Etiquetado"},{"location":"Datos/Exploraci%C3%B3n/","text":"","title":"Exploraci\u00f3n"},{"location":"Datos/Preprocesamiento/","text":"","title":"Preprocesamiento"},{"location":"Datos/Splitting/","text":"","title":"Splitting"},{"location":"Desarrollo/Documentaci%C3%B3n/","text":"Documentaci\u00f3n de c\u00f3digo Documentaci\u00f3n de c\u00f3digo para su equipo y su yo futuro. Introducci\u00f3n Podemos organizar a\u00fan m\u00e1s nuestro c\u00f3digo al documentarlo para que sea m\u00e1s f\u00e1cil para otros (y para nosotros mismos en el futuro) navegarlo y extenderlo f\u00e1cilmente. La documentaci\u00f3n puede significar muchas cosas diferentes para los desarrolladores, as\u00ed que definamos los componentes m\u00e1s comunes: comentarios: breves descripciones de por qu\u00e9 existe un fragmento de c\u00f3digo. tipificaci\u00f3n: especificaci\u00f3n de los tipos de datos de entrada y salida de una funci\u00f3n, proporcionando informaci\u00f3n relacionada con lo que una funci\u00f3n consume y produce. docstrings: descripciones significativas para funciones y clases que describen la utilidad general, argumentos, retornos, etc. docs: p\u00e1gina web renderizada que resume todas las funciones, clases, flujos de trabajo, ejemplos, etc. En la secci\u00f3n de CI/CD aprenderemos c\u00f3mo crear autom\u00e1ticamente y mantener nuestros documentos actualizados cada vez que hagamos cambios en nuestra base de c\u00f3digo. Tipificaci\u00f3n Es importante ser lo m\u00e1s expl\u00edcito posible con nuestro c\u00f3digo. Adem\u00e1s de la elecci\u00f3n de nombres expl\u00edcitos para variables, funciones, etc., otra forma en que podemos ser expl\u00edcitos es definiendo los tipos para las entradas y salidas de nuestra funci\u00f3n. En lugar de: def some_function ( a , b ): return c Podemos hacerlo m\u00e1s legible: from typing import List def some_function ( a : List , b : int = 0 ) -> np . ndarray : return c A partir de Python 3.9+, los tipos comunes est\u00e1n integrados, por lo que ya no es necesario importarlos escribiendo import List, Set, Dict, Tuple, Sequence. Docstrings Podemos hacer que nuestro c\u00f3digo sea a\u00fan m\u00e1s expl\u00edcito agregando docstrings para describir la utilidad general, los argumentos, las devoluciones, las excepciones y m\u00e1s. Por ejemplo: from typing import List def some_function ( a : List , b : int = 0 ) -> np . ndarray : \"\"\"Descripci\u00f3n de la funci\u00f3n. ```python c = some_function(a=[], b=0) print (c) ``` <pre> [[1 2] [3 4]] </pre> Args: a (List): descripci\u00f3n de `a`. b (int, optional): descripci\u00f3n de `b`. Defaults to 0. Raises: ValueError: Input list is not one-dimensional. Returns: np.ndarray: Descripci\u00f3n de `c`. \"\"\" return c T\u00f3mese este tiempo para actualizar todas las funciones y clases en el proyecto con docstrings. Tenga en cuenta que debe importar expl\u00edcitamente algunas bibliotecas a ciertos scripts porque el tipo lo requiere. Idealmente, agregar\u00edamos docstrings a nuestras funciones y clases a medida que las desarrollamos, en lugar de hacerlo todo de una vez al final. Si usa Visual Studio Code, aseg\u00farese de usar la extensi\u00f3n Python Docstrings Generator para que pueda generar una plantilla docstring con ctrl+shift+2 . Esta extensi\u00f3n autocompletar\u00e1 partes de docstring usando la informaci\u00f3n de escritura e incluso excepci\u00f3n en su c\u00f3digo. Docs Mediante paquetes de c\u00f3digo abierto podemos recopilar toda la documentaci\u00f3n dentro de nuestro c\u00f3digo y mostrarlo autom\u00e1ticamente como documentaci\u00f3n. Instale los paquetes necesarios: python -m pip install mkdocs == 1 .3.0 mkdocstrings == 0 .18.1 En lugar de agregar directamente estos requisitos a nuestro archivo requirements.txt , vamos a aislarlo de nuestras bibliotecas principales requeridas. Modificaremos nuestro script setup.py para que esto sea posible. Definiremos estos paquetes bajo un objeto docs_packages : # setup.py docs_packages = [ \"mkdocs==1.3.0\" , \"mkdocstrings==0.18.1\" ] y luego agregaremos esto al objeto setup() en el script: # Definir nuestro paquete setup ( ... install_requires =[ required_packages ] , extras_require ={ \"dev\" : docs_packages, \"docs\" : docs_packages, } , ) Ahora podemos instalar este paquete con: python -m pip install -e \".[docs]\" Tambi\u00e9n estamos definiendo una opci\u00f3n de dev que iremos actualizando para que los desarrolladores puedan instalar todos los paquetes necesarios y adicionales en una sola llamada. python -m pip install -e \".[dev]\" Inicialice mkdocs: python -m mkdocs new . Comenzaremos por sobrescribir el archivo index.md predeterminado en nuestro directorio docs con informaci\u00f3n espec\u00edfica de nuestro proyecto: index.md # CoE MLOps Template ## Proyecto Template - [ Workflows ]( coe_template/main.md ): Flujos de trabajo principales. - [ coe_template ]( coe_template/data.md ): documentaci\u00f3n de la funcionalidad. ## Documentaci\u00f3n paso a paso Aprenda a combinar machine learning con la ingenier\u00eda de software para crear aplicaciones de nivel de producci\u00f3n. - Codigo: [ DJuanes/coe-mlops-playbook ]( https://github.com/DJuanes/coe-mlops-playbook ) Luego, crearemos archivos de documentaci\u00f3n para cada script en nuestro directorio coe_template: mkdir docs/coe_template cd docs/coe_template touch main.md utils.md data.md train.md evaluate.md predict.md cd ../../ A continuaci\u00f3n, agregaremos coe_template.<SCRIPT_NAME> a cada archivo en docs/coe_template . Esto llenar\u00e1 el archivo con informaci\u00f3n sobre las funciones y clases (usando sus docstrings) de coe_template/<SCRIPT_NAME>.py gracias al complemento mkdocstrings . # docs/coe_template/data.md ::: coe_template.data Finalmente, agregaremos algunas configuraciones a nuestro archivo mkdocs.yml: # mkdocs.yml site_name : CoE MLOps Playbook site_url : https://coe-mlops.com/ repo_url : https://github.com/DJuanes/coe-mlops-playbook/ nav : - Home : index.md - workflows : - main : coe_template/main.md - coe_template : - data : coe_template/data.md - evaluate : coe_template/evaluate.md - predict : coe_template/predict.md - train : coe_template/train.md - utils : coe_template/utils.md theme : readthedocs plugins : - mkdocstrings watch : - . # recargar documentos para cualquier cambio de archivo Disponibilice la documentaci\u00f3n localmente: python -m mkdocs serve Publicaci\u00f3n Podemos publicar f\u00e1cilmente nuestra documentaci\u00f3n de forma gratuita utilizando p\u00e1ginas de GitHub para repositorios p\u00fablicos, as\u00ed como documentaci\u00f3n privada para repositorios privados. E incluso podemos alojarlo en un dominio personalizado.","title":"Documentaci\u00f3n"},{"location":"Desarrollo/Documentaci%C3%B3n/#documentacion-de-codigo","text":"Documentaci\u00f3n de c\u00f3digo para su equipo y su yo futuro.","title":"Documentaci\u00f3n de c\u00f3digo"},{"location":"Desarrollo/Documentaci%C3%B3n/#introduccion","text":"Podemos organizar a\u00fan m\u00e1s nuestro c\u00f3digo al documentarlo para que sea m\u00e1s f\u00e1cil para otros (y para nosotros mismos en el futuro) navegarlo y extenderlo f\u00e1cilmente. La documentaci\u00f3n puede significar muchas cosas diferentes para los desarrolladores, as\u00ed que definamos los componentes m\u00e1s comunes: comentarios: breves descripciones de por qu\u00e9 existe un fragmento de c\u00f3digo. tipificaci\u00f3n: especificaci\u00f3n de los tipos de datos de entrada y salida de una funci\u00f3n, proporcionando informaci\u00f3n relacionada con lo que una funci\u00f3n consume y produce. docstrings: descripciones significativas para funciones y clases que describen la utilidad general, argumentos, retornos, etc. docs: p\u00e1gina web renderizada que resume todas las funciones, clases, flujos de trabajo, ejemplos, etc. En la secci\u00f3n de CI/CD aprenderemos c\u00f3mo crear autom\u00e1ticamente y mantener nuestros documentos actualizados cada vez que hagamos cambios en nuestra base de c\u00f3digo.","title":"Introducci\u00f3n"},{"location":"Desarrollo/Documentaci%C3%B3n/#tipificacion","text":"Es importante ser lo m\u00e1s expl\u00edcito posible con nuestro c\u00f3digo. Adem\u00e1s de la elecci\u00f3n de nombres expl\u00edcitos para variables, funciones, etc., otra forma en que podemos ser expl\u00edcitos es definiendo los tipos para las entradas y salidas de nuestra funci\u00f3n. En lugar de: def some_function ( a , b ): return c Podemos hacerlo m\u00e1s legible: from typing import List def some_function ( a : List , b : int = 0 ) -> np . ndarray : return c A partir de Python 3.9+, los tipos comunes est\u00e1n integrados, por lo que ya no es necesario importarlos escribiendo import List, Set, Dict, Tuple, Sequence.","title":"Tipificaci\u00f3n"},{"location":"Desarrollo/Documentaci%C3%B3n/#docstrings","text":"Podemos hacer que nuestro c\u00f3digo sea a\u00fan m\u00e1s expl\u00edcito agregando docstrings para describir la utilidad general, los argumentos, las devoluciones, las excepciones y m\u00e1s. Por ejemplo: from typing import List def some_function ( a : List , b : int = 0 ) -> np . ndarray : \"\"\"Descripci\u00f3n de la funci\u00f3n. ```python c = some_function(a=[], b=0) print (c) ``` <pre> [[1 2] [3 4]] </pre> Args: a (List): descripci\u00f3n de `a`. b (int, optional): descripci\u00f3n de `b`. Defaults to 0. Raises: ValueError: Input list is not one-dimensional. Returns: np.ndarray: Descripci\u00f3n de `c`. \"\"\" return c T\u00f3mese este tiempo para actualizar todas las funciones y clases en el proyecto con docstrings. Tenga en cuenta que debe importar expl\u00edcitamente algunas bibliotecas a ciertos scripts porque el tipo lo requiere. Idealmente, agregar\u00edamos docstrings a nuestras funciones y clases a medida que las desarrollamos, en lugar de hacerlo todo de una vez al final. Si usa Visual Studio Code, aseg\u00farese de usar la extensi\u00f3n Python Docstrings Generator para que pueda generar una plantilla docstring con ctrl+shift+2 . Esta extensi\u00f3n autocompletar\u00e1 partes de docstring usando la informaci\u00f3n de escritura e incluso excepci\u00f3n en su c\u00f3digo.","title":"Docstrings"},{"location":"Desarrollo/Documentaci%C3%B3n/#docs","text":"Mediante paquetes de c\u00f3digo abierto podemos recopilar toda la documentaci\u00f3n dentro de nuestro c\u00f3digo y mostrarlo autom\u00e1ticamente como documentaci\u00f3n. Instale los paquetes necesarios: python -m pip install mkdocs == 1 .3.0 mkdocstrings == 0 .18.1 En lugar de agregar directamente estos requisitos a nuestro archivo requirements.txt , vamos a aislarlo de nuestras bibliotecas principales requeridas. Modificaremos nuestro script setup.py para que esto sea posible. Definiremos estos paquetes bajo un objeto docs_packages : # setup.py docs_packages = [ \"mkdocs==1.3.0\" , \"mkdocstrings==0.18.1\" ] y luego agregaremos esto al objeto setup() en el script: # Definir nuestro paquete setup ( ... install_requires =[ required_packages ] , extras_require ={ \"dev\" : docs_packages, \"docs\" : docs_packages, } , ) Ahora podemos instalar este paquete con: python -m pip install -e \".[docs]\" Tambi\u00e9n estamos definiendo una opci\u00f3n de dev que iremos actualizando para que los desarrolladores puedan instalar todos los paquetes necesarios y adicionales en una sola llamada. python -m pip install -e \".[dev]\" Inicialice mkdocs: python -m mkdocs new . Comenzaremos por sobrescribir el archivo index.md predeterminado en nuestro directorio docs con informaci\u00f3n espec\u00edfica de nuestro proyecto: index.md # CoE MLOps Template ## Proyecto Template - [ Workflows ]( coe_template/main.md ): Flujos de trabajo principales. - [ coe_template ]( coe_template/data.md ): documentaci\u00f3n de la funcionalidad. ## Documentaci\u00f3n paso a paso Aprenda a combinar machine learning con la ingenier\u00eda de software para crear aplicaciones de nivel de producci\u00f3n. - Codigo: [ DJuanes/coe-mlops-playbook ]( https://github.com/DJuanes/coe-mlops-playbook ) Luego, crearemos archivos de documentaci\u00f3n para cada script en nuestro directorio coe_template: mkdir docs/coe_template cd docs/coe_template touch main.md utils.md data.md train.md evaluate.md predict.md cd ../../ A continuaci\u00f3n, agregaremos coe_template.<SCRIPT_NAME> a cada archivo en docs/coe_template . Esto llenar\u00e1 el archivo con informaci\u00f3n sobre las funciones y clases (usando sus docstrings) de coe_template/<SCRIPT_NAME>.py gracias al complemento mkdocstrings . # docs/coe_template/data.md ::: coe_template.data Finalmente, agregaremos algunas configuraciones a nuestro archivo mkdocs.yml: # mkdocs.yml site_name : CoE MLOps Playbook site_url : https://coe-mlops.com/ repo_url : https://github.com/DJuanes/coe-mlops-playbook/ nav : - Home : index.md - workflows : - main : coe_template/main.md - coe_template : - data : coe_template/data.md - evaluate : coe_template/evaluate.md - predict : coe_template/predict.md - train : coe_template/train.md - utils : coe_template/utils.md theme : readthedocs plugins : - mkdocstrings watch : - . # recargar documentos para cualquier cambio de archivo Disponibilice la documentaci\u00f3n localmente: python -m mkdocs serve","title":"Docs"},{"location":"Desarrollo/Documentaci%C3%B3n/#publicacion","text":"Podemos publicar f\u00e1cilmente nuestra documentaci\u00f3n de forma gratuita utilizando p\u00e1ginas de GitHub para repositorios p\u00fablicos, as\u00ed como documentaci\u00f3n privada para repositorios privados. E incluso podemos alojarlo en un dominio personalizado.","title":"Publicaci\u00f3n"},{"location":"Desarrollo/Empaquetado/","text":"Empaquetado de c\u00f3digo Python Uso de configuraciones y entornos virtuales para crear un escenario de reproducci\u00f3n de resultados. Introducci\u00f3n Vamos a definir expl\u00edcitamente nuestro entorno para que podamos reproducirlo localmente y cuando lo implementemos en producci\u00f3n. Hay muchas herramientas de gesti\u00f3n de dependencias y empaquetado, como Poetry, pero usaremos Pip, ya que es ampliamente adoptado. Terminal Antes de que podamos comenzar a empaquetar, necesitamos una forma de crear archivos y ejecutar comandos. Esto lo podemos hacer a trav\u00e9s de la terminal. Todos los comandos que ejecutamos deben ser los mismos independientemente de su sistema operativo o lenguaje de programaci\u00f3n de interfaz de l\u00ednea de comandos (CLI). Se recomienda utilizar iTerm2 (Mac) o ConEmu (Windows) en lugar del terminal predeterminado por sus ricas funciones. Proyecto Crearemos el directorio principal del proyecto para que podamos guardar all\u00ed nuestros componentes de empaquetado. # Crear y cambiar al directorio mkdir mlops cd mlops Python Lo primero que haremos ser\u00e1 configurar la versi\u00f3n correcta de Python. Usaremos la versi\u00f3n 3.9.13. Es recomendable utilizar un administrador de versiones como pyenv . Pyenv funciona para Mac y Linux, pero si est\u00e1 en Windows, debe usar pyenv-win . Virtual environment A continuaci\u00f3n, configuraremos un entorno virtual para poder aislar los paquetes necesarios para nuestra aplicaci\u00f3n. Esto tambi\u00e9n mantendr\u00e1 los componentes separados de otros proyectos que pueden tener diferentes dependencias. Una vez que creamos nuestro entorno virtual, lo activaremos e instalaremos nuestros paquetes requeridos. python3 -m venv venv source ./venv/scripts/activate python -m pip install --upgrade pip setuptools wheel Sabremos que nuestro entorno virtual est\u00e1 activo por su nombre en la terminal. Requirements Crearemos un archivo separado llamado requirements.txt donde especificaremos los paquetes (con sus versiones) que queremos instalar. touch requirements.txt No es aconsejable instalar todos los paquetes y luego hacer pip freeze > requirements.txt porque vuelca las dependencias de todos nuestros paquetes en el archivo. Para mitigar esto, existen herramientas como pipreqs , pip-tools , pipchill , etc. que solo enumeran los paquetes que no son dependencias. # requirements.txt <PACKAGE> == <VERSION> # version exacta <PACKAGE>> = <VERSION> # por encima de version <PACKAGE> # sin version Setup Ahora crearemos un archivo llamado setup.py para proporcionar instrucciones sobre c\u00f3mo configurar nuestro entorno virtual. touch setup.py # setup.py from pathlib import Path from setuptools import find_namespace_packages , setup Comenzaremos extrayendo los paquetes requeridos de requirements.txt : # Cargar paquetes desde requirements.txt BASE_DIR = Path ( __file__ ) . parent with open ( Path ( BASE_DIR , \"requirements.txt\" ), \"r\" ) as file : required_packages = [ ln . strip () for ln in file . readlines ()] El coraz\u00f3n del archivo setup.py es el objeto de instalaci\u00f3n que describe c\u00f3mo configurar nuestro paquete y sus dependencias. Nuestro paquete se llamar\u00e1 coe_template y abarcar\u00e1 todos los requisitos necesarios para ejecutarlo. # setup.py setup ( name = \"coe_template\" , version = 0.1 , description = \"Clasificaci\u00f3n de proyectos de machine learning.\" , author = \"Diego Juanes\" , author_email = \"diego.juanes@ypf.com\" , python_requires = \">=3.9\" , install_requires = [ required_packages ], ) Uso Podemos instalar nuestros paquetes as\u00ed: python -m pip install -e . # instala solo los paquetes requeridos","title":"Empaquetado"},{"location":"Desarrollo/Empaquetado/#empaquetado-de-codigo-python","text":"Uso de configuraciones y entornos virtuales para crear un escenario de reproducci\u00f3n de resultados.","title":"Empaquetado de c\u00f3digo Python"},{"location":"Desarrollo/Empaquetado/#introduccion","text":"Vamos a definir expl\u00edcitamente nuestro entorno para que podamos reproducirlo localmente y cuando lo implementemos en producci\u00f3n. Hay muchas herramientas de gesti\u00f3n de dependencias y empaquetado, como Poetry, pero usaremos Pip, ya que es ampliamente adoptado.","title":"Introducci\u00f3n"},{"location":"Desarrollo/Empaquetado/#terminal","text":"Antes de que podamos comenzar a empaquetar, necesitamos una forma de crear archivos y ejecutar comandos. Esto lo podemos hacer a trav\u00e9s de la terminal. Todos los comandos que ejecutamos deben ser los mismos independientemente de su sistema operativo o lenguaje de programaci\u00f3n de interfaz de l\u00ednea de comandos (CLI). Se recomienda utilizar iTerm2 (Mac) o ConEmu (Windows) en lugar del terminal predeterminado por sus ricas funciones.","title":"Terminal"},{"location":"Desarrollo/Empaquetado/#proyecto","text":"Crearemos el directorio principal del proyecto para que podamos guardar all\u00ed nuestros componentes de empaquetado. # Crear y cambiar al directorio mkdir mlops cd mlops","title":"Proyecto"},{"location":"Desarrollo/Empaquetado/#python","text":"Lo primero que haremos ser\u00e1 configurar la versi\u00f3n correcta de Python. Usaremos la versi\u00f3n 3.9.13. Es recomendable utilizar un administrador de versiones como pyenv . Pyenv funciona para Mac y Linux, pero si est\u00e1 en Windows, debe usar pyenv-win .","title":"Python"},{"location":"Desarrollo/Empaquetado/#virtual-environment","text":"A continuaci\u00f3n, configuraremos un entorno virtual para poder aislar los paquetes necesarios para nuestra aplicaci\u00f3n. Esto tambi\u00e9n mantendr\u00e1 los componentes separados de otros proyectos que pueden tener diferentes dependencias. Una vez que creamos nuestro entorno virtual, lo activaremos e instalaremos nuestros paquetes requeridos. python3 -m venv venv source ./venv/scripts/activate python -m pip install --upgrade pip setuptools wheel Sabremos que nuestro entorno virtual est\u00e1 activo por su nombre en la terminal.","title":"Virtual environment"},{"location":"Desarrollo/Empaquetado/#requirements","text":"Crearemos un archivo separado llamado requirements.txt donde especificaremos los paquetes (con sus versiones) que queremos instalar. touch requirements.txt No es aconsejable instalar todos los paquetes y luego hacer pip freeze > requirements.txt porque vuelca las dependencias de todos nuestros paquetes en el archivo. Para mitigar esto, existen herramientas como pipreqs , pip-tools , pipchill , etc. que solo enumeran los paquetes que no son dependencias. # requirements.txt <PACKAGE> == <VERSION> # version exacta <PACKAGE>> = <VERSION> # por encima de version <PACKAGE> # sin version","title":"Requirements"},{"location":"Desarrollo/Empaquetado/#setup","text":"Ahora crearemos un archivo llamado setup.py para proporcionar instrucciones sobre c\u00f3mo configurar nuestro entorno virtual. touch setup.py # setup.py from pathlib import Path from setuptools import find_namespace_packages , setup Comenzaremos extrayendo los paquetes requeridos de requirements.txt : # Cargar paquetes desde requirements.txt BASE_DIR = Path ( __file__ ) . parent with open ( Path ( BASE_DIR , \"requirements.txt\" ), \"r\" ) as file : required_packages = [ ln . strip () for ln in file . readlines ()] El coraz\u00f3n del archivo setup.py es el objeto de instalaci\u00f3n que describe c\u00f3mo configurar nuestro paquete y sus dependencias. Nuestro paquete se llamar\u00e1 coe_template y abarcar\u00e1 todos los requisitos necesarios para ejecutarlo. # setup.py setup ( name = \"coe_template\" , version = 0.1 , description = \"Clasificaci\u00f3n de proyectos de machine learning.\" , author = \"Diego Juanes\" , author_email = \"diego.juanes@ypf.com\" , python_requires = \">=3.9\" , install_requires = [ required_packages ], )","title":"Setup"},{"location":"Desarrollo/Empaquetado/#uso","text":"Podemos instalar nuestros paquetes as\u00ed: python -m pip install -e . # instala solo los paquetes requeridos","title":"Uso"},{"location":"Desarrollo/Estilismo/","text":"Estilo y formato de c\u00f3digo Convenciones de estilo y formato para mantener la coherencia de nuestro c\u00f3digo. Introducci\u00f3n Cuando escribimos un fragmento de c\u00f3digo, casi nunca es la \u00faltima vez que lo vemos o la \u00faltima vez que se edita. As\u00ed que necesitamos explicar lo que est\u00e1 pasando (a trav\u00e9s de la documentaci\u00f3n) y hacer que sea f\u00e1cil de leer. Una de las maneras m\u00e1s f\u00e1ciles de hacer que el c\u00f3digo sea m\u00e1s legible es seguir convenciones de estilo y formato consistentes. Hay muchas opciones cuando se trata de convenciones de estilo de Python a las que adherirse, pero la mayor\u00eda se basan en convenciones de PEP8. Los aspectos m\u00e1s importantes son: consistencia: todos siguen los mismos est\u00e1ndares. automatizaci\u00f3n: el formateo deber\u00eda ser en gran medida sin esfuerzo despu\u00e9s de la configuraci\u00f3n inicial. Herramientas Usaremos una combinaci\u00f3n muy popular de convenciones de estilo y formato. Black: un reformateador que se adhiere a PEP8. isort: ordena y da formato a las sentencias import. flake8: un linter de c\u00f3digo con convenciones estil\u00edsticas que se adhieren a PEP8. Instale los paquetes necesarios: python -m pip install black == 22 .3.0 flake8 == 3 .9.2 isort == 5 .10.1 Dado que estos paquetes de estilo no son parte integral de las operaciones principales de aprendizaje autom\u00e1tico, creemos una lista separada en nuestro setup.py : # setup.py style_packages = [ \"black==22.3.0\" , \"flake8==3.9.2\" , \"isort==5.10.1\" ] # Definir nuestro paquete setup ( ... extras_require ={ \"dev\" : docs_packages + style_packages, \"docs\" : docs_packages, } , ) Configuraci\u00f3n Antes de que podamos usar correctamente estas herramientas, tendremos que configurarlas. Black Para configurar Black es m\u00e1s eficiente hacerlo a trav\u00e9s de un archivo. As\u00ed que crearemos un pyproject.toml en la ra\u00edz de nuestro directorio de proyectos con lo siguiente: touch pyproject.toml # Black formatting [tool.black] line-length = 100 include = '\\.pyi?$' exclude = ''' /( .eggs # excluir algunos directorios comunes en el | .git # ra\u00edz del proyecto | .hg | .mypy_cache | .tox | venv | _build | buck-out | build | dist )/ ''' isort A continuaci\u00f3n, vamos a configurar isort en nuestro archivo pyproject.toml : # isort [tool.isort] profile = \"black\" line_length = 100 multi_line_output = 3 include_trailing_comma = true virtual_env = \"venv\" flake8 Por \u00faltimo, configuraremos flake8, pero esta vez necesitamos crear un archivo .flake8 separado para definir sus configuraciones: touch .flake8 [flake8] exclude = venv ignore = W503, E226 max-line-length = 100 # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Adem\u00e1s de definir opciones de configuraci\u00f3n aqu\u00ed, que se aplican globalmente, tambi\u00e9n podemos optar por ignorar espec\u00edficamente ciertas convenciones l\u00ednea por l\u00ednea. Aqu\u00ed hay un ejemplo de c\u00f3mo utilizamos esto: # coe_template/config.py import pretty_errors # NOQA: F401 (imported but unused) Uso Para usar estas herramientas que hemos configurado, tenemos que ejecutarlas desde el directorio del proyecto: black coe_template flake8 coe_template isort coe_template En la secci\u00f3n de makefile, aprenderemos c\u00f3mo combinar todos estos comandos en uno. Y en la secci\u00f3n pre-commit, aprenderemos c\u00f3mo ejecutar autom\u00e1ticamente este formato siempre que hagamos cambios en nuestro c\u00f3digo.","title":"Estilismo"},{"location":"Desarrollo/Estilismo/#estilo-y-formato-de-codigo","text":"Convenciones de estilo y formato para mantener la coherencia de nuestro c\u00f3digo.","title":"Estilo y formato de c\u00f3digo"},{"location":"Desarrollo/Estilismo/#introduccion","text":"Cuando escribimos un fragmento de c\u00f3digo, casi nunca es la \u00faltima vez que lo vemos o la \u00faltima vez que se edita. As\u00ed que necesitamos explicar lo que est\u00e1 pasando (a trav\u00e9s de la documentaci\u00f3n) y hacer que sea f\u00e1cil de leer. Una de las maneras m\u00e1s f\u00e1ciles de hacer que el c\u00f3digo sea m\u00e1s legible es seguir convenciones de estilo y formato consistentes. Hay muchas opciones cuando se trata de convenciones de estilo de Python a las que adherirse, pero la mayor\u00eda se basan en convenciones de PEP8. Los aspectos m\u00e1s importantes son: consistencia: todos siguen los mismos est\u00e1ndares. automatizaci\u00f3n: el formateo deber\u00eda ser en gran medida sin esfuerzo despu\u00e9s de la configuraci\u00f3n inicial.","title":"Introducci\u00f3n"},{"location":"Desarrollo/Estilismo/#herramientas","text":"Usaremos una combinaci\u00f3n muy popular de convenciones de estilo y formato. Black: un reformateador que se adhiere a PEP8. isort: ordena y da formato a las sentencias import. flake8: un linter de c\u00f3digo con convenciones estil\u00edsticas que se adhieren a PEP8. Instale los paquetes necesarios: python -m pip install black == 22 .3.0 flake8 == 3 .9.2 isort == 5 .10.1 Dado que estos paquetes de estilo no son parte integral de las operaciones principales de aprendizaje autom\u00e1tico, creemos una lista separada en nuestro setup.py : # setup.py style_packages = [ \"black==22.3.0\" , \"flake8==3.9.2\" , \"isort==5.10.1\" ] # Definir nuestro paquete setup ( ... extras_require ={ \"dev\" : docs_packages + style_packages, \"docs\" : docs_packages, } , )","title":"Herramientas"},{"location":"Desarrollo/Estilismo/#configuracion","text":"Antes de que podamos usar correctamente estas herramientas, tendremos que configurarlas.","title":"Configuraci\u00f3n"},{"location":"Desarrollo/Estilismo/#black","text":"Para configurar Black es m\u00e1s eficiente hacerlo a trav\u00e9s de un archivo. As\u00ed que crearemos un pyproject.toml en la ra\u00edz de nuestro directorio de proyectos con lo siguiente: touch pyproject.toml # Black formatting [tool.black] line-length = 100 include = '\\.pyi?$' exclude = ''' /( .eggs # excluir algunos directorios comunes en el | .git # ra\u00edz del proyecto | .hg | .mypy_cache | .tox | venv | _build | buck-out | build | dist )/ '''","title":"Black"},{"location":"Desarrollo/Estilismo/#isort","text":"A continuaci\u00f3n, vamos a configurar isort en nuestro archivo pyproject.toml : # isort [tool.isort] profile = \"black\" line_length = 100 multi_line_output = 3 include_trailing_comma = true virtual_env = \"venv\"","title":"isort"},{"location":"Desarrollo/Estilismo/#flake8","text":"Por \u00faltimo, configuraremos flake8, pero esta vez necesitamos crear un archivo .flake8 separado para definir sus configuraciones: touch .flake8 [flake8] exclude = venv ignore = W503, E226 max-line-length = 100 # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Adem\u00e1s de definir opciones de configuraci\u00f3n aqu\u00ed, que se aplican globalmente, tambi\u00e9n podemos optar por ignorar espec\u00edficamente ciertas convenciones l\u00ednea por l\u00ednea. Aqu\u00ed hay un ejemplo de c\u00f3mo utilizamos esto: # coe_template/config.py import pretty_errors # NOQA: F401 (imported but unused)","title":"flake8"},{"location":"Desarrollo/Estilismo/#uso","text":"Para usar estas herramientas que hemos configurado, tenemos que ejecutarlas desde el directorio del proyecto: black coe_template flake8 coe_template isort coe_template En la secci\u00f3n de makefile, aprenderemos c\u00f3mo combinar todos estos comandos en uno. Y en la secci\u00f3n pre-commit, aprenderemos c\u00f3mo ejecutar autom\u00e1ticamente este formato siempre que hagamos cambios en nuestro c\u00f3digo.","title":"Uso"},{"location":"Desarrollo/Logging/","text":"Logging para sistemas ML Mantenga registros de los eventos importantes en nuestra aplicaci\u00f3n. Introducci\u00f3n Logging es el proceso de rastrear y registrar eventos clave que ocurren en nuestras aplicaciones con el prop\u00f3sito de inspecci\u00f3n, depuraci\u00f3n, etc. Son mucho m\u00e1s poderosos que las sentencias print porque nos permiten enviar informaci\u00f3n espec\u00edfica a ubicaciones espec\u00edficas con formato personalizado, interfaces compartidas, etc. Esto hace que el logueo sea un defensor clave para poder obtener informaci\u00f3n detallada de los procesos internos de nuestra aplicaci\u00f3n. Componentes Hay algunos conceptos generales a tener en cuenta: Logger : emite los mensajes de log de nuestra aplicaci\u00f3n. Handler : env\u00eda registros de log a una ubicaci\u00f3n espec\u00edfica. Formatter : da formato y estilo a los registros. Niveles Antes de crear nuestro logger, veamos c\u00f3mo se ven los mensajes de log usando la configuraci\u00f3n b\u00e1sica. import logging import sys # Crear logger super basico logging . basicConfig ( stream = sys . stdout , level = logging . DEBUG ) # Niveles de logeo (de menor a mayor prioridad) logging . debug ( \"Se utiliza para depurar su c\u00f3digo.\" ) logging . info ( \"Mensajes informativos de tu c\u00f3digo.\" ) logging . warning ( \"Todo funciona, pero hay algo a tener en cuenta.\" ) logging . error ( \"Ha habido un error con el proceso.\" ) logging . critical ( \"Hay algo terriblemente mal y el proceso puede terminar.\" ) Definimos nuestro logger usando basicConfig para emitir mensajes de log a stdout, pero tambi\u00e9n podr\u00edamos haber escrito en cualquier otro flujo o incluso en un archivo. Tambi\u00e9n definimos nuestro log para que sea sensible a los mensajes a partir del nivel DEBUG, que es el nivel m\u00e1s bajo. Si hubi\u00e9ramos hecho el nivel ERROR , solo se mostrar\u00edan los mensajes de registro ERROR y CR\u00cdTICO . Configuraci\u00f3n Primero estableceremos la ubicaci\u00f3n de nuestros logs en el script config.py : # config/config.py # Directorios BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) # Crear carpetas DATA_DIR . mkdir ( parents = True , exist_ok = True ) MODEL_REGISTRY . mkdir ( parents = True , exist_ok = True ) LOGS_DIR . mkdir ( parents = True , exist_ok = True ) A continuaci\u00f3n, configuraremos el logger para nuestra aplicaci\u00f3n: # config/config.py import logging import sys # Logger logging_config = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"formatters\" : { \"minimal\" : { \"format\" : \" %(message)s \" }, \"detailed\" : { \"format\" : \" %(levelname)s %(asctime)s [ %(name)s : %(filename)s : %(funcName)s : %(lineno)d ] \\n %(message)s \\n \" }, }, \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , \"stream\" : sys . stdout , \"formatter\" : \"minimal\" , \"level\" : logging . DEBUG , }, \"info\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"info.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . INFO , }, \"error\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"error.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . ERROR , }, }, \"root\" : { \"handlers\" : [ \"console\" , \"info\" , \"error\" ], \"level\" : logging . INFO , \"propagate\" : True , }, } Elegimos usar un diccionario para configurar nuestro logger, pero hay otras formas, como script de Python, archivo de configuraci\u00f3n, etc: Script de Python import logging from rich.logging import RichHandler # Obtener logger root logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Crear handlers console_handler = RichHandler ( markup = True ) console_handler . setLevel ( logging . DEBUG ) info_handler = logging . handlers . RotatingFileHandler ( filename = Path ( LOGS_DIR , \"info.log\" ), maxBytes = 10485760 , # 1 MB backupCount = 10 , ) info_handler . setLevel ( logging . INFO ) error_handler = logging . handlers . RotatingFileHandler ( filename = Path ( LOGS_DIR , \"error.log\" ), maxBytes = 10485760 , # 1 MB backupCount = 10 , ) error_handler . setLevel ( logging . ERROR ) # Crear formatters minimal_formatter = logging . Formatter ( fmt = \" %(message)s \" ) detailed_formatter = logging . Formatter ( fmt = \" %(levelname)s %(asctime)s [ %(name)s : %(filename)s : %(funcName)s : %(lineno)d ] \\n %(message)s \\n \" ) # Enganchar todo console_handler . setFormatter ( fmt = minimal_formatter ) info_handler . setFormatter ( fmt = detailed_formatter ) error_handler . setFormatter ( fmt = detailed_formatter ) logger . addHandler ( hdlr = console_handler ) logger . addHandler ( hdlr = info_handler ) logger . addHandler ( hdlr = error_handler ) Archivo de configuraci\u00f3n 1. Coloque esto dentro de un archivo `logging.config`: [ formatters ] keys = minimal , detailed [ formatter_minimal ] format =% ( message ) s [ formatter_detailed ] format = % ( levelname ) s % ( asctime ) s [ % ( name ) s : % ( filename ) s : % ( funcName ) s : % ( lineno ) d ] % ( message ) s [ handlers ] keys = console , info , error [ handler_console ] class = StreamHandler level = DEBUG formatter = minimal args = ( sys . stdout ,) [ handler_info ] class = handlers . RotatingFileHandler level = INFO formatter = detailed backupCount = 10 maxBytes = 10485760 args = ( \"logs/info.log\" ,) [ handler_error ] class = handlers . RotatingFileHandler level = ERROR formatter = detailed backupCount = 10 maxBytes = 10485760 args = ( \"logs/error.log\" ,) [ loggers ] keys = root [ logger_root ] level = INFO handlers = console , info , error 2. Coloque esto dentro de su script de Python: import logging import logging.config from rich.logging import RichHandler # Usar el archivo de configuraci\u00f3n para inicializar el logger logging . config . fileConfig ( Path ( CONFIG_DIR , \"logging.config\" )) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True ) # setear rich handler Podemos cargar nuestro diccionario de configuraci\u00f3n del log as\u00ed: # config/config.py from rich.logging import RichHandler logging . config . dictConfig ( logging_config ) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True ) # pretty formatting # Mensajes de muestra (tenga en cuenta que ahora usamos `logger` configurado) logging . debug ( \"Se utiliza para depurar su c\u00f3digo.\" ) logging . info ( \"Mensajes informativos de tu c\u00f3digo.\" ) logging . warning ( \"Todo funciona, pero hay algo a tener en cuenta.\" ) logging . error ( \"Ha habido un error con el proceso.\" ) logging . critical ( \"Hay algo terriblemente mal y el proceso puede terminar.\" ) Necesitaremos instalar la librer\u00eda RichHandler y agregarla a requirements.txt : python -m pip install rich == 12 .4.4 # Agregar a requirements.txt rich == 12 .4.4 Aplicaci\u00f3n En nuestro proyecto, podemos reemplazar todas nuestras sentencias print en sentencias de logging: print ( \"\u2705 Datos guardados!\" ) se convierte en: from config.config import logger logger . info ( \"\u2705 Datos guardados!\" ) qu\u00e9: registre todos los detalles necesarios que desea que surjan de nuestra aplicaci\u00f3n que ser\u00e1n \u00fatiles durante el desarrollo y luego para la inspecci\u00f3n retrospectiva. donde: una mejor pr\u00e1ctica es no saturar nuestras funciones modulares con sentencias de log. En su lugar, deber\u00edamos registrar mensajes fuera de funciones peque\u00f1as y dentro de flujos de trabajo m\u00e1s grandes. Por ejemplo, no hay mensajes de log dentro de ninguno de nuestros scripts, excepto los archivos main.py y train.py .","title":"Logging"},{"location":"Desarrollo/Logging/#logging-para-sistemas-ml","text":"Mantenga registros de los eventos importantes en nuestra aplicaci\u00f3n.","title":"Logging para sistemas ML"},{"location":"Desarrollo/Logging/#introduccion","text":"Logging es el proceso de rastrear y registrar eventos clave que ocurren en nuestras aplicaciones con el prop\u00f3sito de inspecci\u00f3n, depuraci\u00f3n, etc. Son mucho m\u00e1s poderosos que las sentencias print porque nos permiten enviar informaci\u00f3n espec\u00edfica a ubicaciones espec\u00edficas con formato personalizado, interfaces compartidas, etc. Esto hace que el logueo sea un defensor clave para poder obtener informaci\u00f3n detallada de los procesos internos de nuestra aplicaci\u00f3n.","title":"Introducci\u00f3n"},{"location":"Desarrollo/Logging/#componentes","text":"Hay algunos conceptos generales a tener en cuenta: Logger : emite los mensajes de log de nuestra aplicaci\u00f3n. Handler : env\u00eda registros de log a una ubicaci\u00f3n espec\u00edfica. Formatter : da formato y estilo a los registros.","title":"Componentes"},{"location":"Desarrollo/Logging/#niveles","text":"Antes de crear nuestro logger, veamos c\u00f3mo se ven los mensajes de log usando la configuraci\u00f3n b\u00e1sica. import logging import sys # Crear logger super basico logging . basicConfig ( stream = sys . stdout , level = logging . DEBUG ) # Niveles de logeo (de menor a mayor prioridad) logging . debug ( \"Se utiliza para depurar su c\u00f3digo.\" ) logging . info ( \"Mensajes informativos de tu c\u00f3digo.\" ) logging . warning ( \"Todo funciona, pero hay algo a tener en cuenta.\" ) logging . error ( \"Ha habido un error con el proceso.\" ) logging . critical ( \"Hay algo terriblemente mal y el proceso puede terminar.\" ) Definimos nuestro logger usando basicConfig para emitir mensajes de log a stdout, pero tambi\u00e9n podr\u00edamos haber escrito en cualquier otro flujo o incluso en un archivo. Tambi\u00e9n definimos nuestro log para que sea sensible a los mensajes a partir del nivel DEBUG, que es el nivel m\u00e1s bajo. Si hubi\u00e9ramos hecho el nivel ERROR , solo se mostrar\u00edan los mensajes de registro ERROR y CR\u00cdTICO .","title":"Niveles"},{"location":"Desarrollo/Logging/#configuracion","text":"Primero estableceremos la ubicaci\u00f3n de nuestros logs en el script config.py : # config/config.py # Directorios BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) # Crear carpetas DATA_DIR . mkdir ( parents = True , exist_ok = True ) MODEL_REGISTRY . mkdir ( parents = True , exist_ok = True ) LOGS_DIR . mkdir ( parents = True , exist_ok = True ) A continuaci\u00f3n, configuraremos el logger para nuestra aplicaci\u00f3n: # config/config.py import logging import sys # Logger logging_config = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"formatters\" : { \"minimal\" : { \"format\" : \" %(message)s \" }, \"detailed\" : { \"format\" : \" %(levelname)s %(asctime)s [ %(name)s : %(filename)s : %(funcName)s : %(lineno)d ] \\n %(message)s \\n \" }, }, \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , \"stream\" : sys . stdout , \"formatter\" : \"minimal\" , \"level\" : logging . DEBUG , }, \"info\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"info.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . INFO , }, \"error\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"error.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . ERROR , }, }, \"root\" : { \"handlers\" : [ \"console\" , \"info\" , \"error\" ], \"level\" : logging . INFO , \"propagate\" : True , }, } Elegimos usar un diccionario para configurar nuestro logger, pero hay otras formas, como script de Python, archivo de configuraci\u00f3n, etc: Script de Python import logging from rich.logging import RichHandler # Obtener logger root logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) # Crear handlers console_handler = RichHandler ( markup = True ) console_handler . setLevel ( logging . DEBUG ) info_handler = logging . handlers . RotatingFileHandler ( filename = Path ( LOGS_DIR , \"info.log\" ), maxBytes = 10485760 , # 1 MB backupCount = 10 , ) info_handler . setLevel ( logging . INFO ) error_handler = logging . handlers . RotatingFileHandler ( filename = Path ( LOGS_DIR , \"error.log\" ), maxBytes = 10485760 , # 1 MB backupCount = 10 , ) error_handler . setLevel ( logging . ERROR ) # Crear formatters minimal_formatter = logging . Formatter ( fmt = \" %(message)s \" ) detailed_formatter = logging . Formatter ( fmt = \" %(levelname)s %(asctime)s [ %(name)s : %(filename)s : %(funcName)s : %(lineno)d ] \\n %(message)s \\n \" ) # Enganchar todo console_handler . setFormatter ( fmt = minimal_formatter ) info_handler . setFormatter ( fmt = detailed_formatter ) error_handler . setFormatter ( fmt = detailed_formatter ) logger . addHandler ( hdlr = console_handler ) logger . addHandler ( hdlr = info_handler ) logger . addHandler ( hdlr = error_handler ) Archivo de configuraci\u00f3n 1. Coloque esto dentro de un archivo `logging.config`: [ formatters ] keys = minimal , detailed [ formatter_minimal ] format =% ( message ) s [ formatter_detailed ] format = % ( levelname ) s % ( asctime ) s [ % ( name ) s : % ( filename ) s : % ( funcName ) s : % ( lineno ) d ] % ( message ) s [ handlers ] keys = console , info , error [ handler_console ] class = StreamHandler level = DEBUG formatter = minimal args = ( sys . stdout ,) [ handler_info ] class = handlers . RotatingFileHandler level = INFO formatter = detailed backupCount = 10 maxBytes = 10485760 args = ( \"logs/info.log\" ,) [ handler_error ] class = handlers . RotatingFileHandler level = ERROR formatter = detailed backupCount = 10 maxBytes = 10485760 args = ( \"logs/error.log\" ,) [ loggers ] keys = root [ logger_root ] level = INFO handlers = console , info , error 2. Coloque esto dentro de su script de Python: import logging import logging.config from rich.logging import RichHandler # Usar el archivo de configuraci\u00f3n para inicializar el logger logging . config . fileConfig ( Path ( CONFIG_DIR , \"logging.config\" )) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True ) # setear rich handler Podemos cargar nuestro diccionario de configuraci\u00f3n del log as\u00ed: # config/config.py from rich.logging import RichHandler logging . config . dictConfig ( logging_config ) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True ) # pretty formatting # Mensajes de muestra (tenga en cuenta que ahora usamos `logger` configurado) logging . debug ( \"Se utiliza para depurar su c\u00f3digo.\" ) logging . info ( \"Mensajes informativos de tu c\u00f3digo.\" ) logging . warning ( \"Todo funciona, pero hay algo a tener en cuenta.\" ) logging . error ( \"Ha habido un error con el proceso.\" ) logging . critical ( \"Hay algo terriblemente mal y el proceso puede terminar.\" ) Necesitaremos instalar la librer\u00eda RichHandler y agregarla a requirements.txt : python -m pip install rich == 12 .4.4 # Agregar a requirements.txt rich == 12 .4.4","title":"Configuraci\u00f3n"},{"location":"Desarrollo/Logging/#aplicacion","text":"En nuestro proyecto, podemos reemplazar todas nuestras sentencias print en sentencias de logging: print ( \"\u2705 Datos guardados!\" ) se convierte en: from config.config import logger logger . info ( \"\u2705 Datos guardados!\" ) qu\u00e9: registre todos los detalles necesarios que desea que surjan de nuestra aplicaci\u00f3n que ser\u00e1n \u00fatiles durante el desarrollo y luego para la inspecci\u00f3n retrospectiva. donde: una mejor pr\u00e1ctica es no saturar nuestras funciones modulares con sentencias de log. En su lugar, deber\u00edamos registrar mensajes fuera de funciones peque\u00f1as y dentro de flujos de trabajo m\u00e1s grandes. Por ejemplo, no hay mensajes de log dentro de ninguno de nuestros scripts, excepto los archivos main.py y train.py .","title":"Aplicaci\u00f3n"},{"location":"Desarrollo/Makefile/","text":"Makefiles Una herramienta de automatizaci\u00f3n que organiza comandos para los procesos de nuestra aplicaci\u00f3n. Introducci\u00f3n Para ayudar a organizar todo, vamos a utilizar un Makefile, que es una herramienta de automatizaci\u00f3n que organiza nuestros comandos. Comenzaremos creando este archivo en el directorio ra\u00edz de nuestro proyecto. touch Makefile En la parte superior de nuestro Makefile, debemos especificar el entorno de shell en el que queremos que se ejecuten todos nuestros comandos: # Makefile SHELL = /bin/bash Componentes Dentro de nuestro Makefile, crearemos una lista de reglas. Estas reglas tienen un target que a veces puede tener requisitos previos que deben cumplirse y en la siguiente l\u00ednea una tabulaci\u00f3n seguida de una receta que especifica c\u00f3mo crear el target. Por ejemplo, si quisi\u00e9ramos crear una regla para el estilo, agregar\u00edamos lo siguiente: # Styling .PHONY : style style : black coe_template flake8 coe_template isort coe_template Targets Podemos ejecutar cualquiera de las reglas escribiendo make <target> en la terminal: # Make a target $ make style Si est\u00e1 usando Windows, primero debe instalar make en Windows. Siga los pasos en este link . PHONY Los Makefiles se usan com\u00fanmente como accesos directos de comandos, lo que puede generar confusi\u00f3n cuando un target de Makefile y un archivo comparten el mismo nombre. A veces, nombraremos nuestros targets y querremos que se ejecuten, ya sea que exista como un archivo real o no. En estos escenarios, queremos definir un target PHONY en nuestro archivo MAKE agregando esta l\u00ednea arriba del target: .PHONY : < target_name > La mayor\u00eda de las reglas en nuestro Makefile requerir\u00e1n el target PHONY porque queremos que se ejecuten incluso si hay un archivo que comparte el nombre del target. Prerrequisitos Antes de hacer un target, podemos adjuntarles requisitos previos. Estos pueden ser targets de archivos que deben existir o comandos target PHONY que deben ejecutarse antes de crear este target. Por ejemplo, estableceremos el target de estilo como un requisito previo para el target de limpieza para que todos los archivos tengan el formato adecuado antes de limpiarlos. # Cleaning .PHONY : clean clean : style find . -type f -name \"*.DS_Store\" -ls -delete find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf find . | grep -E \".pytest_cache\" | xargs rm -rf find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf find . | grep -E \".trash\" | xargs rm -rf rm -f .coverage Variables Tambi\u00e9n podemos establecer y usar variables dentro de nuestro Makefile para organizar todas nuestras reglas. Podemos establecer las variables directamente dentro del Makefile. Si la variable no est\u00e1 definida en el Makefile, por defecto ser\u00eda cualquier variable de entorno con el mismo nombre. # Set variable MESSAGE := \"hello world\" # Use variable greeting : @echo ${ MESSAGE } Podemos usar variables pasadas al ejecutar la regla de esta manera: make greeting MESSAGE = \"hi\" Shells Cada l\u00ednea en una receta para una regla se ejecutar\u00e1 en una sub-shell separado. Sin embargo, para ciertas recetas, como activar un entorno virtual y cargar paquetes, queremos ejecutar todos los pasos en un solo shell. Para hacer esto, podemos agregar el target especial .ONESHELL encima de cualquier target. # Environment .ONESHELL : venv : python3 -m venv venv source venv/bin/activate && \\ python -m pip install --upgrade pip setuptools wheel && \\ pip install -e Ayuda Lo \u00faltimo que agregaremos a nuestro Makefile es un target de ayuda en la parte superior. Esta regla proporcionar\u00e1 un mensaje informativo para las capacidades de este Makefile: .PHONY : help help : @echo \"venv : crea un virtual environment.\" @echo \"style : ejecuta el formato de estilo.\" @echo \"clean : limpia todos los archivos innecesarios.\" make help","title":"Makefile"},{"location":"Desarrollo/Makefile/#makefiles","text":"Una herramienta de automatizaci\u00f3n que organiza comandos para los procesos de nuestra aplicaci\u00f3n.","title":"Makefiles"},{"location":"Desarrollo/Makefile/#introduccion","text":"Para ayudar a organizar todo, vamos a utilizar un Makefile, que es una herramienta de automatizaci\u00f3n que organiza nuestros comandos. Comenzaremos creando este archivo en el directorio ra\u00edz de nuestro proyecto. touch Makefile En la parte superior de nuestro Makefile, debemos especificar el entorno de shell en el que queremos que se ejecuten todos nuestros comandos: # Makefile SHELL = /bin/bash","title":"Introducci\u00f3n"},{"location":"Desarrollo/Makefile/#componentes","text":"Dentro de nuestro Makefile, crearemos una lista de reglas. Estas reglas tienen un target que a veces puede tener requisitos previos que deben cumplirse y en la siguiente l\u00ednea una tabulaci\u00f3n seguida de una receta que especifica c\u00f3mo crear el target. Por ejemplo, si quisi\u00e9ramos crear una regla para el estilo, agregar\u00edamos lo siguiente: # Styling .PHONY : style style : black coe_template flake8 coe_template isort coe_template","title":"Componentes"},{"location":"Desarrollo/Makefile/#targets","text":"Podemos ejecutar cualquiera de las reglas escribiendo make <target> en la terminal: # Make a target $ make style Si est\u00e1 usando Windows, primero debe instalar make en Windows. Siga los pasos en este link .","title":"Targets"},{"location":"Desarrollo/Makefile/#phony","text":"Los Makefiles se usan com\u00fanmente como accesos directos de comandos, lo que puede generar confusi\u00f3n cuando un target de Makefile y un archivo comparten el mismo nombre. A veces, nombraremos nuestros targets y querremos que se ejecuten, ya sea que exista como un archivo real o no. En estos escenarios, queremos definir un target PHONY en nuestro archivo MAKE agregando esta l\u00ednea arriba del target: .PHONY : < target_name > La mayor\u00eda de las reglas en nuestro Makefile requerir\u00e1n el target PHONY porque queremos que se ejecuten incluso si hay un archivo que comparte el nombre del target.","title":"PHONY"},{"location":"Desarrollo/Makefile/#prerrequisitos","text":"Antes de hacer un target, podemos adjuntarles requisitos previos. Estos pueden ser targets de archivos que deben existir o comandos target PHONY que deben ejecutarse antes de crear este target. Por ejemplo, estableceremos el target de estilo como un requisito previo para el target de limpieza para que todos los archivos tengan el formato adecuado antes de limpiarlos. # Cleaning .PHONY : clean clean : style find . -type f -name \"*.DS_Store\" -ls -delete find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf find . | grep -E \".pytest_cache\" | xargs rm -rf find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf find . | grep -E \".trash\" | xargs rm -rf rm -f .coverage","title":"Prerrequisitos"},{"location":"Desarrollo/Makefile/#variables","text":"Tambi\u00e9n podemos establecer y usar variables dentro de nuestro Makefile para organizar todas nuestras reglas. Podemos establecer las variables directamente dentro del Makefile. Si la variable no est\u00e1 definida en el Makefile, por defecto ser\u00eda cualquier variable de entorno con el mismo nombre. # Set variable MESSAGE := \"hello world\" # Use variable greeting : @echo ${ MESSAGE } Podemos usar variables pasadas al ejecutar la regla de esta manera: make greeting MESSAGE = \"hi\"","title":"Variables"},{"location":"Desarrollo/Makefile/#shells","text":"Cada l\u00ednea en una receta para una regla se ejecutar\u00e1 en una sub-shell separado. Sin embargo, para ciertas recetas, como activar un entorno virtual y cargar paquetes, queremos ejecutar todos los pasos en un solo shell. Para hacer esto, podemos agregar el target especial .ONESHELL encima de cualquier target. # Environment .ONESHELL : venv : python3 -m venv venv source venv/bin/activate && \\ python -m pip install --upgrade pip setuptools wheel && \\ pip install -e","title":"Shells"},{"location":"Desarrollo/Makefile/#ayuda","text":"Lo \u00faltimo que agregaremos a nuestro Makefile es un target de ayuda en la parte superior. Esta regla proporcionar\u00e1 un mensaje informativo para las capacidades de este Makefile: .PHONY : help help : @echo \"venv : crea un virtual environment.\" @echo \"style : ejecuta el formato de estilo.\" @echo \"clean : limpia todos los archivos innecesarios.\" make help","title":"Ayuda"},{"location":"Desarrollo/Organizaci%C3%B3n/","text":"Organizaci\u00f3n del c\u00f3digo de Machine Learning Organizar el c\u00f3digo al pasar de notebooks a scripts de Python. Introducci\u00f3n Tener un c\u00f3digo organizado es tener un c\u00f3digo legible, reproducible y robusto. Tu equipo y, tambi\u00e9n tu yo futuro, te agradecer\u00e1n por hacer el esfuerzo inicial para organizar el trabajo. Veremos c\u00f3mo migrar y organizar el c\u00f3digo de nuestro notebook a scripts de Python. Editor Hay varias opciones para los editores de c\u00f3digo, como VSCode, Atom, Sublime, PyCharm, Vim, etc. y todos ofrecen funciones \u00fanicas al tiempo que proporcionan las operaciones b\u00e1sicas para la edici\u00f3n y ejecuci\u00f3n de c\u00f3digo. Usaremos VSCode para editar y ejecutar nuestro c\u00f3digo gracias a su simplicidad, soporte multiling\u00fce, complementos y creciente adopci\u00f3n en la industria. En Visual Studio Code abra la paleta de comandos (CTRL + SHIFT + P) y escriba \"Preferences: Open Settings (UI)\" y presione Enter. Ajuste cualquier configuraci\u00f3n relevante que desee (espaciado, tama\u00f1o de fuente, etc.) Instale extensiones de VSCode (utilice el \u00edcono de bloques de lego en el panel izquierdo del editor) Extensiones VSCode recomendadas Recomendamos instalar estas extensiones: code --install-extension alefragnani.project-manager code --install-extension bierner.markdown-preview-github-styles code --install-extension christian-kohler.path-intellisense code --install-extension euskadi31.json-pretty-printer code --install-extension mechatroner.rainbow-csv code --install-extension mikestead.dotenv code --install-extension ms-azuretools.vscode-docker code --install-extension ms-python.python code --install-extension ms-python.vscode-pylance code --install-extension njpwerner.autodocstring code --install-extension PKief.material-icon-theme code --install-extension redhat.vscode-yaml code --install-extension shardulm94.trailing-spaces code --install-extension streetsidesoftware.code-spell-checker-spanish code --install-extension DavidAnson.vscode-markdownlint code --install-extension donjayamanne.python-environment-manager code --install-extension donjayamanne.python-extension-pack code --install-extension equinusocio.vsc-material-theme-icons code --install-extension etmoffat.pip-packages code --install-extension KevinRose.vsc-python-indent code --install-extension magicstack.MagicPython code --install-extension mdickin.markdown-shortcuts code --install-extension ms-azure-devops.azure-pipelines code --install-extension ms-azuretools.vscode-azureappservice code --install-extension ms-azuretools.vscode-azurefunctions code --install-extension ms-azuretools.vscode-azureresourcegroups code --install-extension ms-azuretools.vscode-azurestaticwebapps code --install-extension ms-azuretools.vscode-azurestorage code --install-extension ms-azuretools.vscode-azurevirtualmachines code --install-extension ms-azuretools.vscode-bicep code --install-extension ms-azuretools.vscode-cosmosdb code --install-extension ms-python.isort code --install-extension ms-toolsai.jupyter code --install-extension ms-toolsai.jupyter-keymap code --install-extension ms-toolsai.jupyter-renderers code --install-extension ms-toolsai.vscode-ai code --install-extension ms-toolsai.vscode-ai-remote code --install-extension ms-toolsai.vscode-jupyter-cell-tags code --install-extension ms-toolsai.vscode-jupyter-slideshow code --install-extension ms-vscode-remote.remote-containers code --install-extension ms-vscode-remote.remote-wsl code --install-extension ms-vscode.azure-account code --install-extension ms-vscode.azurecli code --install-extension ms-vscode.powershell code --install-extension ms-vscode.vscode-node-azure-pack code --install-extension msazurermtools.azurerm-vscode-tools code --install-extension VisualStudioExptTeam.vscodeintellicode code --install-extension vscode-icons-team.vscode-icons code --install-extension yzhang.markdown-all-in-one Si agrega sus propias extensiones y desea compartirlas con otros, simplemente ejecute este comando para generar la lista de comandos: code --list-extensions | xargs -L 1 echo code --install-extension Luego de configurar VSCode, podemos comenzar creando nuestro directorio de proyectos, que usaremos para organizar todos nuestros scripts. Hay muchas maneras de iniciar un proyecto, pero esta es nuestra ruta recomendada: Utilice la terminal para crear un directorio ( mkdir <NOMBRE_PROYECTO> ). Cambie al directorio del proyecto que acaba de crear ( cd <PROJECT_NAME> ). Inicie VSCode desde este directorio escribiendo el code . Abra una terminal dentro de VSCode ( View > Terminal ) para continuar creando scripts ( touch <NOMBRE_DE_ARCHIVO> ) o subdirectorios adicionales ( mkdir <SUBDIR> ) seg\u00fan sea necesario. Setup README Comenzaremos nuestra organizaci\u00f3n con un archivo README.md, que proporcionar\u00e1 informaci\u00f3n sobre los archivos en nuestro directorio, instrucciones para ejecutar operaciones, etc. Mantendremos constantemente actualizado este archivo para que podamos catalogar informaci\u00f3n para el futuro. touch README.md Comencemos agregando las instrucciones que usamos para crear un virtual environment: # CoE MLOps Template ## Virtual environment python3 -m venv venv source ./venv/scripts/activate python -m pip install --upgrade pip setuptools wheel pip install -e . Configuraciones A continuaci\u00f3n, crearemos un directorio de configuraci\u00f3n llamado config donde podemos almacenar los componentes que ser\u00e1n necesarios para nuestra aplicaci\u00f3n. Dentro de este directorio, crearemos un config.py y un args.json . mkdir config touch config/config.py config/args.json Dentro de config.py , agregaremos el c\u00f3digo para definir las ubicaciones clave del directorio (agregaremos m\u00e1s configuraciones a medida que sean necesarias): # config.py from pathlib import Path # Directorios BASE_DIR = Path ( __file__ ) .parent.parent.absolute () CONFIG_DIR = Path ( BASE_DIR, \"config\" ) y dentro de args.json , agregaremos los par\u00e1metros que son relevantes para el procesamiento de datos y el entrenamiento del modelo. { \"shuffle\" : true, \"subset\" : null, \"min_freq\" : 75 , \"lower\" : true, \"stem\" : false, \"analyzer\" : \"char\" , \"ngram_max_range\" : 7 , \"alpha\" : 1e-4, \"learning_rate\" : 1e-1, \"power_t\" : 0 .1, \"num_epochs\" : 100 } Operaciones Comenzaremos creando nuestro directorio de paquetes ( src ) dentro de nuestro directorio de proyectos ( mlops ). Dentro de este directorio de paquetes, crearemos un archivo main.py que definir\u00e1 las operaciones principales que queremos tener disponibles. mkdir src touch src/main.py Definiremos estas operaciones centrales dentro de main.py a medida que movemos el c\u00f3digo de los notebooks a los scripts apropiados a continuaci\u00f3n: elt_data : extrae, carga y transforma datos. optimize : ajustar los hiperpar\u00e1metros para optimizar para el objetivo. train_model : entrena un modelo utilizando los mejores par\u00e1metros del estudio de optimizaci\u00f3n. load_artifacts : carga artefactos entrenados de una ejecuci\u00f3n determinada. predict_tag : predecir una etiqueta para una entrada determinada. Utilidades Es com\u00fan tener procesos ad-hoc dentro de los notebooks porque mantiene el estado siempre que el notebook se est\u00e9 ejecutando. Por ejemplo, podemos establecer seeds en nuestros notebooks as\u00ed: # Setear seeds np . random . seed ( seed ) random . seed ( seed ) Pero en nuestros scripts, debemos envolver esta funcionalidad como una funci\u00f3n limpia y reutilizable con los par\u00e1metros apropiados: def set_seeds ( seed = 42 ): \"\"\"Setear seeds para la reproducibilidad.\"\"\" np . random . seed ( seed ) random . seed ( seed ) Podemos almacenar todas estas funciones dentro de un archivo Utils.py en el directorio de paquetes src . touch src/utils.py Ver utils.py import json import numpy as np import random def load_dict ( filepath ): \"\"\"Cargar un diccionario desde la ruta de archivo de un JSON.\"\"\" with open ( filepath , \"r\" ) as fp : d = json . load ( fp ) return d def save_dict ( d , filepath , cls = None , sortkeys = False ): \"\"\"Guardar un diccionario en una ubicaci\u00f3n espec\u00edfica.\"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) def set_seeds ( seed = 42 ): \"\"\"Setear seeds para la reproducibilidad.\"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) Proyecto Cuando se trata de migrar nuestro c\u00f3digo de notebooks a scripts, es mejor organizarse en funci\u00f3n de la utilidad. Por ejemplo, podemos crear scripts para las diversas etapas del desarrollo de ML, como procesamiento de datos, entrenamiento, evaluaci\u00f3n, predicci\u00f3n, etc. Crearemos los diferentes archivos de Python para envolver nuestros datos y funcionalidad ML: cd src touch data.py train.py evaluate.py predict.py Es posible que tengamos scripts adicionales en otros proyectos, ya que son necesarios. Por ejemplo, normalmente tendr\u00edamos un script modelos.py. Organizar nuestra base de c\u00f3digo de esta manera tambi\u00e9n nos hace m\u00e1s f\u00e1cil comprender (o modificar) la base de c\u00f3digo. Podr\u00edamos haber colocado todo el c\u00f3digo en un script main.py, pero a medida que nuestro proyecto crezca, ser\u00e1 dif\u00edcil navegar por un archivo monol\u00edtico. Por otro lado, podr\u00edamos haber asumido una postura m\u00e1s granular descomponiendo datos.py en split.py, preprocess.py, etc. Esto podr\u00eda tener m\u00e1s sentido si tenemos m\u00faltiples formas de dividir, preprocesamiento, etc. Pero en general, es suficiente estar en este nivel m\u00e1s alto de organizaci\u00f3n. Principios A trav\u00e9s del proceso de migraci\u00f3n utilizaremos varios principios b\u00e1sicos de ingenier\u00eda de software repetidamente: Envolver la funcionalidad en funciones \u00bfC\u00f3mo decidimos cu\u00e1ndo deben envolver l\u00edneas de c\u00f3digo espec\u00edficas como una funci\u00f3n separada? Las funciones deben ser at\u00f3micas porque cada una tiene una sola responsabilidad para que podamos probarlas f\u00e1cilmente. Si no, necesitaremos dividirlos en unidades m\u00e1s granulares. Por ejemplo, podr\u00edamos reemplazar las etiquetas en nuestros proyectos con estas l\u00edneas: oos_tags = [ item for item in df . tag . unique () if item not in tags_dict . keys ()] df . tag = df . tag . apply ( lambda x : \"other\" if x in oos_tags else x ) Refactorizado quedar\u00eda as\u00ed: def replace_oos_tags ( df , tags_dict ): \"\"\"Reemplazar etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df . tag . unique () if item not in tags_dict . keys ()] df . tag = df . tag . apply ( lambda x : \"other\" if x in oos_tags else x ) return df Componer funciones generalizadas def replace_oos_tags ( df , tags_dict ): \"\"\"Reemplazar etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df . tag . unique () if item not in tags_dict . keys ()] df . tag = df . tag . apply ( lambda x : \"other\" if x in oos_tags else x ) return df Versus: def replace_oos_labels ( df , labels , label_col , oos_label = \"other\" ): \"\"\"Reemplazar etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df [ label_col ] . unique () if item not in labels ] df [ label_col ] = df [ label_col ] . apply ( lambda x : oos_label if x in oos_tags else x ) return df De esta manera, cuando cambian los nombres de las columnas o queremos reemplazar con diferentes etiquetas, es muy f\u00e1cil ajustar nuestro c\u00f3digo. Esto tambi\u00e9n incluye el uso de nombres generalizados en las funciones como la etiqueta en lugar del nombre de la columna de etiqueta espec\u00edfica. Tambi\u00e9n permite a otros reutilizar esta funcionalidad para sus casos de uso. Data Load Cargar y guardar datos Primero, nombraremos y crearemos el directorio para guardar nuestros activos de datos # config/config.py from pathlib import Path # Directorios BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) # Crear carpeta DATA_DIR . mkdir ( parents = True , exist_ok = True ) A continuaci\u00f3n, agregaremos la ubicaci\u00f3n de nuestros activos de datos sin procesar a nuestro `config.py`. Es importante que almacenemos esta informaci\u00f3n en nuestro archivo de configuraci\u00f3n central para que podamos descubrirlo y actualizarla f\u00e1cilmente. # config/config.py ... # Activos PROJECTS_URL = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\" TAGS_URL = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\" Definiremos esta operaci\u00f3n en `main.py`: # src/main.py import pandas as pd from pathlib import Path import warnings from config import config from src import utils warnings . filterwarnings ( \"ignore\" ) def elt_data (): \"\"\"Extraer, cargar y transformar nuestros activos de datos.\"\"\" # Extract + Load projects = pd . read_csv ( config . PROJECTS_URL ) tags = pd . read_csv ( config . TAGS_URL ) projects . to_csv ( Path ( config . DATA_DIR , \"projects.csv\" ), index = False ) tags . to_csv ( Path ( config . DATA_DIR , \"tags.csv\" ), index = False ) # Transform df = pd . merge ( projects , tags , on = \"id\" ) df = df [ df . tag . notnull ()] # eliminamos filas sin tags df . to_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" ), index = False ) print ( \"\u2705 Datos guardados!\" ) Debemos asegurarnos de tener los paquetes necesarios cargados en nuestro entorno. Cargamos los paquetes requeridos y los agregemos a nuestro archivo `requirements.txt`: python -m pip install numpy == 1 .19.5 pandas == 1 .3.5 pretty-errors == 1 .2.19 # Agregar a requirements.txt numpy == 1 .19.5 pandas == 1 .3.5 pretty-errors == 1 .2.19 Ejecutaremos la operaci\u00f3n utilizando el int\u00e9rprete de Python a trav\u00e9s del terminal (escriba `python` en la terminal y luego los comandos a continuaci\u00f3n). from src import main main . elt_data () Preprocesamiento Preprocesamiento de features A continuaci\u00f3n, definiremos las funciones para preprocesar nuestros features de entrada. Usaremos estas funciones cuando estemos preparando los datos antes de entrenar nuestro modelo. No guardaremos los datos preprocesados en un archivo porque diferentes experimentos pueden preprocesarlos de manera diferente. # src/data.py def preprocess ( df , lower , stem , min_freq ): \"\"\"Preprocesar los datos.\"\"\" df [ \"text\" ] = df . title + \" \" + df . description # feature engineering df . text = df . text . apply ( clean_text , lower = lower , stem = stem ) # limpiar texto df = replace_oos_labels ( df = df , labels = config . ACCEPTED_TAGS , label_col = \"tag\" , oos_label = \"other\" ) # reemplazar etiquetas OOS df = replace_minority_labels ( df = df , label_col = \"tag\" , min_freq = min_freq , new_label = \"other\" ) # reemplazar las etiquetas por debajo de la frecuencia m\u00ednima return df Esta funci\u00f3n usa la funci\u00f3n `clean_text()`: # src/data.py from nltk.stem import PorterStemmer import re from config import config def clean_text ( text , lower = True , stem = False , stopwords = config . STOPWORDS ): \"\"\"Limpiar texto sin procesar.\"\"\" # Lower if lower : text = text . lower () # Remover stopwords if len ( stopwords ): pattern = re . compile ( r '\\b(' + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( '' , text ) # Espaciado y filtros text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # a\u00f1adir espacio entre los objetos a filtrar text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # eliminar caracteres no alfanum\u00e9ricos text = re . sub ( \" +\" , \" \" , text ) # eliminar m\u00faltiples espacios text = text . strip () # eliminar espacios en blanco en los extremos # Quitar links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = PorterStemmer () text = \" \" . join ([ stemmer . stem ( word , to_lowercase = lower ) for word in text . split ( \" \" )]) return text Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install nltk == 3 .7 # Agregar a requirements.txt nltk == 3 .7 Tenga en cuenta que estamos usando un conjunto expl\u00edcito de stopwords en lugar de la lista predeterminada de NLTK. Esto se debe a que queremos tener una visibilidad completa de exactamente qu\u00e9 palabras estamos filtrando. La lista general puede tener algunos t\u00e9rminos valiosos que deseamos conservar y viceversa. Tambi\u00e9n agregamos la lista de tags permitidos. # config/config.py ACCEPTED_TAGS = [ \"natural-language-processing\" , \"computer-vision\" , \"mlops\" , \"graph-learning\" ] STOPWORDS = [ \"i\" , \"me\" , \"my\" , \"myself\" , \"we\" , \"our\" , \"ours\" , \"ourselves\" , \"you\" , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , \"your\" , \"yours\" , \"yourself\" , \"yourselves\" , \"he\" , \"him\" , \"his\" , \"himself\" , \"she\" , \"she's\" , \"her\" , \"hers\" , \"herself\" , \"it\" , \"it's\" , \"its\" , \"itself\" , \"they\" , \"them\" , \"their\" , \"theirs\" , \"themselves\" , \"what\" , \"which\" , \"who\" , \"whom\" , \"this\" , \"that\" , \"that'll\" , \"these\" , \"those\" , \"am\" , \"is\" , \"are\" , \"was\" , \"were\" , \"be\" , \"been\" , \"being\" , \"have\" , \"has\" , \"had\" , \"having\" , \"do\" , \"does\" , \"did\" , \"doing\" , \"a\" , \"an\" , \"the\" , \"and\" , \"but\" , \"if\" , \"or\" , \"because\" , \"as\" , \"until\" , \"while\" , \"of\" , \"at\" , \"by\" , \"for\" , \"with\" , \"about\" , \"against\" , \"between\" , \"into\" , \"through\" , \"during\" , \"before\" , \"after\" , \"above\" , \"below\" , \"to\" , \"from\" , \"up\" , \"down\" , \"in\" , \"out\" , \"on\" , \"off\" , \"over\" , \"under\" , \"again\" , \"further\" , \"then\" , \"once\" , \"here\" , \"there\" , \"when\" , \"where\" , \"why\" , \"how\" , \"all\" , \"any\" , \"both\" , \"each\" , \"few\" , \"more\" , \"most\" , \"other\" , \"some\" , \"such\" , \"no\" , \"nor\" , \"not\" , \"only\" , \"own\" , \"same\" , \"so\" , \"than\" , \"too\" , \"very\" , \"s\" , \"t\" , \"can\" , \"will\" , \"just\" , \"don\" , \"don't\" , \"should\" , \"should've\" , \"now\" , \"d\" , \"ll\" , \"m\" , \"o\" , \"re\" , \"ve\" , \"y\" , \"ain\" , \"aren\" , \"aren't\" , \"couldn\" , \"couldn't\" , \"didn\" , \"didn't\" , \"doesn\" , \"doesn't\" , \"hadn\" , \"hadn't\" , \"hasn\" , \"hasn't\" , \"haven\" , \"haven't\" , \"isn\" , \"isn't\" , \"ma\" , \"mightn\" , \"mightn't\" , \"mustn\" , \"mustn't\" , \"needn\" , \"needn't\" , \"shan\" , \"shan't\" , \"shouldn\" , \"shouldn't\" , \"wasn\" , \"wasn't\" , \"weren\" , \"weren't\" , \"won\" , \"won't\" , \"wouldn\" , \"wouldn't\" , ] A continuaci\u00f3n, debemos definir las dos funciones a las que llamamos desde `data.py`: # src/data.py from collections import Counter def replace_oos_labels ( df , labels , label_col , oos_label = \"other\" ): \"\"\"Reemplazar las etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df [ label_col ] . unique () if item not in labels ] df [ label_col ] = df [ label_col ] . apply ( lambda x : oos_label if x in oos_tags else x ) return df def replace_minority_labels ( df , label_col , min_freq , new_label = \"other\" ): \"\"\"Reemplazar las etiquetas minoritarias con otra etiqueta.\"\"\" labels = Counter ( df [ label_col ] . values ) labels_above_freq = Counter ( label for label in labels . elements () if ( labels [ label ] >= min_freq )) df [ label_col ] = df [ label_col ] . apply ( lambda label : label if label in labels_above_freq else None ) df [ label_col ] = df [ label_col ] . fillna ( new_label ) return df Etiquetado Codificar etiquetas Ahora definamos el codificador para nuestras etiquetas: # src/data.py import json import numpy as np class LabelEncoder (): \"\"\"Codificar las etiquetas en \u00edndices \u00fanicos.\"\"\" def __init__ ( self , class_to_index = {}): self . class_to_index = class_to_index or {} # mutable defaults ;) self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) def __len__ ( self ): return len ( self . class_to_index ) def __str__ ( self ): return f \"<LabelEncoder(num_classes= { len ( self ) } )>\" def fit ( self , y ): classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self def encode ( self , y ): encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded def decode ( self , y ): classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes def save ( self , fp ): with open ( fp , \"w\" ) as fp : contents = { \"class_to_index\" : self . class_to_index } json . dump ( contents , fp , indent = 4 , sort_keys = False ) @classmethod def load ( cls , fp ): with open ( fp , \"r\" ) as fp : kwargs = json . load ( fp = fp ) return cls ( ** kwargs ) Split Split dataset Y finalmente, concluiremos nuestras operaciones de datos con nuestra funci\u00f3n de divisi\u00f3n: from sklearn.model_selection import train_test_split def get_data_splits ( X , y , train_size = 0.7 ): \"\"\"Generar divisiones de datos equilibradas.\"\"\" X_train , X_ , y_train , y_ = train_test_split ( X , y , train_size = train_size , stratify = y ) X_val , X_test , y_val , y_test = train_test_split ( X_ , y_ , train_size = 0.5 , stratify = y_ ) return X_train , X_val , X_test , y_train , y_val , y_test Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install scikit-learn == 0 .24.2 # Agregar a requirements.txt scikit-learn == 0 .24.2 Modelado Entrenamiento Entrenar con argumentos predeterminados Comenzaremos definiendo la operaci\u00f3n en nuestro `main.py`: # src/main.py import json from argparse import Namespace from src import data , train , utils def train_model ( args_fp ): \"\"\"Entrenar un modelo con argumentos dados.\"\"\" # Cargar datos etiquetados df = pd . read_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" )) # Entrenar args = Namespace ( ** utils . load_dict ( filepath = args_fp )) artifacts = train . train ( df = df , args = args ) performance = artifacts [ \"performance\" ] print ( json . dumps ( performance , indent = 2 )) Agregaremos m\u00e1s a nuestra operaci\u00f3n `train_model()` cuando tengamos en cuenta el seguimiento de experimentos pero, por ahora, es bastante simple. # src/train.py from imblearn.over_sampling import RandomOverSampler import json import numpy as np import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import log_loss from src import data , predict , utils , evaluate def train ( args , df , trial = None ): \"\"\"Entrenar modelo en datos.\"\"\" # Setup utils . set_seeds () if args . shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) df = df [: args . subset ] # Ninguno = todas las muestras df = data . preprocess ( df , lower = args . lower , stem = args . stem , min_freq = args . min_freq ) label_encoder = data . LabelEncoder () . fit ( df . tag ) X_train , X_val , X_test , y_train , y_val , y_test = \\ data . get_data_splits ( X = df . text . to_numpy (), y = label_encoder . encode ( df . tag )) test_df = pd . DataFrame ({ \"text\" : X_test , \"tag\" : label_encoder . decode ( y_test )}) # Tf-idf vectorizer = TfidfVectorizer ( analyzer = args . analyzer , ngram_range = ( 2 , args . ngram_max_range )) # char n-grams X_train = vectorizer . fit_transform ( X_train ) X_val = vectorizer . transform ( X_val ) X_test = vectorizer . transform ( X_test ) # Oversample oversample = RandomOverSampler ( sampling_strategy = \"all\" ) X_over , y_over = oversample . fit_resample ( X_train , y_train ) # Model model = SGDClassifier ( loss = \"log\" , penalty = \"l2\" , alpha = args . alpha , max_iter = 1 , learning_rate = \"constant\" , eta0 = args . learning_rate , power_t = args . power_t , warm_start = True ) # Training for epoch in range ( args . num_epochs ): model . fit ( X_over , y_over ) train_loss = log_loss ( y_train , model . predict_proba ( X_train )) val_loss = log_loss ( y_val , model . predict_proba ( X_val )) if not epoch % 10 : print ( f \"Epoch: { epoch : 02d } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } \" ) # Threshold y_pred = model . predict ( X_val ) y_prob = model . predict_proba ( X_val ) args . threshold = np . quantile ( [ y_prob [ i ][ j ] for i , j in enumerate ( y_pred )], q = 0.25 ) # Q1 # Evaluacion other_index = label_encoder . class_to_index [ \"other\" ] y_prob = model . predict_proba ( X_test ) y_pred = predict . custom_predict ( y_prob = y_prob , threshold = args . threshold , index = other_index ) performance = evaluate . get_metrics ( y_true = y_test , y_pred = y_pred , classes = label_encoder . classes , df = test_df ) return { \"args\" : args , \"label_encoder\" : label_encoder , \"vectorizer\" : vectorizer , \"model\" : model , \"performance\" : performance , } Esta funci\u00f3n `train()` llama a dos funciones externas (`predict.custom_predict()` de `predict.py` y `Evaluation.get_metrics()` de `Evaluation.py`): # src/predict.py import numpy as np def custom_predict ( y_prob , threshold , index ): \"\"\"Funci\u00f3n de predicci\u00f3n personalizada que por defecto es un \u00edndice si no se cumplen las condiciones.\"\"\" y_pred = [ np . argmax ( p ) if max ( p ) > threshold else index for p in y_prob ] return np . array ( y_pred ) # src/evaluate.py import numpy as np from sklearn.metrics import precision_recall_fscore_support from snorkel.slicing import PandasSFApplier from snorkel.slicing import slicing_function @slicing_function () def nlp_cnn ( x ): \"\"\"Proyectos de NLP que utilizan convoluci\u00f3n.\"\"\" nlp_projects = \"natural-language-processing\" in x . tag convolution_projects = \"CNN\" in x . text or \"convolution\" in x . text return ( nlp_projects and convolution_projects ) @slicing_function () def short_text ( x ): \"\"\"Proyectos con t\u00edtulos y descripciones cortos.\"\"\" return len ( x . text . split ()) < 8 # menos de 8 palabras def get_slice_metrics ( y_true , y_pred , slices ): \"\"\"Generar m\u00e9tricas para segmentos de datos.\"\"\" metrics = {} for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): slice_metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) metrics [ slice_name ] = {} metrics [ slice_name ][ \"precision\" ] = slice_metrics [ 0 ] metrics [ slice_name ][ \"recall\" ] = slice_metrics [ 1 ] metrics [ slice_name ][ \"f1\" ] = slice_metrics [ 2 ] metrics [ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ]) return metrics def get_metrics ( y_true , y_pred , classes , df = None ): \"\"\"M\u00e9tricas de rendimiento utilizando verdades y predicciones.\"\"\" # Performance metrics = { \"overall\" : {}, \"class\" : {}} # M\u00e9tricas generales overall_metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) metrics [ \"overall\" ][ \"precision\" ] = overall_metrics [ 0 ] metrics [ \"overall\" ][ \"recall\" ] = overall_metrics [ 1 ] metrics [ \"overall\" ][ \"f1\" ] = overall_metrics [ 2 ] metrics [ \"overall\" ][ \"num_samples\" ] = np . float64 ( len ( y_true )) # M\u00e9tricas por clase class_metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i , _class in enumerate ( classes ): metrics [ \"class\" ][ _class ] = { \"precision\" : class_metrics [ 0 ][ i ], \"recall\" : class_metrics [ 1 ][ i ], \"f1\" : class_metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( class_metrics [ 3 ][ i ]), } # M\u00e9tricas de secciones if df is not None : slices = PandasSFApplier ([ nlp_cnn , short_text ]) . apply ( df ) metrics [ \"slices\" ] = get_slice_metrics ( y_true = y_true , y_pred = y_pred , slices = slices ) return metrics Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install imbalanced-learn == 0 .8.1 snorkel == 0 .9.8 # Agregar a requirements.txt imbalanced-learn == 0 .8.1 snorkel == 0 .9.8 Comandos para entrenar un modelo: from pathlib import Path from config import config from src import main args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) main . train_model ( args_fp ) Puede ser que aparezca un mensaje como este: TypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc > = 3 .19.0. Para solucionar este error hay que hacer un downgrade de protobuf: python -m pip install protobuf == 3 .20.* # Agregar a requirements.txt protobuf == 3 .20.* Luego vuelva a ejecutar los comandos para entrenar el modelo. Optimizaci\u00f3n Optimizar argumentos Ahora que podemos entrenar un modelo, estamos listos para entrenar muchos modelos para optimizar nuestros hiperpar\u00e1metros: # src/main.py import mlflow from numpyencoder import NumpyEncoder import optuna from optuna.integration.mlflow import MLflowCallback def optimize ( args_fp , study_name , num_trials ): \"\"\"Optimizar hiperpar\u00e1metros.\"\"\" # Cargar datos etiquetados df = pd . read_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" )) # Optimizar args = Namespace ( ** utils . load_dict ( filepath = args_fp )) pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = \"optimization\" , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( args , df , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ]) # Mejor prueba trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"user_attrs_f1\" ], ascending = False ) utils . save_dict ({ ** args . __dict__ , ** study . best_trial . params }, args_fp , cls = NumpyEncoder ) print ( f \" \\n Mejor valor (f1): { study . best_trial . value } \" ) print ( f \"Mejores hiperpar\u00e1metros: { json . dumps ( study . best_trial . params , indent = 2 ) } \" ) Definiremos la funci\u00f3n `objective()` dentro de `train.py`: # src/train.py def objective ( args , df , trial ): \"\"\"Funci\u00f3n objetivo para pruebas de optimizaci\u00f3n.\"\"\" # Par\u00e1metros a tunear args . analyzer = trial . suggest_categorical ( \"analyzer\" , [ \"word\" , \"char\" , \"char_wb\" ]) args . ngram_max_range = trial . suggest_int ( \"ngram_max_range\" , 3 , 10 ) args . learning_rate = trial . suggest_loguniform ( \"learning_rate\" , 1e-2 , 1e0 ) args . power_t = trial . suggest_uniform ( \"power_t\" , 0.1 , 0.5 ) # Entrenar & evaluatar artifacts = train ( args = args , df = df , trial = trial ) # Establecer atributos adicionales overall_performance = artifacts [ \"performance\" ][ \"overall\" ] print ( json . dumps ( overall_performance , indent = 2 )) trial . set_user_attr ( \"precision\" , overall_performance [ \"precision\" ]) trial . set_user_attr ( \"recall\" , overall_performance [ \"recall\" ]) trial . set_user_attr ( \"f1\" , overall_performance [ \"f1\" ]) return overall_performance [ \"f1\" ] Recuerde que en nuestro notebook modificamos la funci\u00f3n `train()` para incluir informaci\u00f3n sobre las pruebas durante la optimizaci\u00f3n para pruning: # src/train.py import optuna def train (): ... # Entrenamiento for epoch in range ( args . num_epochs ): ... # Pruning (para optimizaci\u00f3n en la siguiente secci\u00f3n) if trial : trial . report ( val_loss , epoch ) if trial . should_prune (): raise optuna . TrialPruned () Dado que estamos usando `MLflowCallback` aqu\u00ed con Optuna, podemos permitir que todos nuestros experimentos se almacenen en el directorio `mlruns` predeterminado que crear\u00e1 MLflow o podemos configurar esa ubicaci\u00f3n: # config/config.py import mlflow # Directorios BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) # Stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) # Crear carpetas DATA_DIR . mkdir ( parents = True , exist_ok = True ) MODEL_REGISTRY . mkdir ( parents = True , exist_ok = True ) # Registro del modelo MLFlow mlflow . set_tracking_uri ( \"file: \\\\ \" + str ( MODEL_REGISTRY . absolute ())) Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install mlflow == 1 .23.1 optuna == 2 .10.0 numpyencoder == 0 .3.0 # Agregar a requirements.txt mlflow == 1 .23.1 numpyencoder == 0 .3.0 optuna == 2 .10.0 Comandos para optimizar hiperpar\u00e1metros: from pathlib import Path from config import config from src import main args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) main . optimize ( args_fp , study_name = \"optimization\" , num_trials = 20 ) Seguimiento de experimentos Seguimiento de experimentos Ahora que tenemos nuestros hiperpar\u00e1metros optimizados, podemos entrenar un modelo y almacenar sus artefactos a trav\u00e9s del seguimiento de experimentos. Comenzaremos modificando la operaci\u00f3n `train()` en nuestro script `main.py`: # src/main.py import joblib import tempfile def train_model ( args_fp , experiment_name , run_name ): \"\"\"Entrenar un modelo con argumentos dados.\"\"\" # Cargar datos etiquetados df = pd . read_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" )) # Entrenar args = Namespace ( ** utils . load_dict ( filepath = args_fp )) mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id print ( f \"Run ID: { run_id } \" ) artifacts = train . train ( df = df , args = args ) performance = artifacts [ \"performance\" ] print ( json . dumps ( performance , indent = 2 )) # Logeo de m\u00e9tricas y par\u00e1metros performance = artifacts [ \"performance\" ] mlflow . log_metrics ({ \"precision\" : performance [ \"overall\" ][ \"precision\" ]}) mlflow . log_metrics ({ \"recall\" : performance [ \"overall\" ][ \"recall\" ]}) mlflow . log_metrics ({ \"f1\" : performance [ \"overall\" ][ \"f1\" ]}) mlflow . log_params ( vars ( artifacts [ \"args\" ])) # Logeo de artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"args\" ]), Path ( dp , \"args.json\" ), cls = NumpyEncoder ) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) joblib . dump ( artifacts [ \"vectorizer\" ], Path ( dp , \"vectorizer.pkl\" )) joblib . dump ( artifacts [ \"model\" ], Path ( dp , \"model.pkl\" )) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) mlflow . log_artifacts ( dp ) # Guardar en config open ( Path ( config . CONFIG_DIR , \"run_id.txt\" ), \"w\" ) . write ( run_id ) utils . save_dict ( performance , Path ( config . CONFIG_DIR , \"performance.json\" )) Tambi\u00e9n vamos a actualizar la funci\u00f3n `train()` dentro de `train.py` para que se capturen las m\u00e9tricas intermedias: # src/train.py import mlflow def train (): ... # Training for epoch in range ( args . num_epochs ): ... # Log if not trial : mlflow . log_metrics ({ \"train_loss\" : train_loss , \"val_loss\" : val_loss }, step = epoch ) Comandos para entrenar un modelo con seguimiento de experimentos: from pathlib import Path from config import config from src import main args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) main . train_model ( args_fp , experiment_name = \"baselines\" , run_name = \"sgd\" ) Nuestro directorio de configuraci\u00f3n ahora deber\u00eda tener un archivo performance.json y un archivo run_id.txt. Los estamos guardando para poder acceder r\u00e1pidamente a estos metadatos del \u00faltimo entrenamiento exitoso. Predicci\u00f3n Predecir textos Finalmente estamos listos para usar nuestro modelo entrenado para la inferencia. Agregaremos la operaci\u00f3n para predecir una etiqueta a `main.py`: # src/main.py from src import data , predict , train , utils def predict_tag ( text , run_id = None ): \"\"\"Predecir etiqueta para texto.\"\"\" if not run_id : run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () artifacts = load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) print ( json . dumps ( prediction , indent = 2 )) return prediction Esto implica crear la funci\u00f3n `load_artifacts()` dentro de nuestro script `main.py`: # src/main.py def load_artifacts ( run_id ): \"\"\"Cargar artefactos para un run_id determinado.\"\"\" # Localizar el directorio espec\u00edficos de los artefactos experiment_id = mlflow . get_run ( run_id = run_id ) . info . experiment_id artifacts_dir = Path ( config . MODEL_REGISTRY , experiment_id , run_id , \"artifacts\" ) # Cargar objetos desde la ejecuci\u00f3n args = Namespace ( ** utils . load_dict ( filepath = Path ( artifacts_dir , \"args.json\" ))) vectorizer = joblib . load ( Path ( artifacts_dir , \"vectorizer.pkl\" )) label_encoder = data . LabelEncoder . load ( fp = Path ( artifacts_dir , \"label_encoder.json\" )) model = joblib . load ( Path ( artifacts_dir , \"model.pkl\" )) performance = utils . load_dict ( filepath = Path ( artifacts_dir , \"performance.json\" )) return { \"args\" : args , \"label_encoder\" : label_encoder , \"vectorizer\" : vectorizer , \"model\" : model , \"performance\" : performance } Y definir la funci\u00f3n `predict()` dentro de `predict.py`: def predict ( texts , artifacts ): \"\"\"Predecir etiquetas para textos dados.\"\"\" x = artifacts [ \"vectorizer\" ] . transform ( texts ) y_pred = custom_predict ( y_prob = artifacts [ \"model\" ] . predict_proba ( x ), threshold = artifacts [ \"args\" ] . threshold , index = artifacts [ \"label_encoder\" ] . class_to_index [ \"other\" ]) tags = artifacts [ \"label_encoder\" ] . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"predicted_tag\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions Comandos para predecir la etiqueta de texto: from pathlib import Path from config import config from src import main text = \"Transfer learning with transformers for text classification.\" run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () main . predict_tag ( text = text , run_id = run_id ) En la secci\u00f3n de documentaci\u00f3n veremos como formatear nuestras funciones y clases correctamente.","title":"Organizaci\u00f3n"},{"location":"Desarrollo/Organizaci%C3%B3n/#organizacion-del-codigo-de-machine-learning","text":"Organizar el c\u00f3digo al pasar de notebooks a scripts de Python.","title":"Organizaci\u00f3n del c\u00f3digo de Machine Learning"},{"location":"Desarrollo/Organizaci%C3%B3n/#introduccion","text":"Tener un c\u00f3digo organizado es tener un c\u00f3digo legible, reproducible y robusto. Tu equipo y, tambi\u00e9n tu yo futuro, te agradecer\u00e1n por hacer el esfuerzo inicial para organizar el trabajo. Veremos c\u00f3mo migrar y organizar el c\u00f3digo de nuestro notebook a scripts de Python.","title":"Introducci\u00f3n"},{"location":"Desarrollo/Organizaci%C3%B3n/#editor","text":"Hay varias opciones para los editores de c\u00f3digo, como VSCode, Atom, Sublime, PyCharm, Vim, etc. y todos ofrecen funciones \u00fanicas al tiempo que proporcionan las operaciones b\u00e1sicas para la edici\u00f3n y ejecuci\u00f3n de c\u00f3digo. Usaremos VSCode para editar y ejecutar nuestro c\u00f3digo gracias a su simplicidad, soporte multiling\u00fce, complementos y creciente adopci\u00f3n en la industria. En Visual Studio Code abra la paleta de comandos (CTRL + SHIFT + P) y escriba \"Preferences: Open Settings (UI)\" y presione Enter. Ajuste cualquier configuraci\u00f3n relevante que desee (espaciado, tama\u00f1o de fuente, etc.) Instale extensiones de VSCode (utilice el \u00edcono de bloques de lego en el panel izquierdo del editor) Extensiones VSCode recomendadas Recomendamos instalar estas extensiones: code --install-extension alefragnani.project-manager code --install-extension bierner.markdown-preview-github-styles code --install-extension christian-kohler.path-intellisense code --install-extension euskadi31.json-pretty-printer code --install-extension mechatroner.rainbow-csv code --install-extension mikestead.dotenv code --install-extension ms-azuretools.vscode-docker code --install-extension ms-python.python code --install-extension ms-python.vscode-pylance code --install-extension njpwerner.autodocstring code --install-extension PKief.material-icon-theme code --install-extension redhat.vscode-yaml code --install-extension shardulm94.trailing-spaces code --install-extension streetsidesoftware.code-spell-checker-spanish code --install-extension DavidAnson.vscode-markdownlint code --install-extension donjayamanne.python-environment-manager code --install-extension donjayamanne.python-extension-pack code --install-extension equinusocio.vsc-material-theme-icons code --install-extension etmoffat.pip-packages code --install-extension KevinRose.vsc-python-indent code --install-extension magicstack.MagicPython code --install-extension mdickin.markdown-shortcuts code --install-extension ms-azure-devops.azure-pipelines code --install-extension ms-azuretools.vscode-azureappservice code --install-extension ms-azuretools.vscode-azurefunctions code --install-extension ms-azuretools.vscode-azureresourcegroups code --install-extension ms-azuretools.vscode-azurestaticwebapps code --install-extension ms-azuretools.vscode-azurestorage code --install-extension ms-azuretools.vscode-azurevirtualmachines code --install-extension ms-azuretools.vscode-bicep code --install-extension ms-azuretools.vscode-cosmosdb code --install-extension ms-python.isort code --install-extension ms-toolsai.jupyter code --install-extension ms-toolsai.jupyter-keymap code --install-extension ms-toolsai.jupyter-renderers code --install-extension ms-toolsai.vscode-ai code --install-extension ms-toolsai.vscode-ai-remote code --install-extension ms-toolsai.vscode-jupyter-cell-tags code --install-extension ms-toolsai.vscode-jupyter-slideshow code --install-extension ms-vscode-remote.remote-containers code --install-extension ms-vscode-remote.remote-wsl code --install-extension ms-vscode.azure-account code --install-extension ms-vscode.azurecli code --install-extension ms-vscode.powershell code --install-extension ms-vscode.vscode-node-azure-pack code --install-extension msazurermtools.azurerm-vscode-tools code --install-extension VisualStudioExptTeam.vscodeintellicode code --install-extension vscode-icons-team.vscode-icons code --install-extension yzhang.markdown-all-in-one Si agrega sus propias extensiones y desea compartirlas con otros, simplemente ejecute este comando para generar la lista de comandos: code --list-extensions | xargs -L 1 echo code --install-extension Luego de configurar VSCode, podemos comenzar creando nuestro directorio de proyectos, que usaremos para organizar todos nuestros scripts. Hay muchas maneras de iniciar un proyecto, pero esta es nuestra ruta recomendada: Utilice la terminal para crear un directorio ( mkdir <NOMBRE_PROYECTO> ). Cambie al directorio del proyecto que acaba de crear ( cd <PROJECT_NAME> ). Inicie VSCode desde este directorio escribiendo el code . Abra una terminal dentro de VSCode ( View > Terminal ) para continuar creando scripts ( touch <NOMBRE_DE_ARCHIVO> ) o subdirectorios adicionales ( mkdir <SUBDIR> ) seg\u00fan sea necesario.","title":"Editor"},{"location":"Desarrollo/Organizaci%C3%B3n/#setup","text":"","title":"Setup"},{"location":"Desarrollo/Organizaci%C3%B3n/#readme","text":"Comenzaremos nuestra organizaci\u00f3n con un archivo README.md, que proporcionar\u00e1 informaci\u00f3n sobre los archivos en nuestro directorio, instrucciones para ejecutar operaciones, etc. Mantendremos constantemente actualizado este archivo para que podamos catalogar informaci\u00f3n para el futuro. touch README.md Comencemos agregando las instrucciones que usamos para crear un virtual environment: # CoE MLOps Template ## Virtual environment python3 -m venv venv source ./venv/scripts/activate python -m pip install --upgrade pip setuptools wheel pip install -e .","title":"README"},{"location":"Desarrollo/Organizaci%C3%B3n/#configuraciones","text":"A continuaci\u00f3n, crearemos un directorio de configuraci\u00f3n llamado config donde podemos almacenar los componentes que ser\u00e1n necesarios para nuestra aplicaci\u00f3n. Dentro de este directorio, crearemos un config.py y un args.json . mkdir config touch config/config.py config/args.json Dentro de config.py , agregaremos el c\u00f3digo para definir las ubicaciones clave del directorio (agregaremos m\u00e1s configuraciones a medida que sean necesarias): # config.py from pathlib import Path # Directorios BASE_DIR = Path ( __file__ ) .parent.parent.absolute () CONFIG_DIR = Path ( BASE_DIR, \"config\" ) y dentro de args.json , agregaremos los par\u00e1metros que son relevantes para el procesamiento de datos y el entrenamiento del modelo. { \"shuffle\" : true, \"subset\" : null, \"min_freq\" : 75 , \"lower\" : true, \"stem\" : false, \"analyzer\" : \"char\" , \"ngram_max_range\" : 7 , \"alpha\" : 1e-4, \"learning_rate\" : 1e-1, \"power_t\" : 0 .1, \"num_epochs\" : 100 }","title":"Configuraciones"},{"location":"Desarrollo/Organizaci%C3%B3n/#operaciones","text":"Comenzaremos creando nuestro directorio de paquetes ( src ) dentro de nuestro directorio de proyectos ( mlops ). Dentro de este directorio de paquetes, crearemos un archivo main.py que definir\u00e1 las operaciones principales que queremos tener disponibles. mkdir src touch src/main.py Definiremos estas operaciones centrales dentro de main.py a medida que movemos el c\u00f3digo de los notebooks a los scripts apropiados a continuaci\u00f3n: elt_data : extrae, carga y transforma datos. optimize : ajustar los hiperpar\u00e1metros para optimizar para el objetivo. train_model : entrena un modelo utilizando los mejores par\u00e1metros del estudio de optimizaci\u00f3n. load_artifacts : carga artefactos entrenados de una ejecuci\u00f3n determinada. predict_tag : predecir una etiqueta para una entrada determinada.","title":"Operaciones"},{"location":"Desarrollo/Organizaci%C3%B3n/#utilidades","text":"Es com\u00fan tener procesos ad-hoc dentro de los notebooks porque mantiene el estado siempre que el notebook se est\u00e9 ejecutando. Por ejemplo, podemos establecer seeds en nuestros notebooks as\u00ed: # Setear seeds np . random . seed ( seed ) random . seed ( seed ) Pero en nuestros scripts, debemos envolver esta funcionalidad como una funci\u00f3n limpia y reutilizable con los par\u00e1metros apropiados: def set_seeds ( seed = 42 ): \"\"\"Setear seeds para la reproducibilidad.\"\"\" np . random . seed ( seed ) random . seed ( seed ) Podemos almacenar todas estas funciones dentro de un archivo Utils.py en el directorio de paquetes src . touch src/utils.py Ver utils.py import json import numpy as np import random def load_dict ( filepath ): \"\"\"Cargar un diccionario desde la ruta de archivo de un JSON.\"\"\" with open ( filepath , \"r\" ) as fp : d = json . load ( fp ) return d def save_dict ( d , filepath , cls = None , sortkeys = False ): \"\"\"Guardar un diccionario en una ubicaci\u00f3n espec\u00edfica.\"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) def set_seeds ( seed = 42 ): \"\"\"Setear seeds para la reproducibilidad.\"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed )","title":"Utilidades"},{"location":"Desarrollo/Organizaci%C3%B3n/#proyecto","text":"Cuando se trata de migrar nuestro c\u00f3digo de notebooks a scripts, es mejor organizarse en funci\u00f3n de la utilidad. Por ejemplo, podemos crear scripts para las diversas etapas del desarrollo de ML, como procesamiento de datos, entrenamiento, evaluaci\u00f3n, predicci\u00f3n, etc. Crearemos los diferentes archivos de Python para envolver nuestros datos y funcionalidad ML: cd src touch data.py train.py evaluate.py predict.py Es posible que tengamos scripts adicionales en otros proyectos, ya que son necesarios. Por ejemplo, normalmente tendr\u00edamos un script modelos.py. Organizar nuestra base de c\u00f3digo de esta manera tambi\u00e9n nos hace m\u00e1s f\u00e1cil comprender (o modificar) la base de c\u00f3digo. Podr\u00edamos haber colocado todo el c\u00f3digo en un script main.py, pero a medida que nuestro proyecto crezca, ser\u00e1 dif\u00edcil navegar por un archivo monol\u00edtico. Por otro lado, podr\u00edamos haber asumido una postura m\u00e1s granular descomponiendo datos.py en split.py, preprocess.py, etc. Esto podr\u00eda tener m\u00e1s sentido si tenemos m\u00faltiples formas de dividir, preprocesamiento, etc. Pero en general, es suficiente estar en este nivel m\u00e1s alto de organizaci\u00f3n.","title":"Proyecto"},{"location":"Desarrollo/Organizaci%C3%B3n/#principios","text":"A trav\u00e9s del proceso de migraci\u00f3n utilizaremos varios principios b\u00e1sicos de ingenier\u00eda de software repetidamente:","title":"Principios"},{"location":"Desarrollo/Organizaci%C3%B3n/#envolver-la-funcionalidad-en-funciones","text":"\u00bfC\u00f3mo decidimos cu\u00e1ndo deben envolver l\u00edneas de c\u00f3digo espec\u00edficas como una funci\u00f3n separada? Las funciones deben ser at\u00f3micas porque cada una tiene una sola responsabilidad para que podamos probarlas f\u00e1cilmente. Si no, necesitaremos dividirlos en unidades m\u00e1s granulares. Por ejemplo, podr\u00edamos reemplazar las etiquetas en nuestros proyectos con estas l\u00edneas: oos_tags = [ item for item in df . tag . unique () if item not in tags_dict . keys ()] df . tag = df . tag . apply ( lambda x : \"other\" if x in oos_tags else x ) Refactorizado quedar\u00eda as\u00ed: def replace_oos_tags ( df , tags_dict ): \"\"\"Reemplazar etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df . tag . unique () if item not in tags_dict . keys ()] df . tag = df . tag . apply ( lambda x : \"other\" if x in oos_tags else x ) return df","title":"Envolver la funcionalidad en funciones"},{"location":"Desarrollo/Organizaci%C3%B3n/#componer-funciones-generalizadas","text":"def replace_oos_tags ( df , tags_dict ): \"\"\"Reemplazar etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df . tag . unique () if item not in tags_dict . keys ()] df . tag = df . tag . apply ( lambda x : \"other\" if x in oos_tags else x ) return df Versus: def replace_oos_labels ( df , labels , label_col , oos_label = \"other\" ): \"\"\"Reemplazar etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df [ label_col ] . unique () if item not in labels ] df [ label_col ] = df [ label_col ] . apply ( lambda x : oos_label if x in oos_tags else x ) return df De esta manera, cuando cambian los nombres de las columnas o queremos reemplazar con diferentes etiquetas, es muy f\u00e1cil ajustar nuestro c\u00f3digo. Esto tambi\u00e9n incluye el uso de nombres generalizados en las funciones como la etiqueta en lugar del nombre de la columna de etiqueta espec\u00edfica. Tambi\u00e9n permite a otros reutilizar esta funcionalidad para sus casos de uso.","title":"Componer funciones generalizadas"},{"location":"Desarrollo/Organizaci%C3%B3n/#data","text":"","title":"Data"},{"location":"Desarrollo/Organizaci%C3%B3n/#load","text":"Cargar y guardar datos Primero, nombraremos y crearemos el directorio para guardar nuestros activos de datos # config/config.py from pathlib import Path # Directorios BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) # Crear carpeta DATA_DIR . mkdir ( parents = True , exist_ok = True ) A continuaci\u00f3n, agregaremos la ubicaci\u00f3n de nuestros activos de datos sin procesar a nuestro `config.py`. Es importante que almacenemos esta informaci\u00f3n en nuestro archivo de configuraci\u00f3n central para que podamos descubrirlo y actualizarla f\u00e1cilmente. # config/config.py ... # Activos PROJECTS_URL = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\" TAGS_URL = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\" Definiremos esta operaci\u00f3n en `main.py`: # src/main.py import pandas as pd from pathlib import Path import warnings from config import config from src import utils warnings . filterwarnings ( \"ignore\" ) def elt_data (): \"\"\"Extraer, cargar y transformar nuestros activos de datos.\"\"\" # Extract + Load projects = pd . read_csv ( config . PROJECTS_URL ) tags = pd . read_csv ( config . TAGS_URL ) projects . to_csv ( Path ( config . DATA_DIR , \"projects.csv\" ), index = False ) tags . to_csv ( Path ( config . DATA_DIR , \"tags.csv\" ), index = False ) # Transform df = pd . merge ( projects , tags , on = \"id\" ) df = df [ df . tag . notnull ()] # eliminamos filas sin tags df . to_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" ), index = False ) print ( \"\u2705 Datos guardados!\" ) Debemos asegurarnos de tener los paquetes necesarios cargados en nuestro entorno. Cargamos los paquetes requeridos y los agregemos a nuestro archivo `requirements.txt`: python -m pip install numpy == 1 .19.5 pandas == 1 .3.5 pretty-errors == 1 .2.19 # Agregar a requirements.txt numpy == 1 .19.5 pandas == 1 .3.5 pretty-errors == 1 .2.19 Ejecutaremos la operaci\u00f3n utilizando el int\u00e9rprete de Python a trav\u00e9s del terminal (escriba `python` en la terminal y luego los comandos a continuaci\u00f3n). from src import main main . elt_data ()","title":"Load"},{"location":"Desarrollo/Organizaci%C3%B3n/#preprocesamiento","text":"Preprocesamiento de features A continuaci\u00f3n, definiremos las funciones para preprocesar nuestros features de entrada. Usaremos estas funciones cuando estemos preparando los datos antes de entrenar nuestro modelo. No guardaremos los datos preprocesados en un archivo porque diferentes experimentos pueden preprocesarlos de manera diferente. # src/data.py def preprocess ( df , lower , stem , min_freq ): \"\"\"Preprocesar los datos.\"\"\" df [ \"text\" ] = df . title + \" \" + df . description # feature engineering df . text = df . text . apply ( clean_text , lower = lower , stem = stem ) # limpiar texto df = replace_oos_labels ( df = df , labels = config . ACCEPTED_TAGS , label_col = \"tag\" , oos_label = \"other\" ) # reemplazar etiquetas OOS df = replace_minority_labels ( df = df , label_col = \"tag\" , min_freq = min_freq , new_label = \"other\" ) # reemplazar las etiquetas por debajo de la frecuencia m\u00ednima return df Esta funci\u00f3n usa la funci\u00f3n `clean_text()`: # src/data.py from nltk.stem import PorterStemmer import re from config import config def clean_text ( text , lower = True , stem = False , stopwords = config . STOPWORDS ): \"\"\"Limpiar texto sin procesar.\"\"\" # Lower if lower : text = text . lower () # Remover stopwords if len ( stopwords ): pattern = re . compile ( r '\\b(' + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( '' , text ) # Espaciado y filtros text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # a\u00f1adir espacio entre los objetos a filtrar text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # eliminar caracteres no alfanum\u00e9ricos text = re . sub ( \" +\" , \" \" , text ) # eliminar m\u00faltiples espacios text = text . strip () # eliminar espacios en blanco en los extremos # Quitar links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = PorterStemmer () text = \" \" . join ([ stemmer . stem ( word , to_lowercase = lower ) for word in text . split ( \" \" )]) return text Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install nltk == 3 .7 # Agregar a requirements.txt nltk == 3 .7 Tenga en cuenta que estamos usando un conjunto expl\u00edcito de stopwords en lugar de la lista predeterminada de NLTK. Esto se debe a que queremos tener una visibilidad completa de exactamente qu\u00e9 palabras estamos filtrando. La lista general puede tener algunos t\u00e9rminos valiosos que deseamos conservar y viceversa. Tambi\u00e9n agregamos la lista de tags permitidos. # config/config.py ACCEPTED_TAGS = [ \"natural-language-processing\" , \"computer-vision\" , \"mlops\" , \"graph-learning\" ] STOPWORDS = [ \"i\" , \"me\" , \"my\" , \"myself\" , \"we\" , \"our\" , \"ours\" , \"ourselves\" , \"you\" , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , \"your\" , \"yours\" , \"yourself\" , \"yourselves\" , \"he\" , \"him\" , \"his\" , \"himself\" , \"she\" , \"she's\" , \"her\" , \"hers\" , \"herself\" , \"it\" , \"it's\" , \"its\" , \"itself\" , \"they\" , \"them\" , \"their\" , \"theirs\" , \"themselves\" , \"what\" , \"which\" , \"who\" , \"whom\" , \"this\" , \"that\" , \"that'll\" , \"these\" , \"those\" , \"am\" , \"is\" , \"are\" , \"was\" , \"were\" , \"be\" , \"been\" , \"being\" , \"have\" , \"has\" , \"had\" , \"having\" , \"do\" , \"does\" , \"did\" , \"doing\" , \"a\" , \"an\" , \"the\" , \"and\" , \"but\" , \"if\" , \"or\" , \"because\" , \"as\" , \"until\" , \"while\" , \"of\" , \"at\" , \"by\" , \"for\" , \"with\" , \"about\" , \"against\" , \"between\" , \"into\" , \"through\" , \"during\" , \"before\" , \"after\" , \"above\" , \"below\" , \"to\" , \"from\" , \"up\" , \"down\" , \"in\" , \"out\" , \"on\" , \"off\" , \"over\" , \"under\" , \"again\" , \"further\" , \"then\" , \"once\" , \"here\" , \"there\" , \"when\" , \"where\" , \"why\" , \"how\" , \"all\" , \"any\" , \"both\" , \"each\" , \"few\" , \"more\" , \"most\" , \"other\" , \"some\" , \"such\" , \"no\" , \"nor\" , \"not\" , \"only\" , \"own\" , \"same\" , \"so\" , \"than\" , \"too\" , \"very\" , \"s\" , \"t\" , \"can\" , \"will\" , \"just\" , \"don\" , \"don't\" , \"should\" , \"should've\" , \"now\" , \"d\" , \"ll\" , \"m\" , \"o\" , \"re\" , \"ve\" , \"y\" , \"ain\" , \"aren\" , \"aren't\" , \"couldn\" , \"couldn't\" , \"didn\" , \"didn't\" , \"doesn\" , \"doesn't\" , \"hadn\" , \"hadn't\" , \"hasn\" , \"hasn't\" , \"haven\" , \"haven't\" , \"isn\" , \"isn't\" , \"ma\" , \"mightn\" , \"mightn't\" , \"mustn\" , \"mustn't\" , \"needn\" , \"needn't\" , \"shan\" , \"shan't\" , \"shouldn\" , \"shouldn't\" , \"wasn\" , \"wasn't\" , \"weren\" , \"weren't\" , \"won\" , \"won't\" , \"wouldn\" , \"wouldn't\" , ] A continuaci\u00f3n, debemos definir las dos funciones a las que llamamos desde `data.py`: # src/data.py from collections import Counter def replace_oos_labels ( df , labels , label_col , oos_label = \"other\" ): \"\"\"Reemplazar las etiquetas fuera de alcance (oos).\"\"\" oos_tags = [ item for item in df [ label_col ] . unique () if item not in labels ] df [ label_col ] = df [ label_col ] . apply ( lambda x : oos_label if x in oos_tags else x ) return df def replace_minority_labels ( df , label_col , min_freq , new_label = \"other\" ): \"\"\"Reemplazar las etiquetas minoritarias con otra etiqueta.\"\"\" labels = Counter ( df [ label_col ] . values ) labels_above_freq = Counter ( label for label in labels . elements () if ( labels [ label ] >= min_freq )) df [ label_col ] = df [ label_col ] . apply ( lambda label : label if label in labels_above_freq else None ) df [ label_col ] = df [ label_col ] . fillna ( new_label ) return df","title":"Preprocesamiento"},{"location":"Desarrollo/Organizaci%C3%B3n/#etiquetado","text":"Codificar etiquetas Ahora definamos el codificador para nuestras etiquetas: # src/data.py import json import numpy as np class LabelEncoder (): \"\"\"Codificar las etiquetas en \u00edndices \u00fanicos.\"\"\" def __init__ ( self , class_to_index = {}): self . class_to_index = class_to_index or {} # mutable defaults ;) self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) def __len__ ( self ): return len ( self . class_to_index ) def __str__ ( self ): return f \"<LabelEncoder(num_classes= { len ( self ) } )>\" def fit ( self , y ): classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self def encode ( self , y ): encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded def decode ( self , y ): classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes def save ( self , fp ): with open ( fp , \"w\" ) as fp : contents = { \"class_to_index\" : self . class_to_index } json . dump ( contents , fp , indent = 4 , sort_keys = False ) @classmethod def load ( cls , fp ): with open ( fp , \"r\" ) as fp : kwargs = json . load ( fp = fp ) return cls ( ** kwargs )","title":"Etiquetado"},{"location":"Desarrollo/Organizaci%C3%B3n/#split","text":"Split dataset Y finalmente, concluiremos nuestras operaciones de datos con nuestra funci\u00f3n de divisi\u00f3n: from sklearn.model_selection import train_test_split def get_data_splits ( X , y , train_size = 0.7 ): \"\"\"Generar divisiones de datos equilibradas.\"\"\" X_train , X_ , y_train , y_ = train_test_split ( X , y , train_size = train_size , stratify = y ) X_val , X_test , y_val , y_test = train_test_split ( X_ , y_ , train_size = 0.5 , stratify = y_ ) return X_train , X_val , X_test , y_train , y_val , y_test Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install scikit-learn == 0 .24.2 # Agregar a requirements.txt scikit-learn == 0 .24.2","title":"Split"},{"location":"Desarrollo/Organizaci%C3%B3n/#modelado","text":"","title":"Modelado"},{"location":"Desarrollo/Organizaci%C3%B3n/#entrenamiento","text":"Entrenar con argumentos predeterminados Comenzaremos definiendo la operaci\u00f3n en nuestro `main.py`: # src/main.py import json from argparse import Namespace from src import data , train , utils def train_model ( args_fp ): \"\"\"Entrenar un modelo con argumentos dados.\"\"\" # Cargar datos etiquetados df = pd . read_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" )) # Entrenar args = Namespace ( ** utils . load_dict ( filepath = args_fp )) artifacts = train . train ( df = df , args = args ) performance = artifacts [ \"performance\" ] print ( json . dumps ( performance , indent = 2 )) Agregaremos m\u00e1s a nuestra operaci\u00f3n `train_model()` cuando tengamos en cuenta el seguimiento de experimentos pero, por ahora, es bastante simple. # src/train.py from imblearn.over_sampling import RandomOverSampler import json import numpy as np import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import log_loss from src import data , predict , utils , evaluate def train ( args , df , trial = None ): \"\"\"Entrenar modelo en datos.\"\"\" # Setup utils . set_seeds () if args . shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) df = df [: args . subset ] # Ninguno = todas las muestras df = data . preprocess ( df , lower = args . lower , stem = args . stem , min_freq = args . min_freq ) label_encoder = data . LabelEncoder () . fit ( df . tag ) X_train , X_val , X_test , y_train , y_val , y_test = \\ data . get_data_splits ( X = df . text . to_numpy (), y = label_encoder . encode ( df . tag )) test_df = pd . DataFrame ({ \"text\" : X_test , \"tag\" : label_encoder . decode ( y_test )}) # Tf-idf vectorizer = TfidfVectorizer ( analyzer = args . analyzer , ngram_range = ( 2 , args . ngram_max_range )) # char n-grams X_train = vectorizer . fit_transform ( X_train ) X_val = vectorizer . transform ( X_val ) X_test = vectorizer . transform ( X_test ) # Oversample oversample = RandomOverSampler ( sampling_strategy = \"all\" ) X_over , y_over = oversample . fit_resample ( X_train , y_train ) # Model model = SGDClassifier ( loss = \"log\" , penalty = \"l2\" , alpha = args . alpha , max_iter = 1 , learning_rate = \"constant\" , eta0 = args . learning_rate , power_t = args . power_t , warm_start = True ) # Training for epoch in range ( args . num_epochs ): model . fit ( X_over , y_over ) train_loss = log_loss ( y_train , model . predict_proba ( X_train )) val_loss = log_loss ( y_val , model . predict_proba ( X_val )) if not epoch % 10 : print ( f \"Epoch: { epoch : 02d } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } \" ) # Threshold y_pred = model . predict ( X_val ) y_prob = model . predict_proba ( X_val ) args . threshold = np . quantile ( [ y_prob [ i ][ j ] for i , j in enumerate ( y_pred )], q = 0.25 ) # Q1 # Evaluacion other_index = label_encoder . class_to_index [ \"other\" ] y_prob = model . predict_proba ( X_test ) y_pred = predict . custom_predict ( y_prob = y_prob , threshold = args . threshold , index = other_index ) performance = evaluate . get_metrics ( y_true = y_test , y_pred = y_pred , classes = label_encoder . classes , df = test_df ) return { \"args\" : args , \"label_encoder\" : label_encoder , \"vectorizer\" : vectorizer , \"model\" : model , \"performance\" : performance , } Esta funci\u00f3n `train()` llama a dos funciones externas (`predict.custom_predict()` de `predict.py` y `Evaluation.get_metrics()` de `Evaluation.py`): # src/predict.py import numpy as np def custom_predict ( y_prob , threshold , index ): \"\"\"Funci\u00f3n de predicci\u00f3n personalizada que por defecto es un \u00edndice si no se cumplen las condiciones.\"\"\" y_pred = [ np . argmax ( p ) if max ( p ) > threshold else index for p in y_prob ] return np . array ( y_pred ) # src/evaluate.py import numpy as np from sklearn.metrics import precision_recall_fscore_support from snorkel.slicing import PandasSFApplier from snorkel.slicing import slicing_function @slicing_function () def nlp_cnn ( x ): \"\"\"Proyectos de NLP que utilizan convoluci\u00f3n.\"\"\" nlp_projects = \"natural-language-processing\" in x . tag convolution_projects = \"CNN\" in x . text or \"convolution\" in x . text return ( nlp_projects and convolution_projects ) @slicing_function () def short_text ( x ): \"\"\"Proyectos con t\u00edtulos y descripciones cortos.\"\"\" return len ( x . text . split ()) < 8 # menos de 8 palabras def get_slice_metrics ( y_true , y_pred , slices ): \"\"\"Generar m\u00e9tricas para segmentos de datos.\"\"\" metrics = {} for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): slice_metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) metrics [ slice_name ] = {} metrics [ slice_name ][ \"precision\" ] = slice_metrics [ 0 ] metrics [ slice_name ][ \"recall\" ] = slice_metrics [ 1 ] metrics [ slice_name ][ \"f1\" ] = slice_metrics [ 2 ] metrics [ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ]) return metrics def get_metrics ( y_true , y_pred , classes , df = None ): \"\"\"M\u00e9tricas de rendimiento utilizando verdades y predicciones.\"\"\" # Performance metrics = { \"overall\" : {}, \"class\" : {}} # M\u00e9tricas generales overall_metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) metrics [ \"overall\" ][ \"precision\" ] = overall_metrics [ 0 ] metrics [ \"overall\" ][ \"recall\" ] = overall_metrics [ 1 ] metrics [ \"overall\" ][ \"f1\" ] = overall_metrics [ 2 ] metrics [ \"overall\" ][ \"num_samples\" ] = np . float64 ( len ( y_true )) # M\u00e9tricas por clase class_metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i , _class in enumerate ( classes ): metrics [ \"class\" ][ _class ] = { \"precision\" : class_metrics [ 0 ][ i ], \"recall\" : class_metrics [ 1 ][ i ], \"f1\" : class_metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( class_metrics [ 3 ][ i ]), } # M\u00e9tricas de secciones if df is not None : slices = PandasSFApplier ([ nlp_cnn , short_text ]) . apply ( df ) metrics [ \"slices\" ] = get_slice_metrics ( y_true = y_true , y_pred = y_pred , slices = slices ) return metrics Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install imbalanced-learn == 0 .8.1 snorkel == 0 .9.8 # Agregar a requirements.txt imbalanced-learn == 0 .8.1 snorkel == 0 .9.8 Comandos para entrenar un modelo: from pathlib import Path from config import config from src import main args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) main . train_model ( args_fp ) Puede ser que aparezca un mensaje como este: TypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc > = 3 .19.0. Para solucionar este error hay que hacer un downgrade de protobuf: python -m pip install protobuf == 3 .20.* # Agregar a requirements.txt protobuf == 3 .20.* Luego vuelva a ejecutar los comandos para entrenar el modelo.","title":"Entrenamiento"},{"location":"Desarrollo/Organizaci%C3%B3n/#optimizacion","text":"Optimizar argumentos Ahora que podemos entrenar un modelo, estamos listos para entrenar muchos modelos para optimizar nuestros hiperpar\u00e1metros: # src/main.py import mlflow from numpyencoder import NumpyEncoder import optuna from optuna.integration.mlflow import MLflowCallback def optimize ( args_fp , study_name , num_trials ): \"\"\"Optimizar hiperpar\u00e1metros.\"\"\" # Cargar datos etiquetados df = pd . read_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" )) # Optimizar args = Namespace ( ** utils . load_dict ( filepath = args_fp )) pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = \"optimization\" , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( args , df , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ]) # Mejor prueba trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"user_attrs_f1\" ], ascending = False ) utils . save_dict ({ ** args . __dict__ , ** study . best_trial . params }, args_fp , cls = NumpyEncoder ) print ( f \" \\n Mejor valor (f1): { study . best_trial . value } \" ) print ( f \"Mejores hiperpar\u00e1metros: { json . dumps ( study . best_trial . params , indent = 2 ) } \" ) Definiremos la funci\u00f3n `objective()` dentro de `train.py`: # src/train.py def objective ( args , df , trial ): \"\"\"Funci\u00f3n objetivo para pruebas de optimizaci\u00f3n.\"\"\" # Par\u00e1metros a tunear args . analyzer = trial . suggest_categorical ( \"analyzer\" , [ \"word\" , \"char\" , \"char_wb\" ]) args . ngram_max_range = trial . suggest_int ( \"ngram_max_range\" , 3 , 10 ) args . learning_rate = trial . suggest_loguniform ( \"learning_rate\" , 1e-2 , 1e0 ) args . power_t = trial . suggest_uniform ( \"power_t\" , 0.1 , 0.5 ) # Entrenar & evaluatar artifacts = train ( args = args , df = df , trial = trial ) # Establecer atributos adicionales overall_performance = artifacts [ \"performance\" ][ \"overall\" ] print ( json . dumps ( overall_performance , indent = 2 )) trial . set_user_attr ( \"precision\" , overall_performance [ \"precision\" ]) trial . set_user_attr ( \"recall\" , overall_performance [ \"recall\" ]) trial . set_user_attr ( \"f1\" , overall_performance [ \"f1\" ]) return overall_performance [ \"f1\" ] Recuerde que en nuestro notebook modificamos la funci\u00f3n `train()` para incluir informaci\u00f3n sobre las pruebas durante la optimizaci\u00f3n para pruning: # src/train.py import optuna def train (): ... # Entrenamiento for epoch in range ( args . num_epochs ): ... # Pruning (para optimizaci\u00f3n en la siguiente secci\u00f3n) if trial : trial . report ( val_loss , epoch ) if trial . should_prune (): raise optuna . TrialPruned () Dado que estamos usando `MLflowCallback` aqu\u00ed con Optuna, podemos permitir que todos nuestros experimentos se almacenen en el directorio `mlruns` predeterminado que crear\u00e1 MLflow o podemos configurar esa ubicaci\u00f3n: # config/config.py import mlflow # Directorios BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) # Stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) # Crear carpetas DATA_DIR . mkdir ( parents = True , exist_ok = True ) MODEL_REGISTRY . mkdir ( parents = True , exist_ok = True ) # Registro del modelo MLFlow mlflow . set_tracking_uri ( \"file: \\\\ \" + str ( MODEL_REGISTRY . absolute ())) Instale los paquetes requeridos y agr\u00e9guelos a `requirements.txt`: python -m pip install mlflow == 1 .23.1 optuna == 2 .10.0 numpyencoder == 0 .3.0 # Agregar a requirements.txt mlflow == 1 .23.1 numpyencoder == 0 .3.0 optuna == 2 .10.0 Comandos para optimizar hiperpar\u00e1metros: from pathlib import Path from config import config from src import main args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) main . optimize ( args_fp , study_name = \"optimization\" , num_trials = 20 )","title":"Optimizaci\u00f3n"},{"location":"Desarrollo/Organizaci%C3%B3n/#seguimiento-de-experimentos","text":"Seguimiento de experimentos Ahora que tenemos nuestros hiperpar\u00e1metros optimizados, podemos entrenar un modelo y almacenar sus artefactos a trav\u00e9s del seguimiento de experimentos. Comenzaremos modificando la operaci\u00f3n `train()` en nuestro script `main.py`: # src/main.py import joblib import tempfile def train_model ( args_fp , experiment_name , run_name ): \"\"\"Entrenar un modelo con argumentos dados.\"\"\" # Cargar datos etiquetados df = pd . read_csv ( Path ( config . DATA_DIR , \"labeled_projects.csv\" )) # Entrenar args = Namespace ( ** utils . load_dict ( filepath = args_fp )) mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id print ( f \"Run ID: { run_id } \" ) artifacts = train . train ( df = df , args = args ) performance = artifacts [ \"performance\" ] print ( json . dumps ( performance , indent = 2 )) # Logeo de m\u00e9tricas y par\u00e1metros performance = artifacts [ \"performance\" ] mlflow . log_metrics ({ \"precision\" : performance [ \"overall\" ][ \"precision\" ]}) mlflow . log_metrics ({ \"recall\" : performance [ \"overall\" ][ \"recall\" ]}) mlflow . log_metrics ({ \"f1\" : performance [ \"overall\" ][ \"f1\" ]}) mlflow . log_params ( vars ( artifacts [ \"args\" ])) # Logeo de artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"args\" ]), Path ( dp , \"args.json\" ), cls = NumpyEncoder ) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) joblib . dump ( artifacts [ \"vectorizer\" ], Path ( dp , \"vectorizer.pkl\" )) joblib . dump ( artifacts [ \"model\" ], Path ( dp , \"model.pkl\" )) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) mlflow . log_artifacts ( dp ) # Guardar en config open ( Path ( config . CONFIG_DIR , \"run_id.txt\" ), \"w\" ) . write ( run_id ) utils . save_dict ( performance , Path ( config . CONFIG_DIR , \"performance.json\" )) Tambi\u00e9n vamos a actualizar la funci\u00f3n `train()` dentro de `train.py` para que se capturen las m\u00e9tricas intermedias: # src/train.py import mlflow def train (): ... # Training for epoch in range ( args . num_epochs ): ... # Log if not trial : mlflow . log_metrics ({ \"train_loss\" : train_loss , \"val_loss\" : val_loss }, step = epoch ) Comandos para entrenar un modelo con seguimiento de experimentos: from pathlib import Path from config import config from src import main args_fp = Path ( config . CONFIG_DIR , \"args.json\" ) main . train_model ( args_fp , experiment_name = \"baselines\" , run_name = \"sgd\" ) Nuestro directorio de configuraci\u00f3n ahora deber\u00eda tener un archivo performance.json y un archivo run_id.txt. Los estamos guardando para poder acceder r\u00e1pidamente a estos metadatos del \u00faltimo entrenamiento exitoso.","title":"Seguimiento de experimentos"},{"location":"Desarrollo/Organizaci%C3%B3n/#prediccion","text":"Predecir textos Finalmente estamos listos para usar nuestro modelo entrenado para la inferencia. Agregaremos la operaci\u00f3n para predecir una etiqueta a `main.py`: # src/main.py from src import data , predict , train , utils def predict_tag ( text , run_id = None ): \"\"\"Predecir etiqueta para texto.\"\"\" if not run_id : run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () artifacts = load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) print ( json . dumps ( prediction , indent = 2 )) return prediction Esto implica crear la funci\u00f3n `load_artifacts()` dentro de nuestro script `main.py`: # src/main.py def load_artifacts ( run_id ): \"\"\"Cargar artefactos para un run_id determinado.\"\"\" # Localizar el directorio espec\u00edficos de los artefactos experiment_id = mlflow . get_run ( run_id = run_id ) . info . experiment_id artifacts_dir = Path ( config . MODEL_REGISTRY , experiment_id , run_id , \"artifacts\" ) # Cargar objetos desde la ejecuci\u00f3n args = Namespace ( ** utils . load_dict ( filepath = Path ( artifacts_dir , \"args.json\" ))) vectorizer = joblib . load ( Path ( artifacts_dir , \"vectorizer.pkl\" )) label_encoder = data . LabelEncoder . load ( fp = Path ( artifacts_dir , \"label_encoder.json\" )) model = joblib . load ( Path ( artifacts_dir , \"model.pkl\" )) performance = utils . load_dict ( filepath = Path ( artifacts_dir , \"performance.json\" )) return { \"args\" : args , \"label_encoder\" : label_encoder , \"vectorizer\" : vectorizer , \"model\" : model , \"performance\" : performance } Y definir la funci\u00f3n `predict()` dentro de `predict.py`: def predict ( texts , artifacts ): \"\"\"Predecir etiquetas para textos dados.\"\"\" x = artifacts [ \"vectorizer\" ] . transform ( texts ) y_pred = custom_predict ( y_prob = artifacts [ \"model\" ] . predict_proba ( x ), threshold = artifacts [ \"args\" ] . threshold , index = artifacts [ \"label_encoder\" ] . class_to_index [ \"other\" ]) tags = artifacts [ \"label_encoder\" ] . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"predicted_tag\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions Comandos para predecir la etiqueta de texto: from pathlib import Path from config import config from src import main text = \"Transfer learning with transformers for text classification.\" run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () main . predict_tag ( text = text , run_id = run_id ) En la secci\u00f3n de documentaci\u00f3n veremos como formatear nuestras funciones y clases correctamente.","title":"Predicci\u00f3n"},{"location":"Dise%C3%B1o/Ingenier%C3%ADa/","text":"","title":"Ingenier\u00eda"},{"location":"Dise%C3%B1o/Producto/","text":"","title":"Producto"},{"location":"Dise%C3%B1o/Proyecto/","text":"","title":"Proyecto"},{"location":"Inferencia/API%20RESTful/","text":"APIs para la publicaci\u00f3n de modelos Dise\u00f1o e implementaci\u00f3n de una API para servir modelos de machine learning. Introducci\u00f3n Existen varias limitaciones para servir nuestros modelos con una CLI: los usuarios necesitan acceso a la terminal, c\u00f3digo base, entorno virtual, etc. las salidas CLI en el terminal no son exportables. Para abordar estos problemas, vamos a desarrollar una interfaz de programaci\u00f3n de aplicaciones (API) que permitir\u00e1 que cualquier persona interact\u00fae con nuestra aplicaci\u00f3n con una simple solicitud. Es posible que el usuario final no interact\u00fae directamente con nuestra API, pero puede usar componentes de UI/UX que env\u00eden solicitudes a nuestra API. Servicio Las API permiten que diferentes aplicaciones se comuniquen entre s\u00ed en tiempo real. Pero primero debemos decidir si lo haremos en lotes o en tiempo real. Servicio por lotes Podemos hacer predicciones por lotes en un conjunto finito de entradas que luego se escriben en una base de datos para una inferencia de baja latencia. Servicio en tiempo real Tambi\u00e9n podemos ofrecer predicciones en vivo, generalmente a trav\u00e9s de un request a una API con los datos de entrada adecuados. Request Los usuarios interactuar\u00e1n con nuestra API en forma de un request. Los diferentes componentes de un request son: URI (Uniform Resource Identifier) Es un identificador para un recurso espec\u00edfico. Ej.: https://localhost:8000/models/{modelId}/?filter=passed#details M\u00e9todo Es la operaci\u00f3n a ejecutar sobre el recurso espec\u00edfico definido por la URI. Los cuatro metodos a continuaci\u00f3n son los m\u00e1s populares, que a menudo se denominan CRUD porque le permiten crear, leer, actualizar y eliminar. GET: obtener un recurso. POST: crear o actualizar un recurso. PUT/PATCH: crear o actualizar un recurso. DELETE: eliminar un recurso. La principal diferencia entre POST y PUT es que PUT es idempotente, lo que significa que puede llamar al m\u00e9todo repetidamente y producir\u00e1 el mismo estado cada vez. Podemos usar cURL para ejecutar nuestras llamadas API. curl -X GET \"http://localhost:8000/models\" Headers Contienen informaci\u00f3n sobre un determinado evento y generalmente se encuentran tanto en el request del cliente como en la respuesta del servidor. Puede variar desde qu\u00e9 tipo de formato enviar\u00e1n y recibir\u00e1n, informaci\u00f3n de autenticaci\u00f3n y almacenamiento en cach\u00e9, etc. curl -X GET \"http://localhost:8000/\" \\ # method and URI -H \"accept: application/json\" \\ # client accepts JSON -H \"Content-Type: application/json\" \\ # client sends JSON Body Contiene informaci\u00f3n que puede ser necesaria para que se procese la solicitud. Por lo general, es un objeto JSON enviado durante los m\u00e9todos de solicitud POST, PUT/PATCH, DELETE. curl -X POST \"http://localhost:8000/models\" \\ # method and URI -H \"accept: application/json\" \\ # client accepts JSON -H \"Content-Type: application/json\" \\ # client sends JSON -d \"{'name': 'RoBERTa', ...}\" # request body Response La respuesta es el resultado de la solicitud que enviamos. Tambi\u00e9n incluye encabezados y un cuerpo que debe incluir el c\u00f3digo de estado HTTP adecuado, as\u00ed como mensajes expl\u00edcitos, datos, etc. { \"message\" : \"OK\" , \"method\" : \"GET\" , \"status-code\" : 200 , \"url\" : \"http://localhost:8000/\" , \"data\" : {} } Mejores pr\u00e1cticas Al dise\u00f1ar nuestra API, hay algunas mejores pr\u00e1cticas a seguir: Las rutas de URI, los mensajes, etc. deben ser lo m\u00e1s expl\u00edcitos posible. Use sustantivos, en lugar de verbos, para nombrar los recursos. (\u2705 GET /users not \u274c GET /get_users). Sustantivos en plural (\u2705 GET /users/{userId} no \u274c GET /user/{userID}). Utilice guiones en URI para recursos y par\u00e1metros de ruta, pero use guiones bajos para par\u00e1metros de consulta (GET /nlp-models/?find_desc=bert). Devolver mensajes HTTP e informativos apropiados al usuario. Aplicaci\u00f3n Vamos a definir nuestra API en un directorio de aplicaciones separado. Dentro de nuestro directorio de aplicaciones, crearemos los siguientes scripts: mkdir app cd app touch api.py gunicorn.py schemas.py cd ../ api.py: el script principal que incluir\u00e1 la inicializaci\u00f3n y los endpoints de nuestra API. gunicorn.py: script para definir las configuraciones de trabajo de la API. schemas.py: definiciones para los diferentes objetos que usaremos en nuestros endpoints de recursos. FastAPI Vamos a utilizar FastAPI como nuestro framework para construir nuestro servicio de API. Las ventajas de FastAPI incluyen: desarrollo en python de alto rendimiento validaci\u00f3n de datos a trav\u00e9s de pydantic documentaci\u00f3n generada autom\u00e1ticamente inyecci\u00f3n de dependencia seguridad a trav\u00e9s de OAuth2 python -m pip install fastapi == 0 .78.0 # Agregar a requirements.txt fastapi == 0 .78.0 Inicializaci\u00f3n El primer paso es inicializar nuestra API en nuestro script api.py definiendo metadatos como el t\u00edtulo, la descripci\u00f3n y la versi\u00f3n: # app/api.py from fastapi import FastAPI # Definir aplicaci\u00f3n app = FastAPI ( title = \"CoE MLOps Template\" , description = \"Clasificaci\u00f3n de proyectos de machine learning.\" , version = \"0.1\" , ) Nuestro primer endpoint ser\u00e1 uno simple en el que queremos mostrar que todo funciona seg\u00fan lo previsto. La ruta para el endpoint ser\u00e1 / y ser\u00e1 una solicitud GET . # app/api.py from http import HTTPStatus from typing import Dict @app . get ( \"/\" ) def _index () -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response Lanzamiento Usamos Uvicorn, un servidor ASGI r\u00e1pido que puede ejecutar c\u00f3digo as\u00edncrono en un solo proceso para iniciar nuestra aplicaci\u00f3n. python -m pip install uvicorn == 0 .17.6 # Agregar a requirements.txt uvicorn == 0 .17.6 Podemos lanzar nuestra aplicaci\u00f3n con el siguiente comando: uvicorn app.api:app \\ # ubicaci\u00f3n de la aplicaci\u00f3n --host 0 .0.0.0 \\ # localhost --port 8000 \\ # puerto 8000 --reload \\ # recargar cada vez que actualizamos --reload-dir coe_template \\ # solo recargar en las actualizaciones del directorio `coe_template` --reload-dir app # y el directorio `app` Si queremos administrar m\u00faltiples workers de uvicorn para habilitar el paralelismo en nuestra aplicaci\u00f3n, podemos usar Gunicorn junto con Uvicorn. Podemos iniciar todos los workers con el siguiente comando: gunicorn -c config/gunicorn.py -k uvicorn.workers.UvicornWorker app.api:app Tambi\u00e9n agregaremos estos dos comandos a nuestro archivo README.md: uvicorn app.api:app --host 0 .0.0.0 --port 8000 --reload --reload-dir coe_template --reload-dir app # dev gunicorn -c app/gunicorn.py -k uvicorn.workers.UvicornWorker app.api:app # prod Requests Podemos enviar nuestra solicitud GET usando varios m\u00e9todos diferentes: Visitar el endpoint en un navegador en http://localhost:8000/ cURL: curl -X GET http://localhost:8000/ Acceder a los endpoints a trav\u00e9s del c\u00f3digo: import json import requests response = requests . get ( \"http://localhost:8000/\" ) print ( json . loads ( response . text )) Usar herramientas externas como Postman Decoradores Usemos decoradores para agregar autom\u00e1ticamente metadatos relevantes a nuestras respuestas # app/api.py from datetime import datetime from functools import wraps from fastapi import FastAPI , Request def construct_response ( f ): \"\"\"Construir una respuesta JSON para un endpoint.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ) -> Dict : results = f ( request , * args , ** kwargs ) response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap Estamos pasando una instancia de Request para que podamos acceder a informaci\u00f3n como el m\u00e9todo request y la URL. Por lo tanto, nuestras funciones endpoint tambi\u00e9n deben tener este objeto Request como argumento de entrada. Una vez que recibimos los resultados de nuestra funci\u00f3n f, podemos agregar los detalles adicionales y devolver una respuesta m\u00e1s informativa. Para usar este decorador, solo tenemos que envolver nuestras funciones. @app . get ( \"/\" ) @construct_response def _index ( request : Request ) -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response Tambi\u00e9n hay algunos decoradores incorporados que debemos tener en cuenta, como el decorador de eventos (@app.on_event()) que podemos usar para iniciar y cerrar nuestra aplicaci\u00f3n. from pathlib import Path from config import config from config.config import logger from coe_template import main @app . on_event ( \"startup\" ) def load_artifacts (): global artifacts run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () artifacts = main . load_artifacts ( run_id = run_id ) logger . info ( \"Listo para la inferencia!\" ) Documentaci\u00f3n Cuando definimos un endpoint, FastAPI genera autom\u00e1ticamente cierta documentaci\u00f3n (se adhiere a los est\u00e1ndares de OpenAPI) en funci\u00f3n de sus entradas, escritura, salidas, etc. Podemos acceder a la interfaz de usuario de Swagger para nuestra documentaci\u00f3n yendo al endpoint /docs. Tenga en cuenta que nuestro endpoint est\u00e1 organizado en secciones en la interfaz de usuario. Podemos usar etiquetas al definir nuestros endpoints en el script: @app . get ( \"/\" , tags = [ \"General\" ]) @construct_response def _index ( request : Request ) -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response Recursos Al dise\u00f1ar los recursos para nuestra API, debemos pensar en las siguientes preguntas: [USUARIOS]: \u00bfQui\u00e9nes son los usuarios finales? Esto definir\u00e1 qu\u00e9 recursos deben exponerse. desarrolladores que quieran interactuar con la API. equipo del producto que quiere probar e inspeccionar el modelo y su rendimiento. servicio backend que quiere clasificar los proyectos entrantes. [ACCIONES]: \u00bfQu\u00e9 acciones quieren poder realizar nuestros usuarios? predicci\u00f3n para un conjunto dado de entradas inspecci\u00f3n de performance inspecci\u00f3n de argumentos de entrenamiento Par\u00e1metros de consulta Podemos pasar un filtro de par\u00e1metro de consulta opcional para indicar el subconjunto de rendimiento que nos interesa. @app . get ( \"/performance\" , tags = [ \"Performance\" ]) @construct_response def _performance ( request : Request , filter : str = None ) -> Dict : \"\"\"Obtener las m\u00e9tricas de performance.\"\"\" performance = artifacts [ \"performance\" ] data = { \"performance\" : performance . get ( filter , performance )} response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : data , } return response Podemos incluir este par\u00e1metro en nuestra solicitud GET as\u00ed: curl -X \"GET\" \\ \"http://localhost:8000/performance?filter=overall\" \\ -H \"accept: application/json\" Par\u00e1metros de path En el siguiente endpoint obtenemos los argumentos utilizados para entrenar el modelo. Esta vez, usamos un par\u00e1metro de ruta args, que es un campo obligatorio en el URI. @app . get ( \"/args/ {arg} \" , tags = [ \"Arguments\" ]) @construct_response def _arg ( request : Request , arg : str ) -> Dict : \"\"\"Obtener el valor de un par\u00e1metro espec\u00edfico utilizado para la ejecuci\u00f3n.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : { arg : vars ( artifacts [ \"args\" ]) . get ( arg , \"\" ), }, } return response Podemos realizar nuestra solicitud GET de esta manera: curl -X \"GET\" \\ \"http://localhost:8000/args/learning_rate\" \\ -H \"accept: application/json\" Tambi\u00e9n podemos crear un endpoint para producir todos los argumentos que se usaron: @app . get ( \"/args\" , tags = [ \"Arguments\" ]) @construct_response def _args ( request : Request ) -> Dict : \"\"\"Obtener todos los argumentos utilizados para la ejecuci\u00f3n.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : { \"args\" : vars ( artifacts [ \"args\" ]), }, } return response Podemos realizar nuestra solicitud GET de esta manera curl -X \"GET\" \\ \"http://localhost:8000/args\" \\ -H \"accept: application/json\" Schemas Ahora es el momento de definir nuestro endpoint para la predicci\u00f3n. Necesitamos consumir las entradas que queremos clasificar y, por lo tanto, debemos definir el esquema que debe seguirse al definir esas entradas. # app/schemas.py from typing import List from fastapi import Query from pydantic import BaseModel class Text ( BaseModel ): text : str = Query ( None , min_length = 1 ) class PredictPayload ( BaseModel ): texts : List [ Text ] Aqu\u00ed estamos definiendo un objeto PredictPayload como una lista de objetos de texto llamados texts . Cada objeto Text es una cadena cuyo valor predeterminado es None y debe tener una longitud m\u00ednima de 1 car\u00e1cter. Ahora podemos usar este payload en nuestro endpoint: from app.schemas import PredictPayload from coe_template import main , predict @app . post ( \"/predict\" , tags = [ \"Prediction\" ]) @construct_response def _predict ( request : Request , payload : PredictPayload ) -> Dict : \"\"\"Predecir etiquetas para una lista de textos.\"\"\" texts = [ item . text for item in payload . texts ] predictions = predict . predict ( texts = texts , artifacts = artifacts ) response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : { \"predictions\" : predictions }, } return response Necesitamos adherirnos al esquema PredictPayload cuando queremos usar nuestro /predict : curl -X 'POST' 'http://localhost:8000/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"texts\": [ {\"text\": \"Transfer learning with transformers for text classification.\"}, {\"text\": \"Generative adversarial networks for image generation.\"} ] }' Validaci\u00f3n Built-in Estamos usando el objeto BaseModel de pydantic aqu\u00ed porque ofrece una validaci\u00f3n integrada para todos nuestros esquemas. En nuestro caso, si una instancia de texto tiene menos de 1 car\u00e1cter, nuestro servicio devolver\u00e1 el c\u00f3digo y mensaje de error correspondientes. Custom Tambi\u00e9n podemos agregar una validaci\u00f3n personalizada en una entidad espec\u00edfica usando el decorador @validator, como lo hacemos para asegurarnos de que la lista de textos no est\u00e9 vac\u00eda. from pydantic import BaseModel , validator class PredictPayload ( BaseModel ): texts : List [ Text ] @validator ( \"texts\" ) def list_must_not_be_empty ( cls , value ): if not len ( value ): raise ValueError ( \"La lista de textos a clasificar no puede estar vac\u00eda.\" ) return value Extras Por \u00faltimo, podemos agregar un objeto schema_extra en una clase Config para representar c\u00f3mo deber\u00eda verse un PredictPayload de ejemplo. Cuando hacemos esto, aparece autom\u00e1ticamente en la documentaci\u00f3n de nuestro endpoint. class PredictPayload ( BaseModel ): texts : List [ Text ] @validator ( \"texts\" ) def list_must_not_be_empty ( cls , value ): if not len ( value ): raise ValueError ( \"La lista de textos a clasificar no puede estar vac\u00eda.\" ) return value class Config : schema_extra = { \"example\" : { \"texts\" : [ { \"text\" : \"Transfer learning with transformers for text classification.\" }, { \"text\" : \"Generative adversarial networks in both PyTorch and TensorFlow.\" }, ] } } Servidor de modelos Adem\u00e1s de envolver nuestros modelos como microservicios separados y escalables, tambi\u00e9n podemos tener un servidor de modelos especialmente dise\u00f1ado para alojar nuestros modelos. Los servidores de modelos proporcionan un registro con una capa de API para inspeccionar, actualizar, servir, revertir, etc. m\u00faltiples versiones de modelos. Tambi\u00e9n ofrecen escalado autom\u00e1tico para satisfacer las necesidades de rendimiento y latencia. Las opciones populares incluyen BentoML , MLFlow , TorchServe , RedisAI , Nvidia Triton Inference Server , etc. Los servidores de modelo est\u00e1n experimentando una gran adopci\u00f3n por su capacidad para estandarizar la implementaci\u00f3n del modelo y los procesos de servicio en todo el equipo, lo que permite actualizaciones, validaci\u00f3n e integraci\u00f3n sin inconvenientes.","title":"API RESTful"},{"location":"Inferencia/API%20RESTful/#apis-para-la-publicacion-de-modelos","text":"Dise\u00f1o e implementaci\u00f3n de una API para servir modelos de machine learning.","title":"APIs para la publicaci\u00f3n de modelos"},{"location":"Inferencia/API%20RESTful/#introduccion","text":"Existen varias limitaciones para servir nuestros modelos con una CLI: los usuarios necesitan acceso a la terminal, c\u00f3digo base, entorno virtual, etc. las salidas CLI en el terminal no son exportables. Para abordar estos problemas, vamos a desarrollar una interfaz de programaci\u00f3n de aplicaciones (API) que permitir\u00e1 que cualquier persona interact\u00fae con nuestra aplicaci\u00f3n con una simple solicitud. Es posible que el usuario final no interact\u00fae directamente con nuestra API, pero puede usar componentes de UI/UX que env\u00eden solicitudes a nuestra API.","title":"Introducci\u00f3n"},{"location":"Inferencia/API%20RESTful/#servicio","text":"Las API permiten que diferentes aplicaciones se comuniquen entre s\u00ed en tiempo real. Pero primero debemos decidir si lo haremos en lotes o en tiempo real.","title":"Servicio"},{"location":"Inferencia/API%20RESTful/#servicio-por-lotes","text":"Podemos hacer predicciones por lotes en un conjunto finito de entradas que luego se escriben en una base de datos para una inferencia de baja latencia.","title":"Servicio por lotes"},{"location":"Inferencia/API%20RESTful/#servicio-en-tiempo-real","text":"Tambi\u00e9n podemos ofrecer predicciones en vivo, generalmente a trav\u00e9s de un request a una API con los datos de entrada adecuados.","title":"Servicio en tiempo real"},{"location":"Inferencia/API%20RESTful/#request","text":"Los usuarios interactuar\u00e1n con nuestra API en forma de un request. Los diferentes componentes de un request son:","title":"Request"},{"location":"Inferencia/API%20RESTful/#uri-uniform-resource-identifier","text":"Es un identificador para un recurso espec\u00edfico. Ej.: https://localhost:8000/models/{modelId}/?filter=passed#details","title":"URI (Uniform Resource Identifier)"},{"location":"Inferencia/API%20RESTful/#metodo","text":"Es la operaci\u00f3n a ejecutar sobre el recurso espec\u00edfico definido por la URI. Los cuatro metodos a continuaci\u00f3n son los m\u00e1s populares, que a menudo se denominan CRUD porque le permiten crear, leer, actualizar y eliminar. GET: obtener un recurso. POST: crear o actualizar un recurso. PUT/PATCH: crear o actualizar un recurso. DELETE: eliminar un recurso. La principal diferencia entre POST y PUT es que PUT es idempotente, lo que significa que puede llamar al m\u00e9todo repetidamente y producir\u00e1 el mismo estado cada vez. Podemos usar cURL para ejecutar nuestras llamadas API. curl -X GET \"http://localhost:8000/models\"","title":"M\u00e9todo"},{"location":"Inferencia/API%20RESTful/#headers","text":"Contienen informaci\u00f3n sobre un determinado evento y generalmente se encuentran tanto en el request del cliente como en la respuesta del servidor. Puede variar desde qu\u00e9 tipo de formato enviar\u00e1n y recibir\u00e1n, informaci\u00f3n de autenticaci\u00f3n y almacenamiento en cach\u00e9, etc. curl -X GET \"http://localhost:8000/\" \\ # method and URI -H \"accept: application/json\" \\ # client accepts JSON -H \"Content-Type: application/json\" \\ # client sends JSON","title":"Headers"},{"location":"Inferencia/API%20RESTful/#body","text":"Contiene informaci\u00f3n que puede ser necesaria para que se procese la solicitud. Por lo general, es un objeto JSON enviado durante los m\u00e9todos de solicitud POST, PUT/PATCH, DELETE. curl -X POST \"http://localhost:8000/models\" \\ # method and URI -H \"accept: application/json\" \\ # client accepts JSON -H \"Content-Type: application/json\" \\ # client sends JSON -d \"{'name': 'RoBERTa', ...}\" # request body","title":"Body"},{"location":"Inferencia/API%20RESTful/#response","text":"La respuesta es el resultado de la solicitud que enviamos. Tambi\u00e9n incluye encabezados y un cuerpo que debe incluir el c\u00f3digo de estado HTTP adecuado, as\u00ed como mensajes expl\u00edcitos, datos, etc. { \"message\" : \"OK\" , \"method\" : \"GET\" , \"status-code\" : 200 , \"url\" : \"http://localhost:8000/\" , \"data\" : {} }","title":"Response"},{"location":"Inferencia/API%20RESTful/#mejores-practicas","text":"Al dise\u00f1ar nuestra API, hay algunas mejores pr\u00e1cticas a seguir: Las rutas de URI, los mensajes, etc. deben ser lo m\u00e1s expl\u00edcitos posible. Use sustantivos, en lugar de verbos, para nombrar los recursos. (\u2705 GET /users not \u274c GET /get_users). Sustantivos en plural (\u2705 GET /users/{userId} no \u274c GET /user/{userID}). Utilice guiones en URI para recursos y par\u00e1metros de ruta, pero use guiones bajos para par\u00e1metros de consulta (GET /nlp-models/?find_desc=bert). Devolver mensajes HTTP e informativos apropiados al usuario.","title":"Mejores pr\u00e1cticas"},{"location":"Inferencia/API%20RESTful/#aplicacion","text":"Vamos a definir nuestra API en un directorio de aplicaciones separado. Dentro de nuestro directorio de aplicaciones, crearemos los siguientes scripts: mkdir app cd app touch api.py gunicorn.py schemas.py cd ../ api.py: el script principal que incluir\u00e1 la inicializaci\u00f3n y los endpoints de nuestra API. gunicorn.py: script para definir las configuraciones de trabajo de la API. schemas.py: definiciones para los diferentes objetos que usaremos en nuestros endpoints de recursos.","title":"Aplicaci\u00f3n"},{"location":"Inferencia/API%20RESTful/#fastapi","text":"Vamos a utilizar FastAPI como nuestro framework para construir nuestro servicio de API. Las ventajas de FastAPI incluyen: desarrollo en python de alto rendimiento validaci\u00f3n de datos a trav\u00e9s de pydantic documentaci\u00f3n generada autom\u00e1ticamente inyecci\u00f3n de dependencia seguridad a trav\u00e9s de OAuth2 python -m pip install fastapi == 0 .78.0 # Agregar a requirements.txt fastapi == 0 .78.0","title":"FastAPI"},{"location":"Inferencia/API%20RESTful/#inicializacion","text":"El primer paso es inicializar nuestra API en nuestro script api.py definiendo metadatos como el t\u00edtulo, la descripci\u00f3n y la versi\u00f3n: # app/api.py from fastapi import FastAPI # Definir aplicaci\u00f3n app = FastAPI ( title = \"CoE MLOps Template\" , description = \"Clasificaci\u00f3n de proyectos de machine learning.\" , version = \"0.1\" , ) Nuestro primer endpoint ser\u00e1 uno simple en el que queremos mostrar que todo funciona seg\u00fan lo previsto. La ruta para el endpoint ser\u00e1 / y ser\u00e1 una solicitud GET . # app/api.py from http import HTTPStatus from typing import Dict @app . get ( \"/\" ) def _index () -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response","title":"Inicializaci\u00f3n"},{"location":"Inferencia/API%20RESTful/#lanzamiento","text":"Usamos Uvicorn, un servidor ASGI r\u00e1pido que puede ejecutar c\u00f3digo as\u00edncrono en un solo proceso para iniciar nuestra aplicaci\u00f3n. python -m pip install uvicorn == 0 .17.6 # Agregar a requirements.txt uvicorn == 0 .17.6 Podemos lanzar nuestra aplicaci\u00f3n con el siguiente comando: uvicorn app.api:app \\ # ubicaci\u00f3n de la aplicaci\u00f3n --host 0 .0.0.0 \\ # localhost --port 8000 \\ # puerto 8000 --reload \\ # recargar cada vez que actualizamos --reload-dir coe_template \\ # solo recargar en las actualizaciones del directorio `coe_template` --reload-dir app # y el directorio `app` Si queremos administrar m\u00faltiples workers de uvicorn para habilitar el paralelismo en nuestra aplicaci\u00f3n, podemos usar Gunicorn junto con Uvicorn. Podemos iniciar todos los workers con el siguiente comando: gunicorn -c config/gunicorn.py -k uvicorn.workers.UvicornWorker app.api:app Tambi\u00e9n agregaremos estos dos comandos a nuestro archivo README.md: uvicorn app.api:app --host 0 .0.0.0 --port 8000 --reload --reload-dir coe_template --reload-dir app # dev gunicorn -c app/gunicorn.py -k uvicorn.workers.UvicornWorker app.api:app # prod","title":"Lanzamiento"},{"location":"Inferencia/API%20RESTful/#requests","text":"Podemos enviar nuestra solicitud GET usando varios m\u00e9todos diferentes: Visitar el endpoint en un navegador en http://localhost:8000/ cURL: curl -X GET http://localhost:8000/ Acceder a los endpoints a trav\u00e9s del c\u00f3digo: import json import requests response = requests . get ( \"http://localhost:8000/\" ) print ( json . loads ( response . text )) Usar herramientas externas como Postman","title":"Requests"},{"location":"Inferencia/API%20RESTful/#decoradores","text":"Usemos decoradores para agregar autom\u00e1ticamente metadatos relevantes a nuestras respuestas # app/api.py from datetime import datetime from functools import wraps from fastapi import FastAPI , Request def construct_response ( f ): \"\"\"Construir una respuesta JSON para un endpoint.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ) -> Dict : results = f ( request , * args , ** kwargs ) response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap Estamos pasando una instancia de Request para que podamos acceder a informaci\u00f3n como el m\u00e9todo request y la URL. Por lo tanto, nuestras funciones endpoint tambi\u00e9n deben tener este objeto Request como argumento de entrada. Una vez que recibimos los resultados de nuestra funci\u00f3n f, podemos agregar los detalles adicionales y devolver una respuesta m\u00e1s informativa. Para usar este decorador, solo tenemos que envolver nuestras funciones. @app . get ( \"/\" ) @construct_response def _index ( request : Request ) -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response Tambi\u00e9n hay algunos decoradores incorporados que debemos tener en cuenta, como el decorador de eventos (@app.on_event()) que podemos usar para iniciar y cerrar nuestra aplicaci\u00f3n. from pathlib import Path from config import config from config.config import logger from coe_template import main @app . on_event ( \"startup\" ) def load_artifacts (): global artifacts run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () artifacts = main . load_artifacts ( run_id = run_id ) logger . info ( \"Listo para la inferencia!\" )","title":"Decoradores"},{"location":"Inferencia/API%20RESTful/#documentacion","text":"Cuando definimos un endpoint, FastAPI genera autom\u00e1ticamente cierta documentaci\u00f3n (se adhiere a los est\u00e1ndares de OpenAPI) en funci\u00f3n de sus entradas, escritura, salidas, etc. Podemos acceder a la interfaz de usuario de Swagger para nuestra documentaci\u00f3n yendo al endpoint /docs. Tenga en cuenta que nuestro endpoint est\u00e1 organizado en secciones en la interfaz de usuario. Podemos usar etiquetas al definir nuestros endpoints en el script: @app . get ( \"/\" , tags = [ \"General\" ]) @construct_response def _index ( request : Request ) -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response","title":"Documentaci\u00f3n"},{"location":"Inferencia/API%20RESTful/#recursos","text":"Al dise\u00f1ar los recursos para nuestra API, debemos pensar en las siguientes preguntas: [USUARIOS]: \u00bfQui\u00e9nes son los usuarios finales? Esto definir\u00e1 qu\u00e9 recursos deben exponerse. desarrolladores que quieran interactuar con la API. equipo del producto que quiere probar e inspeccionar el modelo y su rendimiento. servicio backend que quiere clasificar los proyectos entrantes. [ACCIONES]: \u00bfQu\u00e9 acciones quieren poder realizar nuestros usuarios? predicci\u00f3n para un conjunto dado de entradas inspecci\u00f3n de performance inspecci\u00f3n de argumentos de entrenamiento","title":"Recursos"},{"location":"Inferencia/API%20RESTful/#parametros-de-consulta","text":"Podemos pasar un filtro de par\u00e1metro de consulta opcional para indicar el subconjunto de rendimiento que nos interesa. @app . get ( \"/performance\" , tags = [ \"Performance\" ]) @construct_response def _performance ( request : Request , filter : str = None ) -> Dict : \"\"\"Obtener las m\u00e9tricas de performance.\"\"\" performance = artifacts [ \"performance\" ] data = { \"performance\" : performance . get ( filter , performance )} response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : data , } return response Podemos incluir este par\u00e1metro en nuestra solicitud GET as\u00ed: curl -X \"GET\" \\ \"http://localhost:8000/performance?filter=overall\" \\ -H \"accept: application/json\"","title":"Par\u00e1metros de consulta"},{"location":"Inferencia/API%20RESTful/#parametros-de-path","text":"En el siguiente endpoint obtenemos los argumentos utilizados para entrenar el modelo. Esta vez, usamos un par\u00e1metro de ruta args, que es un campo obligatorio en el URI. @app . get ( \"/args/ {arg} \" , tags = [ \"Arguments\" ]) @construct_response def _arg ( request : Request , arg : str ) -> Dict : \"\"\"Obtener el valor de un par\u00e1metro espec\u00edfico utilizado para la ejecuci\u00f3n.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : { arg : vars ( artifacts [ \"args\" ]) . get ( arg , \"\" ), }, } return response Podemos realizar nuestra solicitud GET de esta manera: curl -X \"GET\" \\ \"http://localhost:8000/args/learning_rate\" \\ -H \"accept: application/json\" Tambi\u00e9n podemos crear un endpoint para producir todos los argumentos que se usaron: @app . get ( \"/args\" , tags = [ \"Arguments\" ]) @construct_response def _args ( request : Request ) -> Dict : \"\"\"Obtener todos los argumentos utilizados para la ejecuci\u00f3n.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : { \"args\" : vars ( artifacts [ \"args\" ]), }, } return response Podemos realizar nuestra solicitud GET de esta manera curl -X \"GET\" \\ \"http://localhost:8000/args\" \\ -H \"accept: application/json\"","title":"Par\u00e1metros de path"},{"location":"Inferencia/API%20RESTful/#schemas","text":"Ahora es el momento de definir nuestro endpoint para la predicci\u00f3n. Necesitamos consumir las entradas que queremos clasificar y, por lo tanto, debemos definir el esquema que debe seguirse al definir esas entradas. # app/schemas.py from typing import List from fastapi import Query from pydantic import BaseModel class Text ( BaseModel ): text : str = Query ( None , min_length = 1 ) class PredictPayload ( BaseModel ): texts : List [ Text ] Aqu\u00ed estamos definiendo un objeto PredictPayload como una lista de objetos de texto llamados texts . Cada objeto Text es una cadena cuyo valor predeterminado es None y debe tener una longitud m\u00ednima de 1 car\u00e1cter. Ahora podemos usar este payload en nuestro endpoint: from app.schemas import PredictPayload from coe_template import main , predict @app . post ( \"/predict\" , tags = [ \"Prediction\" ]) @construct_response def _predict ( request : Request , payload : PredictPayload ) -> Dict : \"\"\"Predecir etiquetas para una lista de textos.\"\"\" texts = [ item . text for item in payload . texts ] predictions = predict . predict ( texts = texts , artifacts = artifacts ) response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : { \"predictions\" : predictions }, } return response Necesitamos adherirnos al esquema PredictPayload cuando queremos usar nuestro /predict : curl -X 'POST' 'http://localhost:8000/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"texts\": [ {\"text\": \"Transfer learning with transformers for text classification.\"}, {\"text\": \"Generative adversarial networks for image generation.\"} ] }'","title":"Schemas"},{"location":"Inferencia/API%20RESTful/#validacion","text":"Built-in Estamos usando el objeto BaseModel de pydantic aqu\u00ed porque ofrece una validaci\u00f3n integrada para todos nuestros esquemas. En nuestro caso, si una instancia de texto tiene menos de 1 car\u00e1cter, nuestro servicio devolver\u00e1 el c\u00f3digo y mensaje de error correspondientes. Custom Tambi\u00e9n podemos agregar una validaci\u00f3n personalizada en una entidad espec\u00edfica usando el decorador @validator, como lo hacemos para asegurarnos de que la lista de textos no est\u00e9 vac\u00eda. from pydantic import BaseModel , validator class PredictPayload ( BaseModel ): texts : List [ Text ] @validator ( \"texts\" ) def list_must_not_be_empty ( cls , value ): if not len ( value ): raise ValueError ( \"La lista de textos a clasificar no puede estar vac\u00eda.\" ) return value","title":"Validaci\u00f3n"},{"location":"Inferencia/API%20RESTful/#extras","text":"Por \u00faltimo, podemos agregar un objeto schema_extra en una clase Config para representar c\u00f3mo deber\u00eda verse un PredictPayload de ejemplo. Cuando hacemos esto, aparece autom\u00e1ticamente en la documentaci\u00f3n de nuestro endpoint. class PredictPayload ( BaseModel ): texts : List [ Text ] @validator ( \"texts\" ) def list_must_not_be_empty ( cls , value ): if not len ( value ): raise ValueError ( \"La lista de textos a clasificar no puede estar vac\u00eda.\" ) return value class Config : schema_extra = { \"example\" : { \"texts\" : [ { \"text\" : \"Transfer learning with transformers for text classification.\" }, { \"text\" : \"Generative adversarial networks in both PyTorch and TensorFlow.\" }, ] } }","title":"Extras"},{"location":"Inferencia/API%20RESTful/#servidor-de-modelos","text":"Adem\u00e1s de envolver nuestros modelos como microservicios separados y escalables, tambi\u00e9n podemos tener un servidor de modelos especialmente dise\u00f1ado para alojar nuestros modelos. Los servidores de modelos proporcionan un registro con una capa de API para inspeccionar, actualizar, servir, revertir, etc. m\u00faltiples versiones de modelos. Tambi\u00e9n ofrecen escalado autom\u00e1tico para satisfacer las necesidades de rendimiento y latencia. Las opciones populares incluyen BentoML , MLFlow , TorchServe , RedisAI , Nvidia Triton Inference Server , etc. Los servidores de modelo est\u00e1n experimentando una gran adopci\u00f3n por su capacidad para estandarizar la implementaci\u00f3n del modelo y los procesos de servicio en todo el equipo, lo que permite actualizaciones, validaci\u00f3n e integraci\u00f3n sin inconvenientes.","title":"Servidor de modelos"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/","text":"Aplicaciones CLI (Command Line Interface) Uso de una aplicaci\u00f3n CLI (command line interface) para organizar los procesos de la aplicaci\u00f3n. Introducci\u00f3n Cuando se trata de servir nuestros modelos, debemos pensar en exponer la funcionalidad de la aplicaci\u00f3n a nosotros mismos, a los miembros del equipo y, en \u00faltima instancia, a nuestros usuarios finales. Y las interfaces para lograr esto ser\u00e1n diferentes. En las secciones anteriores ejecutamos nuestras operaciones principales a trav\u00e9s de la terminal y el int\u00e9rprete de Python. Esto se vuelve extremadamente tedioso al tener que profundizar en el c\u00f3digo y ejecutar funciones una a la vez. Una soluci\u00f3n es crear una aplicaci\u00f3n de interfaz de l\u00ednea de comandos (CLI) que permita la interacci\u00f3n a nivel operativo. Debe dise\u00f1arse de tal manera que podamos ver todas las operaciones posibles (y sus argumentos requeridos) y ejecutarlas desde el shell. Aplicaci\u00f3n Vamos a crear nuestra CLI usando Typer: python -m pip install typer == 0 .4.1 # Agregar a requirements.txt typer == 0 .4.1 Es tan simple como inicializar la aplicaci\u00f3n y luego agregar el decorador apropiado para cada operaci\u00f3n de funci\u00f3n que deseamos usar como un comando CLI en nuestro main.py : # coe_template/main.py import typer # Inicializar la aplicaci\u00f3n CLI de Typer app = typer . Typer () @app . command () def elt_data (): ... Repetiremos lo mismo para todas las dem\u00e1s funciones a las que queremos acceder a trav\u00e9s de la CLI: elt_data(), train_model(), optimize(), predict_tag(). Haremos que todos los argumentos sean opcionales para que podamos definirlos expl\u00edcitamente en nuestros comandos bash. Ver encabezados de funci\u00f3n coe_template/main.py @app . command () def elt_data (): ... @app . command () def train_model ( args_fp : str = \"config/args.json\" , experiment_name : str = \"baselines\" , run_name : str = \"sgd\" , ) -> None : ... @app . command () def optimize ( args_fp : str = \"config/args.json\" , study_name : str = \"optimization\" , num_trials : int = 20 ) -> None : ... @app . command () def predict_tag ( text : str = \"\" , run_id : str = None ) -> None : ... Al final del archivo debemos inicializar la aplicaci\u00f3n CLI: if __name__ == \"__main__\" : app () # pragma: aplicaci\u00f3n en vivo Comandos Para usar nuestra aplicaci\u00f3n CLI, primero podemos ver los comandos disponibles gracias a los decoradores: python coe_template/main.py --help Argumentos Con Typer, los argumentos de entrada de una funci\u00f3n se representan autom\u00e1ticamente como opciones de l\u00ednea de comandos. Pero tambi\u00e9n podemos pedir ayuda con este comando espec\u00edfico sin tener que entrar en el c\u00f3digo: python coe_template/main.py predict-tag --help Uso Finalmente, podemos ejecutar el comando espec\u00edfico con todos los argumentos: python coe_template/main.py predict-tag --text = \"Transfer learning with transformers for text classification.\"","title":"L\u00ednea de comando"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/#aplicaciones-cli-command-line-interface","text":"Uso de una aplicaci\u00f3n CLI (command line interface) para organizar los procesos de la aplicaci\u00f3n.","title":"Aplicaciones CLI (Command Line Interface)"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/#introduccion","text":"Cuando se trata de servir nuestros modelos, debemos pensar en exponer la funcionalidad de la aplicaci\u00f3n a nosotros mismos, a los miembros del equipo y, en \u00faltima instancia, a nuestros usuarios finales. Y las interfaces para lograr esto ser\u00e1n diferentes. En las secciones anteriores ejecutamos nuestras operaciones principales a trav\u00e9s de la terminal y el int\u00e9rprete de Python. Esto se vuelve extremadamente tedioso al tener que profundizar en el c\u00f3digo y ejecutar funciones una a la vez. Una soluci\u00f3n es crear una aplicaci\u00f3n de interfaz de l\u00ednea de comandos (CLI) que permita la interacci\u00f3n a nivel operativo. Debe dise\u00f1arse de tal manera que podamos ver todas las operaciones posibles (y sus argumentos requeridos) y ejecutarlas desde el shell.","title":"Introducci\u00f3n"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/#aplicacion","text":"Vamos a crear nuestra CLI usando Typer: python -m pip install typer == 0 .4.1 # Agregar a requirements.txt typer == 0 .4.1 Es tan simple como inicializar la aplicaci\u00f3n y luego agregar el decorador apropiado para cada operaci\u00f3n de funci\u00f3n que deseamos usar como un comando CLI en nuestro main.py : # coe_template/main.py import typer # Inicializar la aplicaci\u00f3n CLI de Typer app = typer . Typer () @app . command () def elt_data (): ... Repetiremos lo mismo para todas las dem\u00e1s funciones a las que queremos acceder a trav\u00e9s de la CLI: elt_data(), train_model(), optimize(), predict_tag(). Haremos que todos los argumentos sean opcionales para que podamos definirlos expl\u00edcitamente en nuestros comandos bash. Ver encabezados de funci\u00f3n coe_template/main.py @app . command () def elt_data (): ... @app . command () def train_model ( args_fp : str = \"config/args.json\" , experiment_name : str = \"baselines\" , run_name : str = \"sgd\" , ) -> None : ... @app . command () def optimize ( args_fp : str = \"config/args.json\" , study_name : str = \"optimization\" , num_trials : int = 20 ) -> None : ... @app . command () def predict_tag ( text : str = \"\" , run_id : str = None ) -> None : ... Al final del archivo debemos inicializar la aplicaci\u00f3n CLI: if __name__ == \"__main__\" : app () # pragma: aplicaci\u00f3n en vivo","title":"Aplicaci\u00f3n"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/#comandos","text":"Para usar nuestra aplicaci\u00f3n CLI, primero podemos ver los comandos disponibles gracias a los decoradores: python coe_template/main.py --help","title":"Comandos"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/#argumentos","text":"Con Typer, los argumentos de entrada de una funci\u00f3n se representan autom\u00e1ticamente como opciones de l\u00ednea de comandos. Pero tambi\u00e9n podemos pedir ayuda con este comando espec\u00edfico sin tener que entrar en el c\u00f3digo: python coe_template/main.py predict-tag --help","title":"Argumentos"},{"location":"Inferencia/L%C3%ADnea%20de%20comando/#uso","text":"Finalmente, podemos ejecutar el comando espec\u00edfico con todos los argumentos: python coe_template/main.py predict-tag --text = \"Transfer learning with transformers for text classification.\"","title":"Uso"},{"location":"Ingenier%C3%ADa%20de%20datos/Data%20stack/","text":"","title":"Data stack"},{"location":"Ingenier%C3%ADa%20de%20datos/Feature%20store/","text":"","title":"Feature store"},{"location":"Ingenier%C3%ADa%20de%20datos/Orquestaci%C3%B3n/","text":"","title":"Orquestaci\u00f3n"},{"location":"Modelado/Evaluaci%C3%B3n/","text":"","title":"Evaluaci\u00f3n"},{"location":"Modelado/L%C3%ADneas%20base/","text":"","title":"L\u00edneas base"},{"location":"Modelado/Optimizaci%C3%B3n/","text":"","title":"Optimizaci\u00f3n"},{"location":"Modelado/Seguimiento%20de%20experimentos/","text":"","title":"Seguimiento de experimentos"},{"location":"Producci%C3%B3n/Dashboard/","text":"Dashboard Creaci\u00f3n de un dashboard interactivo para inspeccionar visualmente nuestra aplicaci\u00f3n utilizando Streamlit. Introducci\u00f3n Al desarrollar una aplicaci\u00f3n, hay muchas decisiones t\u00e9cnicas y resultados que son parte integral de nuestro sistema. Debemos poder comunicar esto de forma eficaz a otros desarrolladores y a las partes interesadas del negocio. Necesitamos crear un dashboard al que se pueda acceder sin ning\u00fan requisito t\u00e9cnico previo y que comunique eficazmente los resultados clave. Ser\u00eda a\u00fan m\u00e1s \u00fatil si nuestro dashboard fuera interactivo, de modo que proporcionara utilidad incluso a los desarrolladores t\u00e9cnicos. Streamlit Dado que muchos desarrolladores que trabajan en el aprendizaje autom\u00e1tico utilizan Python, el panorama de las herramientas para crear dashboards para ofrecer informaci\u00f3n orientada a los datos ha evolucionado para salvar este vac\u00edo. Estas herramientas permiten ahora a los desarrolladores de ML crear dashboards interactivos y visualizaciones en Python, al tiempo que ofrecen una personalizaci\u00f3n completa a trav\u00e9s de HTML, JS y CSS. Utilizaremos Streamlit para crear nuestros dashboards debido a su intuitiva API, sus capacidades para compartir y su creciente adopci\u00f3n por parte de la comunidad. Configuraci\u00f3n Con Streamlit, podemos crear r\u00e1pidamente una aplicaci\u00f3n vac\u00eda y, a medida que vayamos desarrollando, la interfaz de usuario se ir\u00e1 actualizando tambi\u00e9n. # Setup python -m pip install streamlit == 1 .10.0 mkdir streamlit touch streamlit/app.py streamlit run streamlit/app.py Aseg\u00farese de a\u00f1adir este paquete y la versi\u00f3n a nuestro archivo requirements.txt . Referencia de la API Antes de crear un dashboard para nuestra aplicaci\u00f3n, debemos conocer los diferentes componentes de Streamlit. T\u00f3mate unos minutos y revisa la referencia de la API . Vamos a explorar los diferentes componentes en detalle, ya que se aplican a la creaci\u00f3n de diferentes interacciones para nuestro dashboard espec\u00edfico. Secciones Comenzaremos por delinear las secciones que queremos tener en nuestro tablero editando nuestro script streamlit/app.py : import pandas as pd from pathlib import Path import streamlit as st from config import config from coe_template import main , utils # T\u00edtulo st . title ( \"CoE Template MLOps\" ) # Secciones st . header ( \"\ud83d\udd22 Datos\" ) st . header ( \"\ud83d\udcc8 Performance\" ) st . header ( \"\ud83d\udd2e Inferencia\" ) Datos Vamos a mantener la sencillez de nuestro dashboard, as\u00ed que s\u00f3lo mostraremos los proyectos etiquetados. st . header ( \"\ud83d\udd22 Datos\" ) projects_fp = Path ( config . DATA_DIR , \"labeled_projects.csv\" ) df = pd . read_csv ( projects_fp ) st . text ( f \"Proyectos (count: { len ( df ) } )\" ) st . write ( df ) Performance En esta secci\u00f3n, mostraremos el rendimiento de nuestro \u00faltimo modelo entrenado. Una vez m\u00e1s, vamos a mantenerlo simple, pero tambi\u00e9n podr\u00edamos superponer m\u00e1s informaci\u00f3n, como las mejoras o regresiones de las implementaciones anteriores, accediendo al almac\u00e9n de modelos. st . header ( \"\ud83d\udcc8 Performance\" ) performance_fp = Path ( config . CONFIG_DIR , \"performance.json\" ) performance = utils . load_dict ( filepath = performance_fp ) st . text ( \"Overall:\" ) st . write ( performance [ \"overall\" ]) tag = st . selectbox ( \"Elige una etiqueta: \" , list ( performance [ \"class\" ] . keys ())) st . write ( performance [ \"class\" ][ tag ]) tag = st . selectbox ( \"Elige un slice: \" , list ( performance [ \"slices\" ] . keys ())) st . write ( performance [ \"slices\" ][ tag ]) Inferencia Con la secci\u00f3n de inferencia, queremos ser capaces de predecir r\u00e1pidamente con el \u00faltimo modelo entrenado. st . header ( \"\ud83d\udd2e Inferencia\" ) text = st . text_input ( \"Ingrese texto:\" , \"Transfer learning with transformers for text classification.\" ) run_id = st . text_input ( \"Ingrese run ID:\" , open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read ()) prediction = main . predict_tag ( text = text , run_id = run_id ) st . write ( prediction ) Caching A veces podemos tener vistas que implican operaciones computacionalmente pesadas, como la carga de datos o artefactos del modelo. Es una buena pr\u00e1ctica almacenar en cach\u00e9 estas operaciones envolvi\u00e9ndolas como una funci\u00f3n separada con el decorador @st.cache. Esto hace que Streamlit almacene en cach\u00e9 la funci\u00f3n por la combinaci\u00f3n espec\u00edfica de sus entradas para entregar las respectivas salidas cuando la funci\u00f3n es invocada con las mismas entradas. @st . cache () def load_data (): projects_fp = Path ( config . DATA_DIR , \"labeled_projects.csv\" ) df = pd . read_csv ( projects_fp ) return df Despliegue Tenemos varias opciones para desplegar y gestionar nuestro dashboard de Streamlit. Podemos utilizar la funci\u00f3n de compartir de Streamlit que nos permite desplegar los dashboard directamente desde GitHub. Nuestro dashboard se mantendr\u00e1 actualizado a medida que vayamos introduciendo cambios en nuestro repositorio. Otra opci\u00f3n es desplegarlo junto con nuestro servicio de API. Podemos a\u00f1adirlo al ENTRYPOINT del Dockerfile con los puertos apropiados expuestos.","title":"Dashboard"},{"location":"Producci%C3%B3n/Dashboard/#dashboard","text":"Creaci\u00f3n de un dashboard interactivo para inspeccionar visualmente nuestra aplicaci\u00f3n utilizando Streamlit.","title":"Dashboard"},{"location":"Producci%C3%B3n/Dashboard/#introduccion","text":"Al desarrollar una aplicaci\u00f3n, hay muchas decisiones t\u00e9cnicas y resultados que son parte integral de nuestro sistema. Debemos poder comunicar esto de forma eficaz a otros desarrolladores y a las partes interesadas del negocio. Necesitamos crear un dashboard al que se pueda acceder sin ning\u00fan requisito t\u00e9cnico previo y que comunique eficazmente los resultados clave. Ser\u00eda a\u00fan m\u00e1s \u00fatil si nuestro dashboard fuera interactivo, de modo que proporcionara utilidad incluso a los desarrolladores t\u00e9cnicos.","title":"Introducci\u00f3n"},{"location":"Producci%C3%B3n/Dashboard/#streamlit","text":"Dado que muchos desarrolladores que trabajan en el aprendizaje autom\u00e1tico utilizan Python, el panorama de las herramientas para crear dashboards para ofrecer informaci\u00f3n orientada a los datos ha evolucionado para salvar este vac\u00edo. Estas herramientas permiten ahora a los desarrolladores de ML crear dashboards interactivos y visualizaciones en Python, al tiempo que ofrecen una personalizaci\u00f3n completa a trav\u00e9s de HTML, JS y CSS. Utilizaremos Streamlit para crear nuestros dashboards debido a su intuitiva API, sus capacidades para compartir y su creciente adopci\u00f3n por parte de la comunidad.","title":"Streamlit"},{"location":"Producci%C3%B3n/Dashboard/#configuracion","text":"Con Streamlit, podemos crear r\u00e1pidamente una aplicaci\u00f3n vac\u00eda y, a medida que vayamos desarrollando, la interfaz de usuario se ir\u00e1 actualizando tambi\u00e9n. # Setup python -m pip install streamlit == 1 .10.0 mkdir streamlit touch streamlit/app.py streamlit run streamlit/app.py Aseg\u00farese de a\u00f1adir este paquete y la versi\u00f3n a nuestro archivo requirements.txt .","title":"Configuraci\u00f3n"},{"location":"Producci%C3%B3n/Dashboard/#referencia-de-la-api","text":"Antes de crear un dashboard para nuestra aplicaci\u00f3n, debemos conocer los diferentes componentes de Streamlit. T\u00f3mate unos minutos y revisa la referencia de la API . Vamos a explorar los diferentes componentes en detalle, ya que se aplican a la creaci\u00f3n de diferentes interacciones para nuestro dashboard espec\u00edfico.","title":"Referencia de la API"},{"location":"Producci%C3%B3n/Dashboard/#secciones","text":"Comenzaremos por delinear las secciones que queremos tener en nuestro tablero editando nuestro script streamlit/app.py : import pandas as pd from pathlib import Path import streamlit as st from config import config from coe_template import main , utils # T\u00edtulo st . title ( \"CoE Template MLOps\" ) # Secciones st . header ( \"\ud83d\udd22 Datos\" ) st . header ( \"\ud83d\udcc8 Performance\" ) st . header ( \"\ud83d\udd2e Inferencia\" )","title":"Secciones"},{"location":"Producci%C3%B3n/Dashboard/#datos","text":"Vamos a mantener la sencillez de nuestro dashboard, as\u00ed que s\u00f3lo mostraremos los proyectos etiquetados. st . header ( \"\ud83d\udd22 Datos\" ) projects_fp = Path ( config . DATA_DIR , \"labeled_projects.csv\" ) df = pd . read_csv ( projects_fp ) st . text ( f \"Proyectos (count: { len ( df ) } )\" ) st . write ( df )","title":"Datos"},{"location":"Producci%C3%B3n/Dashboard/#performance","text":"En esta secci\u00f3n, mostraremos el rendimiento de nuestro \u00faltimo modelo entrenado. Una vez m\u00e1s, vamos a mantenerlo simple, pero tambi\u00e9n podr\u00edamos superponer m\u00e1s informaci\u00f3n, como las mejoras o regresiones de las implementaciones anteriores, accediendo al almac\u00e9n de modelos. st . header ( \"\ud83d\udcc8 Performance\" ) performance_fp = Path ( config . CONFIG_DIR , \"performance.json\" ) performance = utils . load_dict ( filepath = performance_fp ) st . text ( \"Overall:\" ) st . write ( performance [ \"overall\" ]) tag = st . selectbox ( \"Elige una etiqueta: \" , list ( performance [ \"class\" ] . keys ())) st . write ( performance [ \"class\" ][ tag ]) tag = st . selectbox ( \"Elige un slice: \" , list ( performance [ \"slices\" ] . keys ())) st . write ( performance [ \"slices\" ][ tag ])","title":"Performance"},{"location":"Producci%C3%B3n/Dashboard/#inferencia","text":"Con la secci\u00f3n de inferencia, queremos ser capaces de predecir r\u00e1pidamente con el \u00faltimo modelo entrenado. st . header ( \"\ud83d\udd2e Inferencia\" ) text = st . text_input ( \"Ingrese texto:\" , \"Transfer learning with transformers for text classification.\" ) run_id = st . text_input ( \"Ingrese run ID:\" , open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read ()) prediction = main . predict_tag ( text = text , run_id = run_id ) st . write ( prediction )","title":"Inferencia"},{"location":"Producci%C3%B3n/Dashboard/#caching","text":"A veces podemos tener vistas que implican operaciones computacionalmente pesadas, como la carga de datos o artefactos del modelo. Es una buena pr\u00e1ctica almacenar en cach\u00e9 estas operaciones envolvi\u00e9ndolas como una funci\u00f3n separada con el decorador @st.cache. Esto hace que Streamlit almacene en cach\u00e9 la funci\u00f3n por la combinaci\u00f3n espec\u00edfica de sus entradas para entregar las respectivas salidas cuando la funci\u00f3n es invocada con las mismas entradas. @st . cache () def load_data (): projects_fp = Path ( config . DATA_DIR , \"labeled_projects.csv\" ) df = pd . read_csv ( projects_fp ) return df","title":"Caching"},{"location":"Producci%C3%B3n/Dashboard/#despliegue","text":"Tenemos varias opciones para desplegar y gestionar nuestro dashboard de Streamlit. Podemos utilizar la funci\u00f3n de compartir de Streamlit que nos permite desplegar los dashboard directamente desde GitHub. Nuestro dashboard se mantendr\u00e1 actualizado a medida que vayamos introduciendo cambios en nuestro repositorio. Otra opci\u00f3n es desplegarlo junto con nuestro servicio de API. Podemos a\u00f1adirlo al ENTRYPOINT del Dockerfile con los puertos apropiados expuestos.","title":"Despliegue"},{"location":"Producci%C3%B3n/Dise%C3%B1o%20de%20sistemas/","text":"","title":"Dise\u00f1o de sistemas"},{"location":"Producci%C3%B3n/Monitoreo/","text":"Supervisi\u00f3n de los sistemas de Machine Learning Aprenda a supervisar los sistemas de ML para identificar y abordar las fuentes de desviaci\u00f3n para evitar el deterioro del rendimiento del modelo. Introducci\u00f3n Aunque hayamos entrenado y evaluado a fondo nuestro modelo, el verdadero trabajo comienza una vez que lo desplegamos en producci\u00f3n. Esta es una de las diferencias fundamentales entre la ingenier\u00eda de software tradicional y el desarrollo de ML. Tradicionalmente, con el software determinista basado en reglas, la mayor parte del trabajo se realiza en la fase inicial y, una vez desplegado, nuestro sistema funciona tal y como lo hemos definido. Pero con el aprendizaje autom\u00e1tico, no hemos definido expl\u00edcitamente c\u00f3mo funciona algo, sino que hemos utilizado los datos para dise\u00f1ar una soluci\u00f3n probabil\u00edstica. Este enfoque est\u00e1 sujeto a una degradaci\u00f3n natural del rendimiento a lo largo del tiempo, as\u00ed como a un comportamiento involuntario, ya que los datos expuestos al modelo ser\u00e1n diferentes de aquellos con los que ha sido entrenado. Esto no es algo que debamos tratar de evitar, sino m\u00e1s bien comprender y mitigar en la medida de lo posible. Salud del sistema El primer paso para asegurar que nuestro modelo est\u00e1 funcionando bien es asegurarse de que el sistema real est\u00e1 funcionando como deber\u00eda. Esto puede incluir m\u00e9tricas espec\u00edficas de las solicitudes de servicio, como la latencia, el rendimiento, las tasas de error, etc., as\u00ed como la utilizaci\u00f3n de la infraestructura, tal como uso de CPU/GPU, memoria, etc. La mayor\u00eda de los proveedores de la nube, e incluso las capas de orquestaci\u00f3n, nos proporcionar\u00e1n esta visi\u00f3n de la salud de nuestro sistema de forma gratuita a trav\u00e9s de un dashboard. Performance La siguiente capa de m\u00e9tricas que hay que supervisar tiene que ver con el rendimiento del modelo. Puede tratarse de m\u00e9tricas de evaluaci\u00f3n cuantitativas que hayamos utilizado durante la evaluaci\u00f3n del modelo (exactitud, precisi\u00f3n, f1, etc.), pero tambi\u00e9n de m\u00e9tricas de negocio clave en las que el modelo influye (ROI, tasa de clics, etc.). Por lo general, nunca es suficiente con analizar las m\u00e9tricas de rendimiento acumuladas a lo largo de todo el periodo de tiempo desde que el modelo se ha desplegado. En su lugar, deber\u00edamos inspeccionar tambi\u00e9n el rendimiento a lo largo de un periodo de tiempo que sea significativo para nuestra aplicaci\u00f3n. Estas m\u00e9tricas deslizantes podr\u00edan ser m\u00e1s indicativas de la salud de nuestro sistema y podr\u00edamos ser capaces de identificar los problemas m\u00e1s r\u00e1pidamente al no oscurecerlos con datos hist\u00f3ricos. import matplotlib.pyplot as plt import numpy as np import seaborn as sns sns . set_theme () # Generar datos hourly_f1 = list ( np . random . randint ( low = 94 , high = 98 , size = 24 * 20 )) + \\ list ( np . random . randint ( low = 92 , high = 96 , size = 24 * 5 )) + \\ list ( np . random . randint ( low = 88 , high = 96 , size = 24 * 5 )) + \\ list ( np . random . randint ( low = 86 , high = 92 , size = 24 * 5 )) # f1 acumulativo cumulative_f1 = [ np . mean ( hourly_f1 [: n ]) for n in range ( 1 , len ( hourly_f1 ) + 1 )] print ( f \"F1 promedio acumulado en el \u00faltimo d\u00eda: { np . mean ( cumulative_f1 [ - 24 :]) : .1f } \" ) # f1 corredizo window_size = 24 sliding_f1 = np . convolve ( hourly_f1 , np . ones ( window_size ) / window_size , mode = \"valid\" ) print ( f \"Deslizamiento promedio de f1 en el \u00faltimo d\u00eda: { np . mean ( sliding_f1 [ - 24 :]) : .1f } \" ) plt . ylim ([ 80 , 100 ]) plt . hlines ( y = 90 , xmin = 0 , xmax = len ( hourly_f1 ), colors = \"blue\" , linestyles = \"dashed\" , label = \"threshold\" ) plt . plot ( cumulative_f1 , label = \"cumulative\" ) plt . plot ( sliding_f1 , label = \"sliding\" ) plt . legend () Puede que necesitemos monitorizar las m\u00e9tricas en varios tama\u00f1os de ventana para detectar la degradaci\u00f3n del rendimiento lo antes posible. En este caso estamos monitorizando el f1 global, pero podemos hacer lo mismo con slices de datos, clases individuales, etc. Resultados retrasados Es posible que no siempre dispongamos de los resultados reales para determinar el rendimiento del modelo en producci\u00f3n. Para mitigar esto, podr\u00edamos: idear una se\u00f1al aproximada que nos ayude a estimar el rendimiento del modelo. Por ejemplo, en nuestra tarea de predicci\u00f3n de etiquetas, podr\u00edamos utilizar las etiquetas reales que un autor atribuye a un proyecto como etiquetas intermedias hasta que tengamos etiquetas verificadas a partir de un proceso de anotaci\u00f3n. etiquetar un peque\u00f1o subconjunto de nuestro conjunto de datos en vivo para estimar el rendimiento. Este subconjunto deber\u00eda intentar ser representativo de las distintas distribuciones de los datos reales. Ponderaci\u00f3n de la importancia Sin embargo, las se\u00f1ales aproximadas no siempre est\u00e1n disponibles para todas las situaciones porque no hay retroalimentaci\u00f3n sobre las salidas del sistema ML o est\u00e1 demasiado retrasada. Para estas situaciones, una l\u00ednea de investigaci\u00f3n reciente se basa en el \u00fanico componente que est\u00e1 disponible en todas las situaciones: los datos de entrada. La idea central es desarrollar funciones slicing que puedan capturar potencialmente las formas en que nuestros datos pueden experimentar un cambio de distribuci\u00f3n. Estas funciones deben capturar slice obvios, como las etiquetas de clase o diferentes valores de features categ\u00f3ricos, pero tambi\u00e9n slices basados en metadatos impl\u00edcitos. Estas funciones de slice se aplican a nuestro conjunto de datos etiquetados para crear matrices con las etiquetas correspondientes. Las mismas funciones se aplican a nuestros datos de producci\u00f3n no etiquetados para aproximar lo que ser\u00edan las etiquetas ponderadas. Con esto, podemos determinar el rendimiento aproximado. La intuici\u00f3n aqu\u00ed es que podemos aproximar mejor el rendimiento en nuestro conjunto de datos sin etiquetar bas\u00e1ndonos en la similitud entre la matriz de slices etiquetados y la matriz de slices sin etiquetar. Una dependencia central de esta suposici\u00f3n es que nuestras funciones de slice son lo suficientemente completas como para capturar las causas del cambio de distribuci\u00f3n. Drift En primer lugar, debemos comprender los diferentes tipos de problemas que pueden hacer que el rendimiento de nuestro modelo decaiga (model drift). La mejor manera de hacerlo es observar todas las piezas m\u00f3viles de lo que estamos tratando de modelar y c\u00f3mo cada una de ellas puede experimentar un desfase. Entidad Descripci\u00f3n Drift X entradas (features) data drift -> P(X) != Pref(X) y salidas (ground-truth) target drift -> P(y) != Pref(y) P(y/X) relaci\u00f3n actual entre X e y concept drift -> P(y/X) != Pref(y/X) Data drift Data drift se produce cuando la distribuci\u00f3n de los datos de producci\u00f3n es diferente a la de los datos de entrenamiento. El modelo no est\u00e1 preparado para hacer frente a este desfase y, por tanto, sus predicciones pueden no ser fiables. La causa real puede atribuirse a cambios naturales en el mundo real, pero tambi\u00e9n a problemas sist\u00e9micos como la falta de datos, errores de canalizaci\u00f3n, cambios de esquema, etc. Es importante inspeccionar los datos desviados y rastrearlos a lo largo de su canalizaci\u00f3n para identificar cu\u00e1ndo y d\u00f3nde se introdujo la desviaci\u00f3n. Target drift Tambi\u00e9n podemos experimentar una deriva en nuestros resultados. Esto puede ser un cambio en las distribuciones, pero tambi\u00e9n la eliminaci\u00f3n o adici\u00f3n de nuevas clases con tareas categ\u00f3ricas. Aunque el reentrenamiento puede mitigar el deterioro del rendimiento causado por la deriva de los datos, a menudo puede evitarse con una comunicaci\u00f3n adecuada entre l\u00edneas sobre nuevas clases, cambios de esquema, etc. Concept drift Adem\u00e1s de la deriva de los datos de entrada y de salida, tambi\u00e9n podemos ver que la relaci\u00f3n actual entre ellos tambi\u00e9n se desv\u00ede. Este desvio hace que nuestro modelo sea ineficaz porque los patrones que aprendi\u00f3 a trazar entre las entradas y salidas originales ya no son relevantes. Localizaci\u00f3n del desfase Ahora que hemos identificado los diferentes tipos de desfase, tenemos que aprender a localizarla y a medirla con frecuencia. Estas son las limitaciones que debemos tener en cuenta: ventana de referencia: el conjunto de puntos con los que se comparan las distribuciones de datos de producci\u00f3n para identificar el desafase. ventana de prueba: el conjunto de puntos que se comparan con la ventana de referencia para determinar si se ha producido un desafase. Dado que se trata de una detecci\u00f3n de desviaciones en l\u00ednea, podemos emplear un enfoque de ventana fija o deslizante para identificar nuestro conjunto de puntos para la comparaci\u00f3n. Normalmente, la ventana de referencia es un subconjunto fijo y reciente de los datos de entrenamiento, mientras que la ventana de prueba se desliza en el tiempo. Scikit-multiflow proporciona un conjunto de herramientas para las t\u00e9cnicas de detecci\u00f3n de concept drift directamente en los datos de flujo. El paquete ofrece la funcionalidad de ventanas y medias m\u00f3viles e incluso m\u00e9todos en torno a conceptos como la deriva conceptual gradual. Midiendo el desfase Una vez que tenemos la ventana de puntos que queremos comparar, necesitamos saber c\u00f3mo compararlos. import great_expectations as ge import json import pandas as pd from urllib.request import urlopen # Cargar proyectos etiquetados projects = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\" ) tags = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\" ) df = ge . dataset . PandasDataset ( pd . merge ( projects , tags , on = \"id\" )) df [ \"text\" ] = df . title + \" \" + df . description df . drop ([ \"title\" , \"description\" ], axis = 1 , inplace = True ) df . head ( 5 ) Expectativas La primera forma de medici\u00f3n puede estar basada en reglas, como la validaci\u00f3n de las expectativas en torno a los valores que faltan, los tipos de datos, los rangos de valores, etc.. La diferencia con las pruebas de datos es que ahora validaremos las expectativas en datos de producci\u00f3n en vivo. # Datos de producci\u00f3n simulados prod_df = ge . dataset . PandasDataset ([{ \"text\" : \"hello\" }, { \"text\" : 0 }, { \"text\" : \"world\" }]) # Suite de expectativas df . expect_column_values_to_not_be_null ( column = \"text\" ) df . expect_column_values_to_be_of_type ( column = \"text\" , type_ = \"str\" ) expectation_suite = df . get_expectation_suite () # Validar datos de referencia df . validate ( expectation_suite = expectation_suite , only_return_failures = True )[ \"statistics\" ] # Validar datos de producci\u00f3n prod_df . validate ( expectation_suite = expectation_suite , only_return_failures = True )[ \"statistics\" ] Una vez que hayamos validado nuestras expectativas basadas en reglas, tenemos que medir cuantitativamente el desafase en los diferentes features de nuestros datos. Univariado Nuestra tarea puede incluir caracter\u00edsticas univariantes (1D) que querremos controlar. Aunque hay muchos tipos de pruebas de hip\u00f3tesis que podemos utilizar, una opci\u00f3n popular es la prueba de Kolmogorov-Smirnov (KS). Kolmogorov-Smirnov (KS) test El test KS determina la distancia m\u00e1xima entre las funciones de densidad acumulativa de dos distribuciones. En este caso, mediremos si hay alguna deriva en el tama\u00f1o de nuestro feature de texto de entrada entre dos subconjuntos de datos diferentes. from alibi_detect.cd import KSDrift # Referencia df [ \"num_tokens\" ] = df . text . apply ( lambda x : len ( x . split ( \" \" ))) ref = df [ \"num_tokens\" ][ 0 : 200 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . legend () plt . show () # Inicializar detector de deriva length_drift_detector = KSDrift ( ref , p_val = 0.01 ) # No drift no_drift = df [ \"num_tokens\" ][ 200 : 400 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( no_drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () length_drift_detector . predict ( no_drift , return_p_val = True , return_distance = True ) # Drift drift = np . random . normal ( 30 , 5 , len ( ref )) plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () length_drift_detector . predict ( drift , return_p_val = True , return_distance = True ) Chi-squared test Del mismo modo, para los datos categ\u00f3ricos, podemos aplicar la prueba chi-squared de Pearson para determinar si una frecuencia de eventos en la producci\u00f3n es coherente con una distribuci\u00f3n de referencia. from alibi_detect.cd import ChiSquareDrift # Referencia df . token_count = df . num_tokens . apply ( lambda x : \"small\" if x <= 10 else ( \"medium\" if x <= 25 else \"large\" )) ref = df . token_count [ 0 : 200 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . legend () # Inicializar detector de deriva target_drift_detector = ChiSquareDrift ( ref , p_val = 0.01 ) # No drift no_drift = df . token_count [ 200 : 400 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( no_drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () target_drift_detector . predict ( no_drift , return_p_val = True , return_distance = True ) # Drift drift = np . array ([ \"small\" ] * 80 + [ \"medium\" ] * 40 + [ \"large\" ] * 80 ) plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () target_drift_detector . predict ( drift , return_p_val = True , return_distance = True ) Multivariado La medici\u00f3n de la deriva es bastante sencilla para los datos univariados, pero dif\u00edcil para los datos multivariados. Resumiremos el enfoque de reducci\u00f3n y medici\u00f3n que se expone en el siguiente documento: Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift . Hemos vectorizado nuestro texto utilizando tf-idf, que tiene una alta dimensionalidad y no es sem\u00e1nticamente rico en contexto. Sin embargo, normalmente con el texto se utilizan incrustaciones de palabras/caracteres. As\u00ed que para ilustrar c\u00f3mo ser\u00eda la detecci\u00f3n de desfase en datos multivariados, vamos a representar nuestro texto utilizando incrustaciones preentrenadas. Empezaremos cargando el tokenizador desde un modelo preentrenado. from transformers import AutoTokenizer model_name = \"allenai/scibert_scivocab_uncased\" tokenizer = AutoTokenizer . from_pretrained ( model_name ) vocab_size = len ( tokenizer ) print ( vocab_size ) # Tokenizar entradas encoded_input = tokenizer ( df . text . tolist (), return_tensors = \"pt\" , padding = True ) ids = encoded_input [ \"input_ids\" ] masks = encoded_input [ \"attention_mask\" ] # Decode print ( f \" { ids [ 0 ] } \\n { tokenizer . decode ( ids [ 0 ]) } \" ) # Tokens de subpalabras print ( tokenizer . convert_ids_to_tokens ( ids = ids [ 0 ])) A continuaci\u00f3n, cargaremos los pesos del modelo preentrenado y utilizaremos el objeto TransformerEmbedding para extraer las incrustaciones del estado oculto. from alibi_detect.models.pytorch import TransformerEmbedding # Capa de incrustaci\u00f3n emb_type = \"hidden_state\" layers = [ - x for x in range ( 1 , 9 )] # last 8 layers embedding_layer = TransformerEmbedding ( model_name , emb_type , layers ) # Dimensi\u00f3n de incrustaci\u00f3n embedding_dim = embedding_layer . model . embeddings . word_embeddings . embedding_dim embedding_dim","title":"Supervisi\u00f3n de los sistemas de Machine Learning"},{"location":"Producci%C3%B3n/Monitoreo/#supervision-de-los-sistemas-de-machine-learning","text":"Aprenda a supervisar los sistemas de ML para identificar y abordar las fuentes de desviaci\u00f3n para evitar el deterioro del rendimiento del modelo.","title":"Supervisi\u00f3n de los sistemas de Machine Learning"},{"location":"Producci%C3%B3n/Monitoreo/#introduccion","text":"Aunque hayamos entrenado y evaluado a fondo nuestro modelo, el verdadero trabajo comienza una vez que lo desplegamos en producci\u00f3n. Esta es una de las diferencias fundamentales entre la ingenier\u00eda de software tradicional y el desarrollo de ML. Tradicionalmente, con el software determinista basado en reglas, la mayor parte del trabajo se realiza en la fase inicial y, una vez desplegado, nuestro sistema funciona tal y como lo hemos definido. Pero con el aprendizaje autom\u00e1tico, no hemos definido expl\u00edcitamente c\u00f3mo funciona algo, sino que hemos utilizado los datos para dise\u00f1ar una soluci\u00f3n probabil\u00edstica. Este enfoque est\u00e1 sujeto a una degradaci\u00f3n natural del rendimiento a lo largo del tiempo, as\u00ed como a un comportamiento involuntario, ya que los datos expuestos al modelo ser\u00e1n diferentes de aquellos con los que ha sido entrenado. Esto no es algo que debamos tratar de evitar, sino m\u00e1s bien comprender y mitigar en la medida de lo posible.","title":"Introducci\u00f3n"},{"location":"Producci%C3%B3n/Monitoreo/#salud-del-sistema","text":"El primer paso para asegurar que nuestro modelo est\u00e1 funcionando bien es asegurarse de que el sistema real est\u00e1 funcionando como deber\u00eda. Esto puede incluir m\u00e9tricas espec\u00edficas de las solicitudes de servicio, como la latencia, el rendimiento, las tasas de error, etc., as\u00ed como la utilizaci\u00f3n de la infraestructura, tal como uso de CPU/GPU, memoria, etc. La mayor\u00eda de los proveedores de la nube, e incluso las capas de orquestaci\u00f3n, nos proporcionar\u00e1n esta visi\u00f3n de la salud de nuestro sistema de forma gratuita a trav\u00e9s de un dashboard.","title":"Salud del sistema"},{"location":"Producci%C3%B3n/Monitoreo/#performance","text":"La siguiente capa de m\u00e9tricas que hay que supervisar tiene que ver con el rendimiento del modelo. Puede tratarse de m\u00e9tricas de evaluaci\u00f3n cuantitativas que hayamos utilizado durante la evaluaci\u00f3n del modelo (exactitud, precisi\u00f3n, f1, etc.), pero tambi\u00e9n de m\u00e9tricas de negocio clave en las que el modelo influye (ROI, tasa de clics, etc.). Por lo general, nunca es suficiente con analizar las m\u00e9tricas de rendimiento acumuladas a lo largo de todo el periodo de tiempo desde que el modelo se ha desplegado. En su lugar, deber\u00edamos inspeccionar tambi\u00e9n el rendimiento a lo largo de un periodo de tiempo que sea significativo para nuestra aplicaci\u00f3n. Estas m\u00e9tricas deslizantes podr\u00edan ser m\u00e1s indicativas de la salud de nuestro sistema y podr\u00edamos ser capaces de identificar los problemas m\u00e1s r\u00e1pidamente al no oscurecerlos con datos hist\u00f3ricos. import matplotlib.pyplot as plt import numpy as np import seaborn as sns sns . set_theme () # Generar datos hourly_f1 = list ( np . random . randint ( low = 94 , high = 98 , size = 24 * 20 )) + \\ list ( np . random . randint ( low = 92 , high = 96 , size = 24 * 5 )) + \\ list ( np . random . randint ( low = 88 , high = 96 , size = 24 * 5 )) + \\ list ( np . random . randint ( low = 86 , high = 92 , size = 24 * 5 )) # f1 acumulativo cumulative_f1 = [ np . mean ( hourly_f1 [: n ]) for n in range ( 1 , len ( hourly_f1 ) + 1 )] print ( f \"F1 promedio acumulado en el \u00faltimo d\u00eda: { np . mean ( cumulative_f1 [ - 24 :]) : .1f } \" ) # f1 corredizo window_size = 24 sliding_f1 = np . convolve ( hourly_f1 , np . ones ( window_size ) / window_size , mode = \"valid\" ) print ( f \"Deslizamiento promedio de f1 en el \u00faltimo d\u00eda: { np . mean ( sliding_f1 [ - 24 :]) : .1f } \" ) plt . ylim ([ 80 , 100 ]) plt . hlines ( y = 90 , xmin = 0 , xmax = len ( hourly_f1 ), colors = \"blue\" , linestyles = \"dashed\" , label = \"threshold\" ) plt . plot ( cumulative_f1 , label = \"cumulative\" ) plt . plot ( sliding_f1 , label = \"sliding\" ) plt . legend () Puede que necesitemos monitorizar las m\u00e9tricas en varios tama\u00f1os de ventana para detectar la degradaci\u00f3n del rendimiento lo antes posible. En este caso estamos monitorizando el f1 global, pero podemos hacer lo mismo con slices de datos, clases individuales, etc.","title":"Performance"},{"location":"Producci%C3%B3n/Monitoreo/#resultados-retrasados","text":"Es posible que no siempre dispongamos de los resultados reales para determinar el rendimiento del modelo en producci\u00f3n. Para mitigar esto, podr\u00edamos: idear una se\u00f1al aproximada que nos ayude a estimar el rendimiento del modelo. Por ejemplo, en nuestra tarea de predicci\u00f3n de etiquetas, podr\u00edamos utilizar las etiquetas reales que un autor atribuye a un proyecto como etiquetas intermedias hasta que tengamos etiquetas verificadas a partir de un proceso de anotaci\u00f3n. etiquetar un peque\u00f1o subconjunto de nuestro conjunto de datos en vivo para estimar el rendimiento. Este subconjunto deber\u00eda intentar ser representativo de las distintas distribuciones de los datos reales.","title":"Resultados retrasados"},{"location":"Producci%C3%B3n/Monitoreo/#ponderacion-de-la-importancia","text":"Sin embargo, las se\u00f1ales aproximadas no siempre est\u00e1n disponibles para todas las situaciones porque no hay retroalimentaci\u00f3n sobre las salidas del sistema ML o est\u00e1 demasiado retrasada. Para estas situaciones, una l\u00ednea de investigaci\u00f3n reciente se basa en el \u00fanico componente que est\u00e1 disponible en todas las situaciones: los datos de entrada. La idea central es desarrollar funciones slicing que puedan capturar potencialmente las formas en que nuestros datos pueden experimentar un cambio de distribuci\u00f3n. Estas funciones deben capturar slice obvios, como las etiquetas de clase o diferentes valores de features categ\u00f3ricos, pero tambi\u00e9n slices basados en metadatos impl\u00edcitos. Estas funciones de slice se aplican a nuestro conjunto de datos etiquetados para crear matrices con las etiquetas correspondientes. Las mismas funciones se aplican a nuestros datos de producci\u00f3n no etiquetados para aproximar lo que ser\u00edan las etiquetas ponderadas. Con esto, podemos determinar el rendimiento aproximado. La intuici\u00f3n aqu\u00ed es que podemos aproximar mejor el rendimiento en nuestro conjunto de datos sin etiquetar bas\u00e1ndonos en la similitud entre la matriz de slices etiquetados y la matriz de slices sin etiquetar. Una dependencia central de esta suposici\u00f3n es que nuestras funciones de slice son lo suficientemente completas como para capturar las causas del cambio de distribuci\u00f3n.","title":"Ponderaci\u00f3n de la importancia"},{"location":"Producci%C3%B3n/Monitoreo/#drift","text":"En primer lugar, debemos comprender los diferentes tipos de problemas que pueden hacer que el rendimiento de nuestro modelo decaiga (model drift). La mejor manera de hacerlo es observar todas las piezas m\u00f3viles de lo que estamos tratando de modelar y c\u00f3mo cada una de ellas puede experimentar un desfase. Entidad Descripci\u00f3n Drift X entradas (features) data drift -> P(X) != Pref(X) y salidas (ground-truth) target drift -> P(y) != Pref(y) P(y/X) relaci\u00f3n actual entre X e y concept drift -> P(y/X) != Pref(y/X)","title":"Drift"},{"location":"Producci%C3%B3n/Monitoreo/#data-drift","text":"Data drift se produce cuando la distribuci\u00f3n de los datos de producci\u00f3n es diferente a la de los datos de entrenamiento. El modelo no est\u00e1 preparado para hacer frente a este desfase y, por tanto, sus predicciones pueden no ser fiables. La causa real puede atribuirse a cambios naturales en el mundo real, pero tambi\u00e9n a problemas sist\u00e9micos como la falta de datos, errores de canalizaci\u00f3n, cambios de esquema, etc. Es importante inspeccionar los datos desviados y rastrearlos a lo largo de su canalizaci\u00f3n para identificar cu\u00e1ndo y d\u00f3nde se introdujo la desviaci\u00f3n.","title":"Data drift"},{"location":"Producci%C3%B3n/Monitoreo/#target-drift","text":"Tambi\u00e9n podemos experimentar una deriva en nuestros resultados. Esto puede ser un cambio en las distribuciones, pero tambi\u00e9n la eliminaci\u00f3n o adici\u00f3n de nuevas clases con tareas categ\u00f3ricas. Aunque el reentrenamiento puede mitigar el deterioro del rendimiento causado por la deriva de los datos, a menudo puede evitarse con una comunicaci\u00f3n adecuada entre l\u00edneas sobre nuevas clases, cambios de esquema, etc.","title":"Target drift"},{"location":"Producci%C3%B3n/Monitoreo/#concept-drift","text":"Adem\u00e1s de la deriva de los datos de entrada y de salida, tambi\u00e9n podemos ver que la relaci\u00f3n actual entre ellos tambi\u00e9n se desv\u00ede. Este desvio hace que nuestro modelo sea ineficaz porque los patrones que aprendi\u00f3 a trazar entre las entradas y salidas originales ya no son relevantes.","title":"Concept drift"},{"location":"Producci%C3%B3n/Monitoreo/#localizacion-del-desfase","text":"Ahora que hemos identificado los diferentes tipos de desfase, tenemos que aprender a localizarla y a medirla con frecuencia. Estas son las limitaciones que debemos tener en cuenta: ventana de referencia: el conjunto de puntos con los que se comparan las distribuciones de datos de producci\u00f3n para identificar el desafase. ventana de prueba: el conjunto de puntos que se comparan con la ventana de referencia para determinar si se ha producido un desafase. Dado que se trata de una detecci\u00f3n de desviaciones en l\u00ednea, podemos emplear un enfoque de ventana fija o deslizante para identificar nuestro conjunto de puntos para la comparaci\u00f3n. Normalmente, la ventana de referencia es un subconjunto fijo y reciente de los datos de entrenamiento, mientras que la ventana de prueba se desliza en el tiempo. Scikit-multiflow proporciona un conjunto de herramientas para las t\u00e9cnicas de detecci\u00f3n de concept drift directamente en los datos de flujo. El paquete ofrece la funcionalidad de ventanas y medias m\u00f3viles e incluso m\u00e9todos en torno a conceptos como la deriva conceptual gradual.","title":"Localizaci\u00f3n del desfase"},{"location":"Producci%C3%B3n/Monitoreo/#midiendo-el-desfase","text":"Una vez que tenemos la ventana de puntos que queremos comparar, necesitamos saber c\u00f3mo compararlos. import great_expectations as ge import json import pandas as pd from urllib.request import urlopen # Cargar proyectos etiquetados projects = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\" ) tags = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\" ) df = ge . dataset . PandasDataset ( pd . merge ( projects , tags , on = \"id\" )) df [ \"text\" ] = df . title + \" \" + df . description df . drop ([ \"title\" , \"description\" ], axis = 1 , inplace = True ) df . head ( 5 )","title":"Midiendo el desfase"},{"location":"Producci%C3%B3n/Monitoreo/#expectativas","text":"La primera forma de medici\u00f3n puede estar basada en reglas, como la validaci\u00f3n de las expectativas en torno a los valores que faltan, los tipos de datos, los rangos de valores, etc.. La diferencia con las pruebas de datos es que ahora validaremos las expectativas en datos de producci\u00f3n en vivo. # Datos de producci\u00f3n simulados prod_df = ge . dataset . PandasDataset ([{ \"text\" : \"hello\" }, { \"text\" : 0 }, { \"text\" : \"world\" }]) # Suite de expectativas df . expect_column_values_to_not_be_null ( column = \"text\" ) df . expect_column_values_to_be_of_type ( column = \"text\" , type_ = \"str\" ) expectation_suite = df . get_expectation_suite () # Validar datos de referencia df . validate ( expectation_suite = expectation_suite , only_return_failures = True )[ \"statistics\" ] # Validar datos de producci\u00f3n prod_df . validate ( expectation_suite = expectation_suite , only_return_failures = True )[ \"statistics\" ] Una vez que hayamos validado nuestras expectativas basadas en reglas, tenemos que medir cuantitativamente el desafase en los diferentes features de nuestros datos.","title":"Expectativas"},{"location":"Producci%C3%B3n/Monitoreo/#univariado","text":"Nuestra tarea puede incluir caracter\u00edsticas univariantes (1D) que querremos controlar. Aunque hay muchos tipos de pruebas de hip\u00f3tesis que podemos utilizar, una opci\u00f3n popular es la prueba de Kolmogorov-Smirnov (KS). Kolmogorov-Smirnov (KS) test El test KS determina la distancia m\u00e1xima entre las funciones de densidad acumulativa de dos distribuciones. En este caso, mediremos si hay alguna deriva en el tama\u00f1o de nuestro feature de texto de entrada entre dos subconjuntos de datos diferentes. from alibi_detect.cd import KSDrift # Referencia df [ \"num_tokens\" ] = df . text . apply ( lambda x : len ( x . split ( \" \" ))) ref = df [ \"num_tokens\" ][ 0 : 200 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . legend () plt . show () # Inicializar detector de deriva length_drift_detector = KSDrift ( ref , p_val = 0.01 ) # No drift no_drift = df [ \"num_tokens\" ][ 200 : 400 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( no_drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () length_drift_detector . predict ( no_drift , return_p_val = True , return_distance = True ) # Drift drift = np . random . normal ( 30 , 5 , len ( ref )) plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () length_drift_detector . predict ( drift , return_p_val = True , return_distance = True ) Chi-squared test Del mismo modo, para los datos categ\u00f3ricos, podemos aplicar la prueba chi-squared de Pearson para determinar si una frecuencia de eventos en la producci\u00f3n es coherente con una distribuci\u00f3n de referencia. from alibi_detect.cd import ChiSquareDrift # Referencia df . token_count = df . num_tokens . apply ( lambda x : \"small\" if x <= 10 else ( \"medium\" if x <= 25 else \"large\" )) ref = df . token_count [ 0 : 200 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . legend () # Inicializar detector de deriva target_drift_detector = ChiSquareDrift ( ref , p_val = 0.01 ) # No drift no_drift = df . token_count [ 200 : 400 ] . to_numpy () plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( no_drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () target_drift_detector . predict ( no_drift , return_p_val = True , return_distance = True ) # Drift drift = np . array ([ \"small\" ] * 80 + [ \"medium\" ] * 40 + [ \"large\" ] * 80 ) plt . hist ( ref , alpha = 0.75 , label = \"reference\" ) plt . hist ( drift , alpha = 0.5 , label = \"test\" ) plt . legend () plt . show () target_drift_detector . predict ( drift , return_p_val = True , return_distance = True )","title":"Univariado"},{"location":"Producci%C3%B3n/Monitoreo/#multivariado","text":"La medici\u00f3n de la deriva es bastante sencilla para los datos univariados, pero dif\u00edcil para los datos multivariados. Resumiremos el enfoque de reducci\u00f3n y medici\u00f3n que se expone en el siguiente documento: Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift . Hemos vectorizado nuestro texto utilizando tf-idf, que tiene una alta dimensionalidad y no es sem\u00e1nticamente rico en contexto. Sin embargo, normalmente con el texto se utilizan incrustaciones de palabras/caracteres. As\u00ed que para ilustrar c\u00f3mo ser\u00eda la detecci\u00f3n de desfase en datos multivariados, vamos a representar nuestro texto utilizando incrustaciones preentrenadas. Empezaremos cargando el tokenizador desde un modelo preentrenado. from transformers import AutoTokenizer model_name = \"allenai/scibert_scivocab_uncased\" tokenizer = AutoTokenizer . from_pretrained ( model_name ) vocab_size = len ( tokenizer ) print ( vocab_size ) # Tokenizar entradas encoded_input = tokenizer ( df . text . tolist (), return_tensors = \"pt\" , padding = True ) ids = encoded_input [ \"input_ids\" ] masks = encoded_input [ \"attention_mask\" ] # Decode print ( f \" { ids [ 0 ] } \\n { tokenizer . decode ( ids [ 0 ]) } \" ) # Tokens de subpalabras print ( tokenizer . convert_ids_to_tokens ( ids = ids [ 0 ])) A continuaci\u00f3n, cargaremos los pesos del modelo preentrenado y utilizaremos el objeto TransformerEmbedding para extraer las incrustaciones del estado oculto. from alibi_detect.models.pytorch import TransformerEmbedding # Capa de incrustaci\u00f3n emb_type = \"hidden_state\" layers = [ - x for x in range ( 1 , 9 )] # last 8 layers embedding_layer = TransformerEmbedding ( model_name , emb_type , layers ) # Dimensi\u00f3n de incrustaci\u00f3n embedding_dim = embedding_layer . model . embeddings . word_embeddings . embedding_dim embedding_dim","title":"Multivariado"},{"location":"Pruebas/C%C3%B3digo/","text":"Testing de C\u00f3digo Aplicaci\u00f3n Comenzaremos creando un directorio de pruebas separado con un subdirectorio de c\u00f3digo para probar nuestros scripts. Mas adelante crearemos subdirectorios para probar los datos y los modelos. tests/ \u2514\u2500\u2500 code/ \u2502 \u251c\u2500\u2500 test_data.py \u2502 \u251c\u2500\u2500 test_evaluate.py \u2502 \u251c\u2500\u2500 test_main.py \u2502 \u251c\u2500\u2500 test_predict.py \u2502 \u2514\u2500\u2500 test_utils.py mkdir tests cd tests mkdir code cd code touch test_data.py test_evaluate.py test_main.py test_predict.py test_utils.py test_args.json cd ../../ Vamos a utilizar pytest como nuestro framework de pruebas por sus poderosas caracter\u00edsticas incorporadas como parametrizaci\u00f3n, fixtures, marcadores y m\u00e1s. python -m pip install pytest == 7 .1.2 Dado que este paquete no es parte integral de las operaciones principales de machine learning, creemos una lista separada en nuestro setup.py y la agregamos a extras_require : # setup.py test_packages = [ \"pytest==7.1.2\" ] # Definir nuestro paquete setup ( ... extras_require ={ \"dev\" : docs_packages + style_packages + test_packages, \"docs\" : docs_packages, \"test\" : test_packages, } , ) De esta manera tendremos mayor flexibilidad, podemos ejecutar s\u00f3lo las pruebas, o todo los pasos necesarios para el entorno de desarrollo. Configuraci\u00f3n Pytest espera que las pruebas se organicen bajo un directorio tests por defecto. Una vez en el directorio, pytest busca scripts de python que empiecen por tests_*.py pero podemos configurarlo para que lea tambi\u00e9n cualquier otro patr\u00f3n de archivos. Podemos configurarlo en nuestro archivo pyproject.toml : # Pytest [tool.pytest.ini_options] testpaths = [\"tests\"] python_files = \"test_*.py\" Assertions Veamos c\u00f3mo es una prueba de ejemplo y sus resultados. Supongamos que tenemos una funci\u00f3n simple que determina si una fruta es fresca o no: # food/fruits.py def is_crisp ( fruit ): if fruit : fruit = fruit . lower () if fruit in [ \"apple\" , \"watermelon\" , \"cherries\" ]: return True elif fruit in [ \"orange\" , \"mango\" , \"strawberry\" ]: return False else : raise ValueError ( f \" { fruit } not in known list of fruits.\" ) return False Para probar esta funci\u00f3n, podemos utilizar sentencias assert para relacionar las entradas con las salidas esperadas. La sentencia que sigue a la palabra assert debe devolver True. Tambi\u00e9n es una buena idea agregar afirmaciones sobre las excepciones. # tests/food/test_fruits.py def test_is_crisp (): assert is_crisp ( fruit = \"apple\" ) assert is_crisp ( fruit = \"Apple\" ) assert not is_crisp ( fruit = \"orange\" ) with pytest . raises ( ValueError ): is_crisp ( fruit = None ) is_crisp ( fruit = \"pear\" ) Ejemplo # tests/code/test_evaluate.py import numpy as np import pytest from coe_template import evaluate def test_get_metrics (): y_true = np . array ([ 0 , 0 , 1 , 1 ]) y_pred = np . array ([ 0 , 1 , 0 , 1 ]) classes = [ \"a\" , \"b\" ] performance = evaluate . get_metrics ( y_true = y_true , y_pred = y_pred , classes = classes , df = None ) assert performance [ \"overall\" ][ \"precision\" ] == 2 / 4 assert performance [ \"overall\" ][ \"recall\" ] == 2 / 4 assert performance [ \"class\" ][ \"a\" ][ \"precision\" ] == 1 / 2 assert performance [ \"class\" ][ \"a\" ][ \"recall\" ] == 1 / 2 assert performance [ \"class\" ][ \"b\" ][ \"precision\" ] == 1 / 2 assert performance [ \"class\" ][ \"b\" ][ \"recall\" ] == 1 / 2 Ejecuci\u00f3n Podemos ejecutar nuestras pruebas anteriores utilizando varios niveles diferentes de granularidad: python -m pytest # todas las pruebas python -m pytest tests/food # pruebas bajo un directorio python -m pytest tests/food/test_fruits.py # pruebas para un solo archivo python -m pytest tests/food/test_fruits.py::test_is_crisp # pruebas para una sola funci\u00f3n Si alguna de nuestras afirmaciones en esta prueba fallara, ver\u00edamos las afirmaciones fallidas, junto con la salida esperada y real de nuestra funci\u00f3n. Clases Tambi\u00e9n podemos probar las clases y sus respectivas funciones creando clases de prueba. Dentro de nuestra clase de prueba, podemos definir opcionalmente funciones que se ejecutar\u00e1n autom\u00e1ticamente cuando configuremos o eliminemos una instancia de la clase o utilicemos un m\u00e9todo de la misma. setup_class : configura el estado de cualquier instancia de la clase. teardown_class : elimina el estado creado en setup_class. setup_method : se llama antes de cada m\u00e9todo para configurar cualquier estado. teardown_method : se llama despu\u00e9s de cada m\u00e9todo para desmontar cualquier estado. Podemos ejecutar todas las pruebas de nuestra clase especificando el nombre de la misma: python -m pytest tests/food/test_fruits.py::TestFruit Ejemplo de prueba de una clase # tests/code/test_data.py import tempfile from pathlib import Path import numpy as np import pandas as pd import pytest from coe_template import data class TestLabelEncoder : @classmethod def setup_class ( cls ): \"\"\"Llamada antes de cada inicializaci\u00f3n de clase.\"\"\" pass @classmethod def teardown_class ( cls ): \"\"\"Llamada despu\u00e9s de cada inicializaci\u00f3n de clase.\"\"\" pass def setup_method ( self ): \"\"\"Llamada antes de cada m\u00e9todo.\"\"\" self . label_encoder = data . LabelEncoder () def teardown_method ( self ): \"\"\"Llamada despu\u00e9s de cada m\u00e9todo.\"\"\" del self . label_encoder def test_empty_init ( self ): label_encoder = data . LabelEncoder () assert label_encoder . index_to_class == {} assert len ( label_encoder . classes ) == 0 def test_dict_init ( self ): class_to_index = { \"apple\" : 0 , \"banana\" : 1 } label_encoder = data . LabelEncoder ( class_to_index = class_to_index ) assert label_encoder . index_to_class == { 0 : \"apple\" , 1 : \"banana\" } assert len ( label_encoder . classes ) == 2 def test_len ( self ): assert len ( self . label_encoder ) == 0 def test_save_and_load ( self ): with tempfile . TemporaryDirectory () as dp : fp = Path ( dp , \"label_encoder.json\" ) self . label_encoder . save ( fp = fp ) label_encoder = data . LabelEncoder . load ( fp = fp ) assert len ( label_encoder . classes ) == 0 def test_str ( self ): assert str ( data . LabelEncoder ()) == \"<LabelEncoder(num_classes=0)>\" def test_fit ( self ): label_encoder = data . LabelEncoder () label_encoder . fit ([ \"apple\" , \"apple\" , \"banana\" ]) assert \"apple\" in label_encoder . class_to_index assert \"banana\" in label_encoder . class_to_index assert len ( label_encoder . classes ) == 2 def test_encode_decode ( self ): class_to_index = { \"apple\" : 0 , \"banana\" : 1 } y_encoded = [ 0 , 0 , 1 ] y_decoded = [ \"apple\" , \"apple\" , \"banana\" ] label_encoder = data . LabelEncoder ( class_to_index = class_to_index ) label_encoder . fit ([ \"apple\" , \"apple\" , \"banana\" ]) assert np . array_equal ( label_encoder . encode ( y_decoded ), np . array ( y_encoded )) assert label_encoder . decode ( y_encoded ) == y_decoded Parametrizaci\u00f3n Para eliminar la redundancia en las pruebas, pytest tiene el decorador @pytest.mark.parametrize que nos permite representar nuestras entradas y salidas como par\u00e1metros. @pytest . mark . parametrize ( \"fruit, crisp\" , [ ( \"apple\" , True ), ( \"Apple\" , True ), ( \"orange\" , False ), ], ) def test_is_crisp_parametrize ( fruit , crisp ): assert is_crisp ( fruit = fruit ) == crisp Del mismo modo, tambi\u00e9n podr\u00edamos pasar una excepci\u00f3n como resultado esperado: @pytest . mark . parametrize ( \"fruit, exception\" , [ ( \"pear\" , ValueError ), ], ) def test_is_crisp_exceptions ( fruit , exception ): with pytest . raises ( exception ): is_crisp ( fruit = fruit ) Ejemplo # tests/code/test_data.py @pytest . mark . parametrize ( \"text, lower, stem, stopwords, cleaned_text\" , [ ( \"Hello worlds\" , False , False , [], \"Hello worlds\" ), ( \"Hello worlds\" , True , False , [], \"hello worlds\" ), ( \"Hello worlds\" , False , True , [], \"Hello world\" ), ( \"Hello worlds\" , True , True , [], \"hello world\" ), ( \"Hello worlds\" , True , True , [ \"world\" ], \"hello world\" ), ( \"Hello worlds\" , True , True , [ \"worlds\" ], \"hello\" ), ], ) def test_clean_text ( text , lower , stem , stopwords , cleaned_text ): assert ( data . clean_text ( text = text , lower = lower , stem = stem , stopwords = stopwords , ) == cleaned_text ) Fixtures Asi como la parametrizaci\u00f3n nos permite reducir la redundancia dentro de las funciones de prueba, los fixtures nos permite reducir la redundancia entre diferentes funciones de prueba, ya que se ejecuta antes de la funci\u00f3n de prueba. @pytest . fixture def my_fruit (): fruit = Fruit ( name = \"apple\" ) return fruit def test_fruit ( my_fruit ): assert my_fruit . name == \"apple\" Tambi\u00e9n podemos aplicar fixtures a las clases donde la funci\u00f3n fixture ser\u00e1 invocada cuando se llame a cualquier m\u00e9todo de la clase. @pytest . mark . usefixtures ( \"my_fruit\" ) class TestFruit : ... Ejemplo # tests/code/test_data.py @pytest . fixture ( scope = \"module\" ) def df (): data = [ { \"title\" : \"a0\" , \"description\" : \"b0\" , \"tag\" : \"c0\" }, { \"title\" : \"a1\" , \"description\" : \"b1\" , \"tag\" : \"c1\" }, { \"title\" : \"a2\" , \"description\" : \"b2\" , \"tag\" : \"c1\" }, { \"title\" : \"a3\" , \"description\" : \"b3\" , \"tag\" : \"c2\" }, { \"title\" : \"a4\" , \"description\" : \"b4\" , \"tag\" : \"c2\" }, { \"title\" : \"a5\" , \"description\" : \"b5\" , \"tag\" : \"c2\" }, ] df = pd . DataFrame ( data * 10 ) return df @pytest . mark . parametrize ( \"labels, unique_labels\" , [ ([], [ \"other\" ]), # ning\u00fan conjunto de etiquetas aprobadas ([ \"c3\" ], [ \"other\" ]), # sin superposici\u00f3n de etiquetas aprobadas/reales ([ \"c0\" ], [ \"c0\" , \"other\" ]), # superposici\u00f3n parcial ([ \"c0\" , \"c1\" , \"c2\" ], [ \"c0\" , \"c1\" , \"c2\" ]), # superposici\u00f3n completa ], ) def test_replace_oos_labels ( df , labels , unique_labels ): replaced_df = data . replace_oos_labels ( df = df . copy (), labels = labels , label_col = \"tag\" , oos_label = \"other\" ) assert set ( replaced_df . tag . unique ()) == set ( unique_labels ) Los fixture pueden tener diferentes alcances dependiendo de c\u00f3mo queramos utilizarlos. function : el fixture se destruye despu\u00e9s de cada prueba. [default] class : el fixture se destruye despu\u00e9s de la \u00faltima prueba de la clase. module : el fixture se destruye despu\u00e9s de la \u00faltima prueba del m\u00f3dulo (script). package : el fixture se destruye despu\u00e9s de la \u00faltima prueba en el paquete. session : el fixture se destruye despu\u00e9s de la \u00faltima prueba de la sesi\u00f3n. Los fixtures de mayor nivel de alcance se ejecutan primero. Normalmente, cuando tenemos muchos fixtures en un archivo de prueba concreto, podemos organizarlos todos en un script fixtures.py e invocarlos seg\u00fan sea necesario. Marcadores Podemos crear una granularidad de ejecuci\u00f3n personalizada utilizando marcadores. Hay varios marcadores incorporados: ya hemos utilizado parametrize. El marcador skipif nos permite saltar la ejecuci\u00f3n de una prueba si se cumple una condici\u00f3n. Por ejemplo, supongamos que s\u00f3lo queremos probar el entrenamiento de nuestro modelo si hay una GPU disponible: @pytest . mark . skipif ( not torch . cuda . is_available (), reason = \"Las pruebas de entrenamiento completas requieren una GPU.\" ) def test_training (): pass Tambi\u00e9n podemos crear nuestros propios marcadores personalizados: @pytest . mark . fruits def test_fruit ( my_fruit ): assert my_fruit . name == \"apple\" Podemos ejecutarlos utilizando el flag -m que requiere una expresi\u00f3n marcadora: pytest -m \"fruits\" # ejecuta todas las pruebas marcadas con `fruits` pytest -m \"not fruits\" # ejecuta todas las pruebas excepto las marcadas con `fruits` La forma correcta de usar marcadores es listar expl\u00edcitamente los que hemos creado en nuestro archivo pyproject.toml . Aqu\u00ed podemos especificar que todos los marcadores deben definirse en este archivo con el flag --strict-markers y luego declarar nuestros marcadores: @pytest . mark . training def test_train_model (): assert ... # Pytest [tool.pytest.ini_options] testpaths = [\"tests\"] python_files = \"test_*.py\" addopts = \"--strict-markers --disable-pytest-warnings\" markers = [ \"training: tests that involve training\", ] Coverage A medida que desarrollamos pruebas para los componentes de nuestra aplicaci\u00f3n, es importante saber qu\u00e9 tan bien estamos cubriendo nuestra base de c\u00f3digo y saber si hemos omitido algo. Podemos utilizar la librer\u00eda Coverage para rastrear y visualizar qu\u00e9 parte de nuestro c\u00f3digo base cubren nuestras pruebas. Con pytest, es a\u00fan m\u00e1s f\u00e1cil utilizar este paquete gracias al plugin pytest-cov. python -m pip install pytest-cov == 2 .10.1 Y agregaremos esto a nuestro script setup.py : # setup.py test_packages = [ \"pytest==7.1.2\" , \"pytest-cov==2.10.1\" ] python -m pytest --cov coe_template --cov-report html Una vez que nuestras pruebas se han completado, podemos ver el informe generado (por defecto es htmlcov/index.html ) y hacer clic en archivos individuales para ver qu\u00e9 partes no fueron cubiertas por ninguna prueba. Esto es especialmente \u00fatil cuando nos olvidamos de probar ciertas condiciones, excepciones, etc. Exclusiones A veces no tiene sentido escribir pruebas que cubran todas las l\u00edneas de nuestra aplicaci\u00f3n, pero a\u00fan as\u00ed queremos tener en cuenta estas l\u00edneas para poder mantener una cobertura del 100%. Tenemos dos niveles de alcance al aplicar exclusiones: Excluir l\u00edneas a\u00f1adiendo este comentario # pragma: no cover, <MESSAGE> if trial : # pragma: no cover, optuna pruning trial . report ( val_loss , epoch ) if trial . should_prune (): raise optuna . TrialPruned () Excluyendo archivos al especificarlos en el archivo de configuraci\u00f3n pyproject.toml : # Pytest coverage [tool.coverage.run] omit = [\"app/gunicorn.py\"] Es importante a\u00f1adir una justificaci\u00f3n a estas exclusiones mediante comentarios para que nuestro equipo pueda seguir nuestro razonamiento.","title":"Testing de C\u00f3digo"},{"location":"Pruebas/C%C3%B3digo/#testing-de-codigo","text":"","title":"Testing de C\u00f3digo"},{"location":"Pruebas/C%C3%B3digo/#aplicacion","text":"Comenzaremos creando un directorio de pruebas separado con un subdirectorio de c\u00f3digo para probar nuestros scripts. Mas adelante crearemos subdirectorios para probar los datos y los modelos. tests/ \u2514\u2500\u2500 code/ \u2502 \u251c\u2500\u2500 test_data.py \u2502 \u251c\u2500\u2500 test_evaluate.py \u2502 \u251c\u2500\u2500 test_main.py \u2502 \u251c\u2500\u2500 test_predict.py \u2502 \u2514\u2500\u2500 test_utils.py mkdir tests cd tests mkdir code cd code touch test_data.py test_evaluate.py test_main.py test_predict.py test_utils.py test_args.json cd ../../ Vamos a utilizar pytest como nuestro framework de pruebas por sus poderosas caracter\u00edsticas incorporadas como parametrizaci\u00f3n, fixtures, marcadores y m\u00e1s. python -m pip install pytest == 7 .1.2 Dado que este paquete no es parte integral de las operaciones principales de machine learning, creemos una lista separada en nuestro setup.py y la agregamos a extras_require : # setup.py test_packages = [ \"pytest==7.1.2\" ] # Definir nuestro paquete setup ( ... extras_require ={ \"dev\" : docs_packages + style_packages + test_packages, \"docs\" : docs_packages, \"test\" : test_packages, } , ) De esta manera tendremos mayor flexibilidad, podemos ejecutar s\u00f3lo las pruebas, o todo los pasos necesarios para el entorno de desarrollo.","title":"Aplicaci\u00f3n"},{"location":"Pruebas/C%C3%B3digo/#configuracion","text":"Pytest espera que las pruebas se organicen bajo un directorio tests por defecto. Una vez en el directorio, pytest busca scripts de python que empiecen por tests_*.py pero podemos configurarlo para que lea tambi\u00e9n cualquier otro patr\u00f3n de archivos. Podemos configurarlo en nuestro archivo pyproject.toml : # Pytest [tool.pytest.ini_options] testpaths = [\"tests\"] python_files = \"test_*.py\"","title":"Configuraci\u00f3n"},{"location":"Pruebas/C%C3%B3digo/#assertions","text":"Veamos c\u00f3mo es una prueba de ejemplo y sus resultados. Supongamos que tenemos una funci\u00f3n simple que determina si una fruta es fresca o no: # food/fruits.py def is_crisp ( fruit ): if fruit : fruit = fruit . lower () if fruit in [ \"apple\" , \"watermelon\" , \"cherries\" ]: return True elif fruit in [ \"orange\" , \"mango\" , \"strawberry\" ]: return False else : raise ValueError ( f \" { fruit } not in known list of fruits.\" ) return False Para probar esta funci\u00f3n, podemos utilizar sentencias assert para relacionar las entradas con las salidas esperadas. La sentencia que sigue a la palabra assert debe devolver True. Tambi\u00e9n es una buena idea agregar afirmaciones sobre las excepciones. # tests/food/test_fruits.py def test_is_crisp (): assert is_crisp ( fruit = \"apple\" ) assert is_crisp ( fruit = \"Apple\" ) assert not is_crisp ( fruit = \"orange\" ) with pytest . raises ( ValueError ): is_crisp ( fruit = None ) is_crisp ( fruit = \"pear\" ) Ejemplo # tests/code/test_evaluate.py import numpy as np import pytest from coe_template import evaluate def test_get_metrics (): y_true = np . array ([ 0 , 0 , 1 , 1 ]) y_pred = np . array ([ 0 , 1 , 0 , 1 ]) classes = [ \"a\" , \"b\" ] performance = evaluate . get_metrics ( y_true = y_true , y_pred = y_pred , classes = classes , df = None ) assert performance [ \"overall\" ][ \"precision\" ] == 2 / 4 assert performance [ \"overall\" ][ \"recall\" ] == 2 / 4 assert performance [ \"class\" ][ \"a\" ][ \"precision\" ] == 1 / 2 assert performance [ \"class\" ][ \"a\" ][ \"recall\" ] == 1 / 2 assert performance [ \"class\" ][ \"b\" ][ \"precision\" ] == 1 / 2 assert performance [ \"class\" ][ \"b\" ][ \"recall\" ] == 1 / 2","title":"Assertions"},{"location":"Pruebas/C%C3%B3digo/#ejecucion","text":"Podemos ejecutar nuestras pruebas anteriores utilizando varios niveles diferentes de granularidad: python -m pytest # todas las pruebas python -m pytest tests/food # pruebas bajo un directorio python -m pytest tests/food/test_fruits.py # pruebas para un solo archivo python -m pytest tests/food/test_fruits.py::test_is_crisp # pruebas para una sola funci\u00f3n Si alguna de nuestras afirmaciones en esta prueba fallara, ver\u00edamos las afirmaciones fallidas, junto con la salida esperada y real de nuestra funci\u00f3n.","title":"Ejecuci\u00f3n"},{"location":"Pruebas/C%C3%B3digo/#clases","text":"Tambi\u00e9n podemos probar las clases y sus respectivas funciones creando clases de prueba. Dentro de nuestra clase de prueba, podemos definir opcionalmente funciones que se ejecutar\u00e1n autom\u00e1ticamente cuando configuremos o eliminemos una instancia de la clase o utilicemos un m\u00e9todo de la misma. setup_class : configura el estado de cualquier instancia de la clase. teardown_class : elimina el estado creado en setup_class. setup_method : se llama antes de cada m\u00e9todo para configurar cualquier estado. teardown_method : se llama despu\u00e9s de cada m\u00e9todo para desmontar cualquier estado. Podemos ejecutar todas las pruebas de nuestra clase especificando el nombre de la misma: python -m pytest tests/food/test_fruits.py::TestFruit Ejemplo de prueba de una clase # tests/code/test_data.py import tempfile from pathlib import Path import numpy as np import pandas as pd import pytest from coe_template import data class TestLabelEncoder : @classmethod def setup_class ( cls ): \"\"\"Llamada antes de cada inicializaci\u00f3n de clase.\"\"\" pass @classmethod def teardown_class ( cls ): \"\"\"Llamada despu\u00e9s de cada inicializaci\u00f3n de clase.\"\"\" pass def setup_method ( self ): \"\"\"Llamada antes de cada m\u00e9todo.\"\"\" self . label_encoder = data . LabelEncoder () def teardown_method ( self ): \"\"\"Llamada despu\u00e9s de cada m\u00e9todo.\"\"\" del self . label_encoder def test_empty_init ( self ): label_encoder = data . LabelEncoder () assert label_encoder . index_to_class == {} assert len ( label_encoder . classes ) == 0 def test_dict_init ( self ): class_to_index = { \"apple\" : 0 , \"banana\" : 1 } label_encoder = data . LabelEncoder ( class_to_index = class_to_index ) assert label_encoder . index_to_class == { 0 : \"apple\" , 1 : \"banana\" } assert len ( label_encoder . classes ) == 2 def test_len ( self ): assert len ( self . label_encoder ) == 0 def test_save_and_load ( self ): with tempfile . TemporaryDirectory () as dp : fp = Path ( dp , \"label_encoder.json\" ) self . label_encoder . save ( fp = fp ) label_encoder = data . LabelEncoder . load ( fp = fp ) assert len ( label_encoder . classes ) == 0 def test_str ( self ): assert str ( data . LabelEncoder ()) == \"<LabelEncoder(num_classes=0)>\" def test_fit ( self ): label_encoder = data . LabelEncoder () label_encoder . fit ([ \"apple\" , \"apple\" , \"banana\" ]) assert \"apple\" in label_encoder . class_to_index assert \"banana\" in label_encoder . class_to_index assert len ( label_encoder . classes ) == 2 def test_encode_decode ( self ): class_to_index = { \"apple\" : 0 , \"banana\" : 1 } y_encoded = [ 0 , 0 , 1 ] y_decoded = [ \"apple\" , \"apple\" , \"banana\" ] label_encoder = data . LabelEncoder ( class_to_index = class_to_index ) label_encoder . fit ([ \"apple\" , \"apple\" , \"banana\" ]) assert np . array_equal ( label_encoder . encode ( y_decoded ), np . array ( y_encoded )) assert label_encoder . decode ( y_encoded ) == y_decoded","title":"Clases"},{"location":"Pruebas/C%C3%B3digo/#parametrizacion","text":"Para eliminar la redundancia en las pruebas, pytest tiene el decorador @pytest.mark.parametrize que nos permite representar nuestras entradas y salidas como par\u00e1metros. @pytest . mark . parametrize ( \"fruit, crisp\" , [ ( \"apple\" , True ), ( \"Apple\" , True ), ( \"orange\" , False ), ], ) def test_is_crisp_parametrize ( fruit , crisp ): assert is_crisp ( fruit = fruit ) == crisp Del mismo modo, tambi\u00e9n podr\u00edamos pasar una excepci\u00f3n como resultado esperado: @pytest . mark . parametrize ( \"fruit, exception\" , [ ( \"pear\" , ValueError ), ], ) def test_is_crisp_exceptions ( fruit , exception ): with pytest . raises ( exception ): is_crisp ( fruit = fruit ) Ejemplo # tests/code/test_data.py @pytest . mark . parametrize ( \"text, lower, stem, stopwords, cleaned_text\" , [ ( \"Hello worlds\" , False , False , [], \"Hello worlds\" ), ( \"Hello worlds\" , True , False , [], \"hello worlds\" ), ( \"Hello worlds\" , False , True , [], \"Hello world\" ), ( \"Hello worlds\" , True , True , [], \"hello world\" ), ( \"Hello worlds\" , True , True , [ \"world\" ], \"hello world\" ), ( \"Hello worlds\" , True , True , [ \"worlds\" ], \"hello\" ), ], ) def test_clean_text ( text , lower , stem , stopwords , cleaned_text ): assert ( data . clean_text ( text = text , lower = lower , stem = stem , stopwords = stopwords , ) == cleaned_text )","title":"Parametrizaci\u00f3n"},{"location":"Pruebas/C%C3%B3digo/#fixtures","text":"Asi como la parametrizaci\u00f3n nos permite reducir la redundancia dentro de las funciones de prueba, los fixtures nos permite reducir la redundancia entre diferentes funciones de prueba, ya que se ejecuta antes de la funci\u00f3n de prueba. @pytest . fixture def my_fruit (): fruit = Fruit ( name = \"apple\" ) return fruit def test_fruit ( my_fruit ): assert my_fruit . name == \"apple\" Tambi\u00e9n podemos aplicar fixtures a las clases donde la funci\u00f3n fixture ser\u00e1 invocada cuando se llame a cualquier m\u00e9todo de la clase. @pytest . mark . usefixtures ( \"my_fruit\" ) class TestFruit : ... Ejemplo # tests/code/test_data.py @pytest . fixture ( scope = \"module\" ) def df (): data = [ { \"title\" : \"a0\" , \"description\" : \"b0\" , \"tag\" : \"c0\" }, { \"title\" : \"a1\" , \"description\" : \"b1\" , \"tag\" : \"c1\" }, { \"title\" : \"a2\" , \"description\" : \"b2\" , \"tag\" : \"c1\" }, { \"title\" : \"a3\" , \"description\" : \"b3\" , \"tag\" : \"c2\" }, { \"title\" : \"a4\" , \"description\" : \"b4\" , \"tag\" : \"c2\" }, { \"title\" : \"a5\" , \"description\" : \"b5\" , \"tag\" : \"c2\" }, ] df = pd . DataFrame ( data * 10 ) return df @pytest . mark . parametrize ( \"labels, unique_labels\" , [ ([], [ \"other\" ]), # ning\u00fan conjunto de etiquetas aprobadas ([ \"c3\" ], [ \"other\" ]), # sin superposici\u00f3n de etiquetas aprobadas/reales ([ \"c0\" ], [ \"c0\" , \"other\" ]), # superposici\u00f3n parcial ([ \"c0\" , \"c1\" , \"c2\" ], [ \"c0\" , \"c1\" , \"c2\" ]), # superposici\u00f3n completa ], ) def test_replace_oos_labels ( df , labels , unique_labels ): replaced_df = data . replace_oos_labels ( df = df . copy (), labels = labels , label_col = \"tag\" , oos_label = \"other\" ) assert set ( replaced_df . tag . unique ()) == set ( unique_labels ) Los fixture pueden tener diferentes alcances dependiendo de c\u00f3mo queramos utilizarlos. function : el fixture se destruye despu\u00e9s de cada prueba. [default] class : el fixture se destruye despu\u00e9s de la \u00faltima prueba de la clase. module : el fixture se destruye despu\u00e9s de la \u00faltima prueba del m\u00f3dulo (script). package : el fixture se destruye despu\u00e9s de la \u00faltima prueba en el paquete. session : el fixture se destruye despu\u00e9s de la \u00faltima prueba de la sesi\u00f3n. Los fixtures de mayor nivel de alcance se ejecutan primero. Normalmente, cuando tenemos muchos fixtures en un archivo de prueba concreto, podemos organizarlos todos en un script fixtures.py e invocarlos seg\u00fan sea necesario.","title":"Fixtures"},{"location":"Pruebas/C%C3%B3digo/#marcadores","text":"Podemos crear una granularidad de ejecuci\u00f3n personalizada utilizando marcadores. Hay varios marcadores incorporados: ya hemos utilizado parametrize. El marcador skipif nos permite saltar la ejecuci\u00f3n de una prueba si se cumple una condici\u00f3n. Por ejemplo, supongamos que s\u00f3lo queremos probar el entrenamiento de nuestro modelo si hay una GPU disponible: @pytest . mark . skipif ( not torch . cuda . is_available (), reason = \"Las pruebas de entrenamiento completas requieren una GPU.\" ) def test_training (): pass Tambi\u00e9n podemos crear nuestros propios marcadores personalizados: @pytest . mark . fruits def test_fruit ( my_fruit ): assert my_fruit . name == \"apple\" Podemos ejecutarlos utilizando el flag -m que requiere una expresi\u00f3n marcadora: pytest -m \"fruits\" # ejecuta todas las pruebas marcadas con `fruits` pytest -m \"not fruits\" # ejecuta todas las pruebas excepto las marcadas con `fruits` La forma correcta de usar marcadores es listar expl\u00edcitamente los que hemos creado en nuestro archivo pyproject.toml . Aqu\u00ed podemos especificar que todos los marcadores deben definirse en este archivo con el flag --strict-markers y luego declarar nuestros marcadores: @pytest . mark . training def test_train_model (): assert ... # Pytest [tool.pytest.ini_options] testpaths = [\"tests\"] python_files = \"test_*.py\" addopts = \"--strict-markers --disable-pytest-warnings\" markers = [ \"training: tests that involve training\", ]","title":"Marcadores"},{"location":"Pruebas/C%C3%B3digo/#coverage","text":"A medida que desarrollamos pruebas para los componentes de nuestra aplicaci\u00f3n, es importante saber qu\u00e9 tan bien estamos cubriendo nuestra base de c\u00f3digo y saber si hemos omitido algo. Podemos utilizar la librer\u00eda Coverage para rastrear y visualizar qu\u00e9 parte de nuestro c\u00f3digo base cubren nuestras pruebas. Con pytest, es a\u00fan m\u00e1s f\u00e1cil utilizar este paquete gracias al plugin pytest-cov. python -m pip install pytest-cov == 2 .10.1 Y agregaremos esto a nuestro script setup.py : # setup.py test_packages = [ \"pytest==7.1.2\" , \"pytest-cov==2.10.1\" ] python -m pytest --cov coe_template --cov-report html Una vez que nuestras pruebas se han completado, podemos ver el informe generado (por defecto es htmlcov/index.html ) y hacer clic en archivos individuales para ver qu\u00e9 partes no fueron cubiertas por ninguna prueba. Esto es especialmente \u00fatil cuando nos olvidamos de probar ciertas condiciones, excepciones, etc.","title":"Coverage"},{"location":"Pruebas/C%C3%B3digo/#exclusiones","text":"A veces no tiene sentido escribir pruebas que cubran todas las l\u00edneas de nuestra aplicaci\u00f3n, pero a\u00fan as\u00ed queremos tener en cuenta estas l\u00edneas para poder mantener una cobertura del 100%. Tenemos dos niveles de alcance al aplicar exclusiones: Excluir l\u00edneas a\u00f1adiendo este comentario # pragma: no cover, <MESSAGE> if trial : # pragma: no cover, optuna pruning trial . report ( val_loss , epoch ) if trial . should_prune (): raise optuna . TrialPruned () Excluyendo archivos al especificarlos en el archivo de configuraci\u00f3n pyproject.toml : # Pytest coverage [tool.coverage.run] omit = [\"app/gunicorn.py\"] Es importante a\u00f1adir una justificaci\u00f3n a estas exclusiones mediante comentarios para que nuestro equipo pueda seguir nuestro razonamiento.","title":"Exclusiones"},{"location":"Pruebas/Datos/","text":"Testing de Datos Hasta ahora, hemos utilizado pruebas unitarias y de integraci\u00f3n para probar las funciones que interact\u00faan con nuestros datos, pero no hemos probado la validez de los datos en s\u00ed. Vamos a utilizar la librer\u00eda great expectations para probar c\u00f3mo se espera que sean nuestros datos. Es una librer\u00eda que nos permite crear expectativas sobre el aspecto que deber\u00edan tener nuestros datos de forma estandarizada. Tambi\u00e9n proporciona m\u00f3dulos para conectarse sin problemas con fuentes de datos de backend como sistemas de archivos locales, cloud, bases de datos, etc. python -m pip install great-expectations == 0 .15.15 Y agregaremos esto a nuestro script setup.py : # setup.py test_packages = [ \"pytest==7.1.2\" , \"pytest-cov==2.10.1\" , \"great-expectations==0.15.15\" ] Primero cargaremos los datos sobre los que queremos aplicar nuestras expectativas. import great_expectations as ge import json import pandas as pd from urllib.request import urlopen # Cargar proyectos etiquetados projects = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\" ) tags = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\" ) df = ge . dataset . PandasDataset ( pd . merge ( projects , tags , on = \"id\" )) print ( f \" { len ( df ) } projects\" ) df . head ( 5 ) Expectativas A la hora de crear expectativas sobre el aspecto que deben tener nuestros datos, debemos pensar en todo nuestro conjunto de datos y en todos los features (columnas) que contiene. # Presencia de features espec\u00edficos. df . expect_table_columns_to_match_ordered_list ( column_list = [ \"id\" , \"created_on\" , \"title\" , \"description\" , \"tag\" ] ) # Combinaciones \u00fanicas de features (\u00a1detecta fugas de datos!) df . expect_compound_columns_to_be_unique ( column_list = [ \"title\" , \"description\" ]) # Valores faltantes df . expect_column_values_to_not_be_null ( column = \"tag\" ) # Valores \u00fanicos df . expect_column_values_to_be_unique ( column = \"id\" ) # Adherencia al tipo df . expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # Lista (categ\u00f3rica) / rango (continuo) de valores permitidos tags = [ \"computer-vision\" , \"graph-learning\" , \"reinforcement-learning\" , \"natural-language-processing\" , \"mlops\" , \"time-series\" ] df . expect_column_values_to_be_in_set ( column = \"tag\" , value_set = tags ) Cada una de estas expectativas crear\u00e1 una salida con detalles sobre el \u00e9xito o el fracaso, los valores esperados y observados, las expectativas planteadas, etc. \u00c9stas son s\u00f3lo algunas de las diferentes expectativas que podemos crear. Aqu\u00ed hay otras expectativas populares: relaciones de valores de features con otros valores de features \u2192 expect_column_pair_values_a_to_be_greater_than_b recuento de filas (exacto o en un rango) de muestras \u2192 expect_table_row_count_to_be_between estad\u00edsticas de valores (media, std, mediana, max, min, suma, etc.) \u2192 expect_column_mean_to_be_between Organizaci\u00f3n A la hora de organizar las expectativas, se recomienda empezar por las de nivel de tabla y luego pasar a las columnas de features individuales. Expectativas de la tabla : # columnas df . expect_table_columns_to_match_ordered_list ( column_list = [ \"id\" , \"created_on\" , \"title\" , \"description\" , \"tag\" ]) # data leak df . expect_compound_columns_to_be_unique ( column_list = [ \"title\" , \"description\" ]) Expectativas de columnas : # id df . expect_column_values_to_be_unique ( column = \"id\" ) # created_on df . expect_column_values_to_not_be_null ( column = \"created_on\" ) df . expect_column_values_to_match_strftime_format ( column = \"created_on\" , strftime_format = \"%Y-%m- %d %H:%M:%S\" ) # title df . expect_column_values_to_not_be_null ( column = \"title\" ) df . expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # description df . expect_column_values_to_not_be_null ( column = \"description\" ) df . expect_column_values_to_be_of_type ( column = \"description\" , type_ = \"str\" ) # tag df . expect_column_values_to_not_be_null ( column = \"tag\" ) df . expect_column_values_to_be_of_type ( column = \"tag\" , type_ = \"str\" ) Podemos agrupar todas las expectativas para crear un objeto Expectation Suite que podemos utilizar para validar cualquier m\u00f3dulo Dataset. # Expectation suite expectation_suite = df . get_expectation_suite ( discard_failed_expectations = False ) print ( df . validate ( expectation_suite = expectation_suite , only_return_failures = True )) Proyectos Hasta ahora hemos trabajado con la librer\u00eda Great Expectations a nivel de script / notebook, pero podemos organizar a\u00fan m\u00e1s nuestras expectativas creando un Proyecto. cd tests great_expectations init Esto crear\u00e1 un directorio tests/great_expectations con la siguiente estructura: tests/great_expectations/ \u251c\u2500\u2500 checkpoints/ \u251c\u2500\u2500 expectations/ \u251c\u2500\u2500 plugins/ \u251c\u2500\u2500 uncommitted/ \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 great_expectations.yml Data source El primer paso es establecer nuestro datasource: great_expectations datasource new What data would you like Great Expectations to connect to? 1 . Files on a filesystem ( for processing with Pandas or Spark ) \ud83d\udc48 2 . Relational database ( SQL ) What are you processing your files with? 1 . Pandas \ud83d\udc48 2 . PySpark Enter the path of the root directory where the data files are stored: ../data Ejecutamos las celdas en el notebook generado y cambiamos el datasource_name por local_data . Despu\u00e9s de ejecutar las celdas, podemos cerrar el notebook y podemos ver el Datasource que fue a\u00f1adido a great_expectations.yml . Suites Cree expectativas de forma manual, interactiva o autom\u00e1tica y gu\u00e1rdelas como suites (un conjunto de expectativas para un activo de datos concreto). great_expectations suite new How would you like to create your Expectation Suite? 1 . Manually, without interacting with a sample batch of data ( default ) 2 . Interactively, with a sample batch of data \ud83d\udc48 3 . Automatically, using a profiler Which data asset ( accessible by data connector \"default_inferred_data_connector_name\" ) would you like to use? 1 . labeled_projects.csv 2 . projects.csv \ud83d\udc48 3 . tags.csv Name the new Expectation Suite [ projects.csv.warning ] : projects Esto abrir\u00e1 un notebook interactivo donde podemos a\u00f1adir expectativas. Copie y pegue las expectativas a continuaci\u00f3n y ejecute todas las celdas. Repita este paso para las tags.csv y labeled_projects.csv . Expectativas para projects.csv Expectativas de tabla # Presencia de features validator.expect_table_columns_to_match_ordered_list ( column_list =[ \"id\" , \"created_on\" , \"title\" , \"description\" ]) validator.expect_compound_columns_to_be_unique ( column_list =[ \"title\" , \"description\" ]) # data leak Expectativas de columna # id validator.expect_column_values_to_be_unique ( column = \"id\" ) # create_on validator.expect_column_values_to_not_be_null ( column = \"created_on\" ) validator.expect_column_values_to_match_strftime_format ( column = \"created_on\" , strftime_format = \"%Y-%m-%d %H:%M:%S\" ) # title validator.expect_column_values_to_not_be_null ( column = \"title\" ) validator.expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # description validator.expect_column_values_to_not_be_null ( column = \"description\" ) validator.expect_column_values_to_be_of_type ( column = \"description\" , type_ = \"str\" ) Expectativas para tags.csv Expectativas de tabla # Presencia de features validator.expect_table_columns_to_match_ordered_list ( column_list =[ \"id\" , \"tag\" ]) Expectativas de columna # id validator.expect_column_values_to_be_unique ( column = \"id\" ) # tag validator.expect_column_values_to_not_be_null ( column = \"tag\" ) validator.expect_column_values_to_be_of_type ( column = \"tag\" , type_ = \"str\" ) Expectativas para labeled_projects.csv Expectativas de tabla # Presencia de features validator.expect_table_columns_to_match_ordered_list ( column_list =[ \"id\" , \"created_on\" , \"title\" , \"description\" , \"tag\" ]) validator.expect_compound_columns_to_be_unique ( column_list =[ \"title\" , \"description\" ]) # data leak Expectativas de columna # id validator.expect_column_values_to_be_unique ( column = \"id\" ) # create_on validator.expect_column_values_to_not_be_null ( column = \"created_on\" ) validator.expect_column_values_to_match_strftime_format ( column = \"created_on\" , strftime_format = \"%Y-%m-%d %H:%M:%S\" ) # title validator.expect_column_values_to_not_be_null ( column = \"title\" ) validator.expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # description validator.expect_column_values_to_not_be_null ( column = \"description\" ) validator.expect_column_values_to_be_of_type ( column = \"description\" , type_ = \"str\" ) # tag validator.expect_column_values_to_not_be_null ( column = \"tag\" ) validator.expect_column_values_to_be_of_type ( column = \"tag\" , type_ = \"str\" ) Todas estas expectativas se guardar\u00e1n en great_expectations/expectations . Tambi\u00e9n podemos enumerar las suites con: great_expectations suite list Para editar una suite, podemos ejecutar el siguiente comando: great_expectations suite edit <SUITE_NAME> Checkpoints Crea checkpoints donde se aplica una suite de expectativas a un activo de datos espec\u00edfico. Esta es una gran manera de aplicar program\u00e1ticamente los puntos de control en nuestras fuentes de datos existentes y nuevas. Para nuestro ejemplo, ser\u00eda: cd tests great_expectations checkpoint new projects great_expectations checkpoint new tags great_expectations checkpoint new labeled_projects Cada llamada de creaci\u00f3n de checkpoint lanzar\u00e1 un notebook en el que podemos definir a qu\u00e9 suites aplicar este checkpoint. Tenemos que cambiar las l\u00edneas para data_asset_name (sobre qu\u00e9 activo de datos ejecutar el checkpoint suite) y expectation_suite_name (nombre de la suite a utilizar). Luego podemos ejecutarlo: great_expectations checkpoint run projects great_expectations checkpoint run tags great_expectations checkpoint run labeled_projects Documentaci\u00f3n Cuando creamos expectativas utilizando la aplicaci\u00f3n CLI, Great Expectations genera autom\u00e1ticamente documentaci\u00f3n para nuestras pruebas. Tambi\u00e9n almacena informaci\u00f3n sobre las ejecuciones de validaci\u00f3n y sus resultados. Podemos lanzar la documentaci\u00f3n de los datos generados con el siguiente comando: great_expectations docs build Por defecto, Great Expectations almacena nuestras expectativas, resultados y m\u00e9tricas localmente, pero para producci\u00f3n, querremos configurar almacenes de metadatos remotos. Producci\u00f3n La ventaja de utilizar una biblioteca como great expectations, frente a las sentencias assert aisladas, es que podemos: reducir los esfuerzos redundantes para crear pruebas en todas las modalidades de datos crear autom\u00e1ticamente puntos de control de las pruebas para ejecutarlas a medida que crece nuestro conjunto de datos generar autom\u00e1ticamente documentaci\u00f3n sobre las expectativas e informar sobre las ejecuciones conectar f\u00e1cilmente con fuentes de datos backend como sistemas de archivos locales, cloud, bases de datos, etc.","title":"Testing de Datos"},{"location":"Pruebas/Datos/#testing-de-datos","text":"Hasta ahora, hemos utilizado pruebas unitarias y de integraci\u00f3n para probar las funciones que interact\u00faan con nuestros datos, pero no hemos probado la validez de los datos en s\u00ed. Vamos a utilizar la librer\u00eda great expectations para probar c\u00f3mo se espera que sean nuestros datos. Es una librer\u00eda que nos permite crear expectativas sobre el aspecto que deber\u00edan tener nuestros datos de forma estandarizada. Tambi\u00e9n proporciona m\u00f3dulos para conectarse sin problemas con fuentes de datos de backend como sistemas de archivos locales, cloud, bases de datos, etc. python -m pip install great-expectations == 0 .15.15 Y agregaremos esto a nuestro script setup.py : # setup.py test_packages = [ \"pytest==7.1.2\" , \"pytest-cov==2.10.1\" , \"great-expectations==0.15.15\" ] Primero cargaremos los datos sobre los que queremos aplicar nuestras expectativas. import great_expectations as ge import json import pandas as pd from urllib.request import urlopen # Cargar proyectos etiquetados projects = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\" ) tags = pd . read_csv ( \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\" ) df = ge . dataset . PandasDataset ( pd . merge ( projects , tags , on = \"id\" )) print ( f \" { len ( df ) } projects\" ) df . head ( 5 )","title":"Testing de Datos"},{"location":"Pruebas/Datos/#expectativas","text":"A la hora de crear expectativas sobre el aspecto que deben tener nuestros datos, debemos pensar en todo nuestro conjunto de datos y en todos los features (columnas) que contiene. # Presencia de features espec\u00edficos. df . expect_table_columns_to_match_ordered_list ( column_list = [ \"id\" , \"created_on\" , \"title\" , \"description\" , \"tag\" ] ) # Combinaciones \u00fanicas de features (\u00a1detecta fugas de datos!) df . expect_compound_columns_to_be_unique ( column_list = [ \"title\" , \"description\" ]) # Valores faltantes df . expect_column_values_to_not_be_null ( column = \"tag\" ) # Valores \u00fanicos df . expect_column_values_to_be_unique ( column = \"id\" ) # Adherencia al tipo df . expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # Lista (categ\u00f3rica) / rango (continuo) de valores permitidos tags = [ \"computer-vision\" , \"graph-learning\" , \"reinforcement-learning\" , \"natural-language-processing\" , \"mlops\" , \"time-series\" ] df . expect_column_values_to_be_in_set ( column = \"tag\" , value_set = tags ) Cada una de estas expectativas crear\u00e1 una salida con detalles sobre el \u00e9xito o el fracaso, los valores esperados y observados, las expectativas planteadas, etc. \u00c9stas son s\u00f3lo algunas de las diferentes expectativas que podemos crear. Aqu\u00ed hay otras expectativas populares: relaciones de valores de features con otros valores de features \u2192 expect_column_pair_values_a_to_be_greater_than_b recuento de filas (exacto o en un rango) de muestras \u2192 expect_table_row_count_to_be_between estad\u00edsticas de valores (media, std, mediana, max, min, suma, etc.) \u2192 expect_column_mean_to_be_between","title":"Expectativas"},{"location":"Pruebas/Datos/#organizacion","text":"A la hora de organizar las expectativas, se recomienda empezar por las de nivel de tabla y luego pasar a las columnas de features individuales. Expectativas de la tabla : # columnas df . expect_table_columns_to_match_ordered_list ( column_list = [ \"id\" , \"created_on\" , \"title\" , \"description\" , \"tag\" ]) # data leak df . expect_compound_columns_to_be_unique ( column_list = [ \"title\" , \"description\" ]) Expectativas de columnas : # id df . expect_column_values_to_be_unique ( column = \"id\" ) # created_on df . expect_column_values_to_not_be_null ( column = \"created_on\" ) df . expect_column_values_to_match_strftime_format ( column = \"created_on\" , strftime_format = \"%Y-%m- %d %H:%M:%S\" ) # title df . expect_column_values_to_not_be_null ( column = \"title\" ) df . expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # description df . expect_column_values_to_not_be_null ( column = \"description\" ) df . expect_column_values_to_be_of_type ( column = \"description\" , type_ = \"str\" ) # tag df . expect_column_values_to_not_be_null ( column = \"tag\" ) df . expect_column_values_to_be_of_type ( column = \"tag\" , type_ = \"str\" ) Podemos agrupar todas las expectativas para crear un objeto Expectation Suite que podemos utilizar para validar cualquier m\u00f3dulo Dataset. # Expectation suite expectation_suite = df . get_expectation_suite ( discard_failed_expectations = False ) print ( df . validate ( expectation_suite = expectation_suite , only_return_failures = True ))","title":"Organizaci\u00f3n"},{"location":"Pruebas/Datos/#proyectos","text":"Hasta ahora hemos trabajado con la librer\u00eda Great Expectations a nivel de script / notebook, pero podemos organizar a\u00fan m\u00e1s nuestras expectativas creando un Proyecto. cd tests great_expectations init Esto crear\u00e1 un directorio tests/great_expectations con la siguiente estructura: tests/great_expectations/ \u251c\u2500\u2500 checkpoints/ \u251c\u2500\u2500 expectations/ \u251c\u2500\u2500 plugins/ \u251c\u2500\u2500 uncommitted/ \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 great_expectations.yml Data source El primer paso es establecer nuestro datasource: great_expectations datasource new What data would you like Great Expectations to connect to? 1 . Files on a filesystem ( for processing with Pandas or Spark ) \ud83d\udc48 2 . Relational database ( SQL ) What are you processing your files with? 1 . Pandas \ud83d\udc48 2 . PySpark Enter the path of the root directory where the data files are stored: ../data Ejecutamos las celdas en el notebook generado y cambiamos el datasource_name por local_data . Despu\u00e9s de ejecutar las celdas, podemos cerrar el notebook y podemos ver el Datasource que fue a\u00f1adido a great_expectations.yml . Suites Cree expectativas de forma manual, interactiva o autom\u00e1tica y gu\u00e1rdelas como suites (un conjunto de expectativas para un activo de datos concreto). great_expectations suite new How would you like to create your Expectation Suite? 1 . Manually, without interacting with a sample batch of data ( default ) 2 . Interactively, with a sample batch of data \ud83d\udc48 3 . Automatically, using a profiler Which data asset ( accessible by data connector \"default_inferred_data_connector_name\" ) would you like to use? 1 . labeled_projects.csv 2 . projects.csv \ud83d\udc48 3 . tags.csv Name the new Expectation Suite [ projects.csv.warning ] : projects Esto abrir\u00e1 un notebook interactivo donde podemos a\u00f1adir expectativas. Copie y pegue las expectativas a continuaci\u00f3n y ejecute todas las celdas. Repita este paso para las tags.csv y labeled_projects.csv . Expectativas para projects.csv Expectativas de tabla # Presencia de features validator.expect_table_columns_to_match_ordered_list ( column_list =[ \"id\" , \"created_on\" , \"title\" , \"description\" ]) validator.expect_compound_columns_to_be_unique ( column_list =[ \"title\" , \"description\" ]) # data leak Expectativas de columna # id validator.expect_column_values_to_be_unique ( column = \"id\" ) # create_on validator.expect_column_values_to_not_be_null ( column = \"created_on\" ) validator.expect_column_values_to_match_strftime_format ( column = \"created_on\" , strftime_format = \"%Y-%m-%d %H:%M:%S\" ) # title validator.expect_column_values_to_not_be_null ( column = \"title\" ) validator.expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # description validator.expect_column_values_to_not_be_null ( column = \"description\" ) validator.expect_column_values_to_be_of_type ( column = \"description\" , type_ = \"str\" ) Expectativas para tags.csv Expectativas de tabla # Presencia de features validator.expect_table_columns_to_match_ordered_list ( column_list =[ \"id\" , \"tag\" ]) Expectativas de columna # id validator.expect_column_values_to_be_unique ( column = \"id\" ) # tag validator.expect_column_values_to_not_be_null ( column = \"tag\" ) validator.expect_column_values_to_be_of_type ( column = \"tag\" , type_ = \"str\" ) Expectativas para labeled_projects.csv Expectativas de tabla # Presencia de features validator.expect_table_columns_to_match_ordered_list ( column_list =[ \"id\" , \"created_on\" , \"title\" , \"description\" , \"tag\" ]) validator.expect_compound_columns_to_be_unique ( column_list =[ \"title\" , \"description\" ]) # data leak Expectativas de columna # id validator.expect_column_values_to_be_unique ( column = \"id\" ) # create_on validator.expect_column_values_to_not_be_null ( column = \"created_on\" ) validator.expect_column_values_to_match_strftime_format ( column = \"created_on\" , strftime_format = \"%Y-%m-%d %H:%M:%S\" ) # title validator.expect_column_values_to_not_be_null ( column = \"title\" ) validator.expect_column_values_to_be_of_type ( column = \"title\" , type_ = \"str\" ) # description validator.expect_column_values_to_not_be_null ( column = \"description\" ) validator.expect_column_values_to_be_of_type ( column = \"description\" , type_ = \"str\" ) # tag validator.expect_column_values_to_not_be_null ( column = \"tag\" ) validator.expect_column_values_to_be_of_type ( column = \"tag\" , type_ = \"str\" ) Todas estas expectativas se guardar\u00e1n en great_expectations/expectations . Tambi\u00e9n podemos enumerar las suites con: great_expectations suite list Para editar una suite, podemos ejecutar el siguiente comando: great_expectations suite edit <SUITE_NAME> Checkpoints Crea checkpoints donde se aplica una suite de expectativas a un activo de datos espec\u00edfico. Esta es una gran manera de aplicar program\u00e1ticamente los puntos de control en nuestras fuentes de datos existentes y nuevas. Para nuestro ejemplo, ser\u00eda: cd tests great_expectations checkpoint new projects great_expectations checkpoint new tags great_expectations checkpoint new labeled_projects Cada llamada de creaci\u00f3n de checkpoint lanzar\u00e1 un notebook en el que podemos definir a qu\u00e9 suites aplicar este checkpoint. Tenemos que cambiar las l\u00edneas para data_asset_name (sobre qu\u00e9 activo de datos ejecutar el checkpoint suite) y expectation_suite_name (nombre de la suite a utilizar). Luego podemos ejecutarlo: great_expectations checkpoint run projects great_expectations checkpoint run tags great_expectations checkpoint run labeled_projects","title":"Proyectos"},{"location":"Pruebas/Datos/#documentacion","text":"Cuando creamos expectativas utilizando la aplicaci\u00f3n CLI, Great Expectations genera autom\u00e1ticamente documentaci\u00f3n para nuestras pruebas. Tambi\u00e9n almacena informaci\u00f3n sobre las ejecuciones de validaci\u00f3n y sus resultados. Podemos lanzar la documentaci\u00f3n de los datos generados con el siguiente comando: great_expectations docs build Por defecto, Great Expectations almacena nuestras expectativas, resultados y m\u00e9tricas localmente, pero para producci\u00f3n, querremos configurar almacenes de metadatos remotos.","title":"Documentaci\u00f3n"},{"location":"Pruebas/Datos/#produccion","text":"La ventaja de utilizar una biblioteca como great expectations, frente a las sentencias assert aisladas, es que podemos: reducir los esfuerzos redundantes para crear pruebas en todas las modalidades de datos crear autom\u00e1ticamente puntos de control de las pruebas para ejecutarlas a medida que crece nuestro conjunto de datos generar autom\u00e1ticamente documentaci\u00f3n sobre las expectativas e informar sobre las ejecuciones conectar f\u00e1cilmente con fuentes de datos backend como sistemas de archivos locales, cloud, bases de datos, etc.","title":"Producci\u00f3n"},{"location":"Pruebas/Modelos/","text":"Testing de Modelos El \u00faltimo aspecto de las pruebas de los sistemas de ML consiste en probar nuestros modelos durante el entrenamiento, la evaluaci\u00f3n, la inferencia y el despliegue. Entrenamiento Queremos escribir pruebas de forma iterativa mientras desarrollamos nuestros pipelines de entrenamiento para poder detectar errores r\u00e1pidamente. Esto es especialmente importante porque, a diferencia del software tradicional, los sistemas de ML pueden ejecutarse hasta el final sin lanzar ninguna excepci\u00f3n/error, pero pueden producir sistemas incorrectos. Adem\u00e1s, queremos detectar los errores r\u00e1pidamente para ahorrar tiempo y computo. Comprobar las formas y los valores de los resultados del modelo assert model ( inputs ) . shape == torch . Size ([ len ( inputs ), num_classes ]) Comprobar la disminuci\u00f3n de la p\u00e9rdida despu\u00e9s de una tanda de entrenamiento assert epoch_loss < prev_epoch_loss Sobreajuste en un batch accuracy = train ( model , inputs = batches [ 0 ]) assert accuracy == pytest . approx ( 0.95 , abs = 0.05 ) # 0.95 \u00b1 0.05 Entrenar hasta la finalizaci\u00f3n (pruebas de parada anticipada, guardado, etc.) train ( model ) assert learning_rate >= min_learning_rate assert artifacts En diferentes dispositivos assert train ( model , device = torch . device ( \"cpu\" )) assert train ( model , device = torch . device ( \"cuda\" )) Puede marcar las pruebas de c\u00e1lculo intensivo con un marcador pytest y s\u00f3lo ejecutarlas cuando se produzca un cambio en el sistema que afecte al modelo. @pytest . mark . training def test_train_model (): ... Pruebas de comportamiento Las pruebas de comportamiento (behavioral testing) son el proceso de probar los datos de entrada y los resultados esperados mientras se trata el modelo como una caja negra. Un art\u00edculo de referencia sobre este tema es Beyond Accuracy: Behavioral Testing of NLP Models with CheckList , que desglosa las pruebas de comportamiento en tres tipos de pruebas: invarianza : Los cambios no deben afectar a los resultados. # INVariance a trav\u00e9s de inyecci\u00f3n de verbos (los cambios no deber\u00edan afectar los resultados). tokens = [ \"revolutionized\" , \"disrupted\" ] texts = [ f \"Transformers applied to NLP have { token } the ML field.\" for token in tokens ] predict . predict ( texts = texts , artifacts = artifacts ) direccional : El cambio debe afectar a las salidas. # Expectativas DIReccionales (cambios con resultados conocidos). tokens = [ \"text classification\" , \"image classification\" ] texts = [ f \"ML applied to { token } .\" for token in tokens ] predict . predict ( texts = texts , artifacts = artifacts ) funcionalidad m\u00ednima : Combinaci\u00f3n simple de entradas y salidas previstas. # Pruebas de Funcionalidad M\u00ednima (pares simples de entrada/salida). tokens = [ \"natural language processing\" , \"mlops\" ] texts = [ f \" { token } is the next big wave in machine learning.\" for token in tokens ] predict . predict ( texts = texts , artifacts = artifacts ) Y podemos convertir estas pruebas en pruebas sistem\u00e1ticas parametrizadas: mkdir tests/model touch tests/model/test_behavioral.py Ver tests/model/test_behavioral.py # tests/model/test_behavioral.py from pathlib import Path import pytest from config import config from coe_template import main , predict @pytest . fixture ( scope = \"module\" ) def artifacts (): run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () artifacts = main . load_artifacts ( run_id = run_id ) return artifacts @pytest . mark . parametrize ( \"text, tag\" , [ ( \"Transformers applied to NLP have revolutionized machine learning.\" , \"natural-language-processing\" , ), ( \"Transformers applied to NLP have disrupted machine learning.\" , \"natural-language-processing\" , ), ], ) def test_inv ( text , tag , artifacts ): \"\"\"INVariance a trav\u00e9s de inyecci\u00f3n de verbos (los cambios no deber\u00edan afectar los resultados).\"\"\" predicted_tag = predict . predict ( texts = [ text ], artifacts = artifacts )[ 0 ][ \"predicted_tag\" ] assert tag == predicted_tag @pytest . mark . parametrize ( \"text, tag\" , [ ( \"ML applied to text classification.\" , \"natural-language-processing\" , ), ( \"ML applied to image classification.\" , \"computer-vision\" , ), ( \"CNNs for text classification.\" , \"natural-language-processing\" , ) ], ) def test_dir ( text , tag , artifacts ): \"\"\"Expectativas DIReccionales (cambios con resultados conocidos).\"\"\" predicted_tag = predict . predict ( texts = [ text ], artifacts = artifacts )[ 0 ][ \"predicted_tag\" ] assert tag == predicted_tag @pytest . mark . parametrize ( \"text, tag\" , [ ( \"Natural language processing is the next big wave in machine learning.\" , \"natural-language-processing\" , ), ( \"MLOps is the next big wave in machine learning.\" , \"mlops\" , ), ( \"This should not produce any relevant topics.\" , \"other\" , ), ], ) def test_mft ( text , tag , artifacts ): \"\"\"Pruebas de Funcionalidad M\u00ednima (pares simples de entrada/salida).\"\"\" predicted_tag = predict . predict ( texts = [ text ], artifacts = artifacts )[ 0 ][ \"predicted_tag\" ] assert tag == predicted_tag Inferencia Cuando nuestro modelo se despliegue, los usuarios lo utilizar\u00e1n para la inferencia (directa / indirectamente), por lo que es muy importante que probemos todos sus aspectos. Carga de artefactos Esta es la primera vez que no cargamos nuestros componentes desde la memoria, por lo que queremos asegurarnos de que los artefactos necesarios (model weights, encoders, config, etc.) son capaces de ser cargados. artifacts = main . load_artifacts ( run_id = run_id ) assert isinstance ( artifacts [ \"label_encoder\" ], data . LabelEncoder ) ... Predicci\u00f3n Una vez que tenemos nuestros artefactos cargados, estamos listos para probar el pipeline de predicci\u00f3n. Deber\u00edamos probar las muestras con una sola entrada, as\u00ed como con un lote de entradas. # probar nuestra llamada API directamente data = { \"texts\" : [ { \"text\" : \"Transfer learning with transformers for text classification.\" }, { \"text\" : \"Generative adversarial networks in both PyTorch and TensorFlow.\" }, ] } response = client . post ( \"/predict\" , json = data ) assert response . status_code == HTTPStatus . OK assert response . request . method == \"POST\" assert len ( response . json ()[ \"data\" ][ \"predictions\" ]) == len ( data [ \"texts\" ]) ...","title":"Testing de Modelos"},{"location":"Pruebas/Modelos/#testing-de-modelos","text":"El \u00faltimo aspecto de las pruebas de los sistemas de ML consiste en probar nuestros modelos durante el entrenamiento, la evaluaci\u00f3n, la inferencia y el despliegue.","title":"Testing de Modelos"},{"location":"Pruebas/Modelos/#entrenamiento","text":"Queremos escribir pruebas de forma iterativa mientras desarrollamos nuestros pipelines de entrenamiento para poder detectar errores r\u00e1pidamente. Esto es especialmente importante porque, a diferencia del software tradicional, los sistemas de ML pueden ejecutarse hasta el final sin lanzar ninguna excepci\u00f3n/error, pero pueden producir sistemas incorrectos. Adem\u00e1s, queremos detectar los errores r\u00e1pidamente para ahorrar tiempo y computo. Comprobar las formas y los valores de los resultados del modelo assert model ( inputs ) . shape == torch . Size ([ len ( inputs ), num_classes ]) Comprobar la disminuci\u00f3n de la p\u00e9rdida despu\u00e9s de una tanda de entrenamiento assert epoch_loss < prev_epoch_loss Sobreajuste en un batch accuracy = train ( model , inputs = batches [ 0 ]) assert accuracy == pytest . approx ( 0.95 , abs = 0.05 ) # 0.95 \u00b1 0.05 Entrenar hasta la finalizaci\u00f3n (pruebas de parada anticipada, guardado, etc.) train ( model ) assert learning_rate >= min_learning_rate assert artifacts En diferentes dispositivos assert train ( model , device = torch . device ( \"cpu\" )) assert train ( model , device = torch . device ( \"cuda\" )) Puede marcar las pruebas de c\u00e1lculo intensivo con un marcador pytest y s\u00f3lo ejecutarlas cuando se produzca un cambio en el sistema que afecte al modelo. @pytest . mark . training def test_train_model (): ...","title":"Entrenamiento"},{"location":"Pruebas/Modelos/#pruebas-de-comportamiento","text":"Las pruebas de comportamiento (behavioral testing) son el proceso de probar los datos de entrada y los resultados esperados mientras se trata el modelo como una caja negra. Un art\u00edculo de referencia sobre este tema es Beyond Accuracy: Behavioral Testing of NLP Models with CheckList , que desglosa las pruebas de comportamiento en tres tipos de pruebas: invarianza : Los cambios no deben afectar a los resultados. # INVariance a trav\u00e9s de inyecci\u00f3n de verbos (los cambios no deber\u00edan afectar los resultados). tokens = [ \"revolutionized\" , \"disrupted\" ] texts = [ f \"Transformers applied to NLP have { token } the ML field.\" for token in tokens ] predict . predict ( texts = texts , artifacts = artifacts ) direccional : El cambio debe afectar a las salidas. # Expectativas DIReccionales (cambios con resultados conocidos). tokens = [ \"text classification\" , \"image classification\" ] texts = [ f \"ML applied to { token } .\" for token in tokens ] predict . predict ( texts = texts , artifacts = artifacts ) funcionalidad m\u00ednima : Combinaci\u00f3n simple de entradas y salidas previstas. # Pruebas de Funcionalidad M\u00ednima (pares simples de entrada/salida). tokens = [ \"natural language processing\" , \"mlops\" ] texts = [ f \" { token } is the next big wave in machine learning.\" for token in tokens ] predict . predict ( texts = texts , artifacts = artifacts ) Y podemos convertir estas pruebas en pruebas sistem\u00e1ticas parametrizadas: mkdir tests/model touch tests/model/test_behavioral.py Ver tests/model/test_behavioral.py # tests/model/test_behavioral.py from pathlib import Path import pytest from config import config from coe_template import main , predict @pytest . fixture ( scope = \"module\" ) def artifacts (): run_id = open ( Path ( config . CONFIG_DIR , \"run_id.txt\" )) . read () artifacts = main . load_artifacts ( run_id = run_id ) return artifacts @pytest . mark . parametrize ( \"text, tag\" , [ ( \"Transformers applied to NLP have revolutionized machine learning.\" , \"natural-language-processing\" , ), ( \"Transformers applied to NLP have disrupted machine learning.\" , \"natural-language-processing\" , ), ], ) def test_inv ( text , tag , artifacts ): \"\"\"INVariance a trav\u00e9s de inyecci\u00f3n de verbos (los cambios no deber\u00edan afectar los resultados).\"\"\" predicted_tag = predict . predict ( texts = [ text ], artifacts = artifacts )[ 0 ][ \"predicted_tag\" ] assert tag == predicted_tag @pytest . mark . parametrize ( \"text, tag\" , [ ( \"ML applied to text classification.\" , \"natural-language-processing\" , ), ( \"ML applied to image classification.\" , \"computer-vision\" , ), ( \"CNNs for text classification.\" , \"natural-language-processing\" , ) ], ) def test_dir ( text , tag , artifacts ): \"\"\"Expectativas DIReccionales (cambios con resultados conocidos).\"\"\" predicted_tag = predict . predict ( texts = [ text ], artifacts = artifacts )[ 0 ][ \"predicted_tag\" ] assert tag == predicted_tag @pytest . mark . parametrize ( \"text, tag\" , [ ( \"Natural language processing is the next big wave in machine learning.\" , \"natural-language-processing\" , ), ( \"MLOps is the next big wave in machine learning.\" , \"mlops\" , ), ( \"This should not produce any relevant topics.\" , \"other\" , ), ], ) def test_mft ( text , tag , artifacts ): \"\"\"Pruebas de Funcionalidad M\u00ednima (pares simples de entrada/salida).\"\"\" predicted_tag = predict . predict ( texts = [ text ], artifacts = artifacts )[ 0 ][ \"predicted_tag\" ] assert tag == predicted_tag","title":"Pruebas de comportamiento"},{"location":"Pruebas/Modelos/#inferencia","text":"Cuando nuestro modelo se despliegue, los usuarios lo utilizar\u00e1n para la inferencia (directa / indirectamente), por lo que es muy importante que probemos todos sus aspectos. Carga de artefactos Esta es la primera vez que no cargamos nuestros componentes desde la memoria, por lo que queremos asegurarnos de que los artefactos necesarios (model weights, encoders, config, etc.) son capaces de ser cargados. artifacts = main . load_artifacts ( run_id = run_id ) assert isinstance ( artifacts [ \"label_encoder\" ], data . LabelEncoder ) ... Predicci\u00f3n Una vez que tenemos nuestros artefactos cargados, estamos listos para probar el pipeline de predicci\u00f3n. Deber\u00edamos probar las muestras con una sola entrada, as\u00ed como con un lote de entradas. # probar nuestra llamada API directamente data = { \"texts\" : [ { \"text\" : \"Transfer learning with transformers for text classification.\" }, { \"text\" : \"Generative adversarial networks in both PyTorch and TensorFlow.\" }, ] } response = client . post ( \"/predict\" , json = data ) assert response . status_code == HTTPStatus . OK assert response . request . method == \"POST\" assert len ( response . json ()[ \"data\" ][ \"predictions\" ]) == len ( data [ \"texts\" ]) ...","title":"Inferencia"},{"location":"Pruebas/Testing/","text":"Pruebas de sistemas de Machine Learning: C\u00f3digo, datos y modelos Aprenda a probar modelos de ML (y su c\u00f3digo y datos) para garantizar un comportamiento coherente. Introducci\u00f3n En esta secci\u00f3n veremos como probar el c\u00f3digo, los datos y los modelos para construir un sistema de ML con el que podamos iterar de forma fiable. Las pruebas son una forma de asegurarnos de que algo funciona seg\u00fan lo previsto. Las pruebas son muy \u00fatiles para descubrir las fuentes de error tan pronto como sea posible para que podamos reducir los costos posteriores y el tiempo perdido. Una vez que hemos dise\u00f1ado nuestras pruebas, podemos ejecutarlas autom\u00e1ticamente cada vez que cambiamos o a\u00f1adimos algo a nuestra base de c\u00f3digo. Tipos de pruebas Hay varios tipos de pruebas que se utilizan en diferentes puntos del ciclo de desarrollo, entre los que se encuentran: Pruebas unitarias: pruebas sobre componentes individuales que tienen una \u00fanica responsabilidad. Pruebas de integraci\u00f3n: pruebas sobre la funcionalidad combinada de los componentes individuales. Pruebas de sistema: pruebas sobre el dise\u00f1o de un sistema para los resultados esperados en funci\u00f3n de las entradas. Pruebas de aceptaci\u00f3n: pruebas para verificar el cumplimiento de los requisitos, normalmente denominadas pruebas de aceptaci\u00f3n del usuario (UAT). Pruebas de regresi\u00f3n: pruebas basadas en errores que hemos visto antes para garantizar que los nuevos cambios no los reintroduzcan. La distinci\u00f3n entre las pruebas de los sistemas de ML con los sistemas tradicionales comienza cuando pasamos de probar el c\u00f3digo a probar los datos y los modelos. Tambi\u00e9n hay muchos otros tipos de pruebas funcionales y no funcionales, como las pruebas de humo, las pruebas de rendimiento, las pruebas de seguridad, etc., pero podemos generalizar todas ellas bajo las pruebas del sistema. \u00bfC\u00f3mo debemos hacer las pruebas? El framework que hay que utilizar para componer las pruebas es la metodolog\u00eda Arrange Act Assert. Arrange: establecer las diferentes entradas que se van a probar. Act: aplicar las entradas en el componente que queremos probar. Assert: confirmar que recibimos la salida esperada. En Python, hay muchas herramientas, como unittest, pytest, etc. que nos permiten implementar f\u00e1cilmente nuestras pruebas mientras se adhieren al framework de Arrange Act Assert. Estas herramientas vienen con una potente funcionalidad incorporada, como la parametrizaci\u00f3n, los filtros, y m\u00e1s, para probar muchas condiciones a escala. \u00bfQu\u00e9 debemos testear? Algunos de los aspectos que deber\u00edamos probar son: inputs: tipos de datos, formato, longitud, casos l\u00edmite. outputs: tipos de datos, formatos, excepciones, salidas intermedias y finales. Mejores pr\u00e1cticas Independientemente del framework que utilicemos, es importante vincular fuertemente las pruebas al proceso de desarrollo. atomicidad : al crear funciones y clases, debemos asegurarnos de que tienen una \u00fanica responsabilidad para poder probarlas f\u00e1cilmente. Si no es as\u00ed, tendremos que dividirlas en componentes m\u00e1s granulares. composici\u00f3n : cuando creamos nuevos componentes, queremos componer pruebas para validar su funcionalidad. Es una buena manera de garantizar la fiabilidad y detectar errores desde el principio. reutilizaci\u00f3n : debemos mantener repositorios centrales en los que se pruebe la funcionalidad principal en el origen y se reutilice en muchos proyectos. Esto reduce significativamente los esfuerzos de prueba para la base de c\u00f3digo de cada nuevo proyecto. regresi\u00f3n : queremos tener en cuenta los nuevos errores que encontremos con una prueba de regresi\u00f3n para asegurarnos de no reintroducir los mismos errores en el futuro. cobertura : queremos garantizar una cobertura lo m\u00e1s cercana al 100% de nuestra base de c\u00f3digo. automatizaci\u00f3n : queremos autoejecutar las pruebas cuando hagamos cambios en nuestra base de c\u00f3digo. C\u00f3digo Data Modelos Makefile Vamos a crear un target en nuestro Makefile que nos permitir\u00e1 ejecutar todas nuestras pruebas con una sola llamada: # Test .PHONY: test test: pytest -m \"not training\" cd tests && great_expectations checkpoint run projects cd tests && great_expectations checkpoint run tags cd tests && great_expectations checkpoint run labeled_projects make test Testing vs. monitoreo Ambas son partes integrales del proceso de desarrollo de ML y dependen la una de la otra para la iteraci\u00f3n. Las pruebas consisten en asegurar que nuestro sistema (c\u00f3digo, datos y modelos) supera las expectativas que hemos establecido. Por su parte, la monitorizaci\u00f3n implica que estas expectativas sigan pasando en l\u00ednea en los datos de producci\u00f3n en vivo, al tiempo que se garantiza que sus distribuciones de datos son comparables a la ventana de referencia. Cuando estas condiciones dejan de cumplirse, hay que inspeccionar m\u00e1s de cerca (el reentrenamiento no siempre soluciona el problema de fondo). En el caso del monitoreo, hay bastantes preocupaciones distintas que no tuvimos que considerar durante las pruebas, ya que se trata de datos que a\u00fan no hemos visto. Features y distribuciones de predicci\u00f3n (drift), tipificaci\u00f3n, desajustes de esquema, etc. Determinar el rendimiento del modelo (m\u00e9tricas rolling y window, en general y en slices) utilizando se\u00f1ales indirectas (ya que las etiquetas pueden no estar disponibles f\u00e1cilmente). En situaciones con datos de gran tama\u00f1o, necesitamos saber qu\u00e9 puntos de datos hay que etiquetar y sobremuestrear para el entrenamiento. Identificar anomal\u00edas y valores at\u00edpicos. Cubriremos todos estos conceptos en la secci\u00f3n de monitorizaci\u00f3n.","title":"Pruebas de sistemas de Machine Learning: C\u00f3digo, datos y modelos"},{"location":"Pruebas/Testing/#pruebas-de-sistemas-de-machine-learning-codigo-datos-y-modelos","text":"Aprenda a probar modelos de ML (y su c\u00f3digo y datos) para garantizar un comportamiento coherente.","title":"Pruebas de sistemas de Machine Learning: C\u00f3digo, datos y modelos"},{"location":"Pruebas/Testing/#introduccion","text":"En esta secci\u00f3n veremos como probar el c\u00f3digo, los datos y los modelos para construir un sistema de ML con el que podamos iterar de forma fiable. Las pruebas son una forma de asegurarnos de que algo funciona seg\u00fan lo previsto. Las pruebas son muy \u00fatiles para descubrir las fuentes de error tan pronto como sea posible para que podamos reducir los costos posteriores y el tiempo perdido. Una vez que hemos dise\u00f1ado nuestras pruebas, podemos ejecutarlas autom\u00e1ticamente cada vez que cambiamos o a\u00f1adimos algo a nuestra base de c\u00f3digo.","title":"Introducci\u00f3n"},{"location":"Pruebas/Testing/#tipos-de-pruebas","text":"Hay varios tipos de pruebas que se utilizan en diferentes puntos del ciclo de desarrollo, entre los que se encuentran: Pruebas unitarias: pruebas sobre componentes individuales que tienen una \u00fanica responsabilidad. Pruebas de integraci\u00f3n: pruebas sobre la funcionalidad combinada de los componentes individuales. Pruebas de sistema: pruebas sobre el dise\u00f1o de un sistema para los resultados esperados en funci\u00f3n de las entradas. Pruebas de aceptaci\u00f3n: pruebas para verificar el cumplimiento de los requisitos, normalmente denominadas pruebas de aceptaci\u00f3n del usuario (UAT). Pruebas de regresi\u00f3n: pruebas basadas en errores que hemos visto antes para garantizar que los nuevos cambios no los reintroduzcan. La distinci\u00f3n entre las pruebas de los sistemas de ML con los sistemas tradicionales comienza cuando pasamos de probar el c\u00f3digo a probar los datos y los modelos. Tambi\u00e9n hay muchos otros tipos de pruebas funcionales y no funcionales, como las pruebas de humo, las pruebas de rendimiento, las pruebas de seguridad, etc., pero podemos generalizar todas ellas bajo las pruebas del sistema.","title":"Tipos de pruebas"},{"location":"Pruebas/Testing/#como-debemos-hacer-las-pruebas","text":"El framework que hay que utilizar para componer las pruebas es la metodolog\u00eda Arrange Act Assert. Arrange: establecer las diferentes entradas que se van a probar. Act: aplicar las entradas en el componente que queremos probar. Assert: confirmar que recibimos la salida esperada. En Python, hay muchas herramientas, como unittest, pytest, etc. que nos permiten implementar f\u00e1cilmente nuestras pruebas mientras se adhieren al framework de Arrange Act Assert. Estas herramientas vienen con una potente funcionalidad incorporada, como la parametrizaci\u00f3n, los filtros, y m\u00e1s, para probar muchas condiciones a escala.","title":"\u00bfC\u00f3mo debemos hacer las pruebas?"},{"location":"Pruebas/Testing/#que-debemos-testear","text":"Algunos de los aspectos que deber\u00edamos probar son: inputs: tipos de datos, formato, longitud, casos l\u00edmite. outputs: tipos de datos, formatos, excepciones, salidas intermedias y finales.","title":"\u00bfQu\u00e9 debemos testear?"},{"location":"Pruebas/Testing/#mejores-practicas","text":"Independientemente del framework que utilicemos, es importante vincular fuertemente las pruebas al proceso de desarrollo. atomicidad : al crear funciones y clases, debemos asegurarnos de que tienen una \u00fanica responsabilidad para poder probarlas f\u00e1cilmente. Si no es as\u00ed, tendremos que dividirlas en componentes m\u00e1s granulares. composici\u00f3n : cuando creamos nuevos componentes, queremos componer pruebas para validar su funcionalidad. Es una buena manera de garantizar la fiabilidad y detectar errores desde el principio. reutilizaci\u00f3n : debemos mantener repositorios centrales en los que se pruebe la funcionalidad principal en el origen y se reutilice en muchos proyectos. Esto reduce significativamente los esfuerzos de prueba para la base de c\u00f3digo de cada nuevo proyecto. regresi\u00f3n : queremos tener en cuenta los nuevos errores que encontremos con una prueba de regresi\u00f3n para asegurarnos de no reintroducir los mismos errores en el futuro. cobertura : queremos garantizar una cobertura lo m\u00e1s cercana al 100% de nuestra base de c\u00f3digo. automatizaci\u00f3n : queremos autoejecutar las pruebas cuando hagamos cambios en nuestra base de c\u00f3digo.","title":"Mejores pr\u00e1cticas"},{"location":"Pruebas/Testing/#codigo","text":"","title":"C\u00f3digo"},{"location":"Pruebas/Testing/#data","text":"","title":"Data"},{"location":"Pruebas/Testing/#modelos","text":"","title":"Modelos"},{"location":"Pruebas/Testing/#makefile","text":"Vamos a crear un target en nuestro Makefile que nos permitir\u00e1 ejecutar todas nuestras pruebas con una sola llamada: # Test .PHONY: test test: pytest -m \"not training\" cd tests && great_expectations checkpoint run projects cd tests && great_expectations checkpoint run tags cd tests && great_expectations checkpoint run labeled_projects make test","title":"Makefile"},{"location":"Pruebas/Testing/#testing-vs-monitoreo","text":"Ambas son partes integrales del proceso de desarrollo de ML y dependen la una de la otra para la iteraci\u00f3n. Las pruebas consisten en asegurar que nuestro sistema (c\u00f3digo, datos y modelos) supera las expectativas que hemos establecido. Por su parte, la monitorizaci\u00f3n implica que estas expectativas sigan pasando en l\u00ednea en los datos de producci\u00f3n en vivo, al tiempo que se garantiza que sus distribuciones de datos son comparables a la ventana de referencia. Cuando estas condiciones dejan de cumplirse, hay que inspeccionar m\u00e1s de cerca (el reentrenamiento no siempre soluciona el problema de fondo). En el caso del monitoreo, hay bastantes preocupaciones distintas que no tuvimos que considerar durante las pruebas, ya que se trata de datos que a\u00fan no hemos visto. Features y distribuciones de predicci\u00f3n (drift), tipificaci\u00f3n, desajustes de esquema, etc. Determinar el rendimiento del modelo (m\u00e9tricas rolling y window, en general y en slices) utilizando se\u00f1ales indirectas (ya que las etiquetas pueden no estar disponibles f\u00e1cilmente). En situaciones con datos de gran tama\u00f1o, necesitamos saber qu\u00e9 puntos de datos hay que etiquetar y sobremuestrear para el entrenamiento. Identificar anomal\u00edas y valores at\u00edpicos. Cubriremos todos estos conceptos en la secci\u00f3n de monitorizaci\u00f3n.","title":"Testing vs. monitoreo"},{"location":"Reproducibilidad/Docker/","text":"Contenedorizaci\u00f3n Empaquetar nuestra aplicaci\u00f3n en contenedores reproducibles y escalables. Introducci\u00f3n El \u00faltimo paso para lograr la reproducibilidad es desplegar nuestro c\u00f3digo versionado y los artefactos en un entorno reproducible. Esto va mucho m\u00e1s all\u00e1 del entorno virtual que configuramos para nuestras aplicaciones Python porque hay especificaciones a nivel de sistema (sistema operativo, paquetes impl\u00edcitos necesarios, etc.) que no estamos capturando. Queremos ser capaces de encapsular todos los requisitos que necesitamos para que no haya dependencias externas que impidan a otra persona reproducir nuestra aplicaci\u00f3n exacta. Docker En soluciones para la reproducibilidad a nivel de sistema el motor de contenedores Docker es el m\u00e1s popular por varias ventajas clave: reproducibilidad v\u00eda Dockerfile con instrucciones expl\u00edcitas para desplegar nuestra aplicaci\u00f3n en un sistema espec\u00edfico. aislamiento a trav\u00e9s de los contenedores para no afectar a otras aplicaciones que tambi\u00e9n puedan ejecutarse en el mismo sistema operativo subyacente. y muchas m\u00e1s ventajas como el tama\u00f1o, la velocidad, Docker Hub, etc. Vamos a utilizar Docker para desplegar nuestra aplicaci\u00f3n localmente de forma aislada, reproducible y escalable. Una vez que hagamos esto, cualquier m\u00e1quina con el motor Docker instalado podr\u00e1 reproducir nuestro trabajo. Arquitectura El motor de contenedores Docker se encarga de poner en marcha los contenedores configurados, que contienen nuestra aplicaci\u00f3n y sus dependencias (binarios, librer\u00edas, etc.). El motor de contenedores es muy eficiente, ya que no necesita crear un sistema operativo distinto para cada aplicaci\u00f3n en contenedor. Esto tambi\u00e9n significa que nuestros contenedores pueden compartir los recursos del sistema a trav\u00e9s del motor Docker. Instalaci\u00f3n Una vez instalado , podemos iniciar el Docker Desktop que nos permitir\u00e1 crear y desplegar nuestras aplicaciones en contenedores. docker --version Im\u00e1genes El primer paso es construir una imagen Docker que tenga la aplicaci\u00f3n y todas sus dependencias especificadas. Podemos crear esta imagen utilizando un Dockerfile que describe un conjunto de instrucciones. Estas instrucciones esencialmente construyen capas de imagen de s\u00f3lo lectura en la parte superior de cada uno para construir nuestra imagen completa. Echemos un vistazo al Dockerfile de nuestra aplicaci\u00f3n y a las capas de imagen que crea. Dockerfile Empezaremos creando un Dockerfile: touch Dockerfile La primera l\u00ednea que escribiremos en nuestro Dockerfile especifica la imagen base de la que queremos extraer. Queremos usar la imagen base para ejecutar aplicaciones basadas en Python versi\u00f3n 3.9 con la variante slim. Esta variante slim con paquetes m\u00ednimos satisface nuestros requisitos y mantiene el tama\u00f1o de la capa de la imagen bajo. # Base image FROM python:3.9-slim A continuaci\u00f3n vamos a instalar las dependencias de nuestra aplicaci\u00f3n. En primer lugar, vamos a copiar los archivos necesarios de nuestro sistema de archivos local. Una vez que tenemos nuestros archivos, podemos instalar los paquetes necesarios para instalar las dependencias de nuestra aplicaci\u00f3n utilizando el comando RUN. Una vez que hayamos terminado de usar los paquetes, podemos eliminarlos para mantener el tama\u00f1o de nuestra capa de imagen al m\u00ednimo. # Instalar dependencias WORKDIR /mlops COPY setup.py setup.py COPY requirements.txt requirements.txt RUN apt-get update \\ && apt-get install -y --no-install-recommends gcc build-essential \\ && rm -rf /var/lib/apt/lists/* \\ && python3 -m pip install --upgrade pip setuptools wheel \\ && python3 -m pip install -e . --no-cache-dir \\ && python3 -m pip install protobuf == 3 .20.1 --no-cache-dir \\ && apt-get purge -y --auto-remove gcc build-essential A continuaci\u00f3n, estamos listos para copiar los archivos necesarios para ejecutar nuestra aplicaci\u00f3n. # Copy COPY coe_template coe_template COPY app app COPY data data COPY config config COPY stores stores # Extraer activos de S3 RUN dvc init --no-scm RUN dvc remote add -d storage stores/blob RUN dvc pull Como nuestra aplicaci\u00f3n (API) requiere que el puerto 8000 est\u00e9 abierto, necesitamos especificar en nuestro Dockerfile que lo exponga. # Exponer puertos EXPOSE 8000 El \u00faltimo paso es especificar el ejecutable que se ejecutar\u00e1 cuando se construya un contenedor a partir de nuestra imagen. Para nuestra aplicaci\u00f3n, queremos lanzar nuestra API con gunicorn. # Iniciar aplicaci\u00f3n ENTRYPOINT [ \"gunicorn\" , \"-c\" , \"app/gunicorn.py\" , \"-k\" , \"uvicorn.workers.UvicornWorker\" , \"app.api:app\" ] Compilar im\u00e1genes Una vez que hemos terminado de componer el Dockerfile, estamos listos para construir nuestra imagen utilizando el comando build que nos permite a\u00f1adir una etiqueta y especificar la ubicaci\u00f3n del Dockerfile a utilizar. docker build -t coe_template:latest -f Dockerfile . Podemos inspeccionar todas las im\u00e1genes construidas y sus atributos: docker images Tambi\u00e9n podemos eliminar alguna o todas las im\u00e1genes en funci\u00f3n de su identificaci\u00f3n \u00fanica. docker rmi <IMAGE_ID> # eliminar una imagen docker rmi $( docker images -a -q ) # eliminar todas las im\u00e1genes Ejecutar contenedores Una vez que hemos construido nuestra imagen, estamos listos para ejecutar un contenedor usando esa imagen con el comando run que nos permite especificar la imagen, el reenv\u00edo de puertos, etc. docker run -p 8000 :8000 --name coe_template coe_template:latest Una vez que tenemos nuestro contenedor en funcionamiento, podemos utilizar la API gracias al puerto que estamos compartiendo (8000): curl -X 'POST' \\ 'http://localhost:8000/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"texts\": [ { \"text\": \"Transfer learning with transformers for text classification.\" } ] }' Podemos inspeccionar todos los contenedores as\u00ed: docker ps # contenedores en funcionamiento docker ps -a # contenedores detenidos Tambi\u00e9n podemos detener y eliminar cualquiera o todos los contenedores en funci\u00f3n de sus identificadores \u00fanicos: docker stop <CONTAINER_ID> # detener un contenedor en ejecuci\u00f3n docker rm <CONTAINER_ID> # eliminar un contenedor docker stop $( docker ps -a -q ) # detener todos los contenedores docker rm $( docker ps -a -q ) # eliminar todos los contenedores Depurar En el caso de que nos encontremos con errores mientras construimos nuestras capas de imagen, una forma muy f\u00e1cil de depurar el problema es ejecutar el contenedor con las capas de imagen que se han construido hasta el momento. Podemos hacer esto incluyendo s\u00f3lo los comandos que se han ejecutado con \u00e9xito hasta ahora en el Dockerfile. Y luego tenemos que reconstruir la imagen (ya que alteramos el Dockerfile) y ejecutar el contenedor: docker build -t coe_template:latest -f Dockerfile . docker run -p 8000 :8000 -it coe_template /bin/bash Una vez que tenemos nuestro contenedor funcionando, podemos utilizar nuestra aplicaci\u00f3n como lo har\u00edamos en nuestra m\u00e1quina local, pero ahora es reproducible en cualquier sistema operativo. Producci\u00f3n Este Dockerfile es com\u00fanmente el artefacto final para desplegar y escalar sus servicios, con algunos cambios: los activos de datos se extraer\u00edan de una ubicaci\u00f3n de almacenamiento remota. los artefactos del modelo se cargar\u00edan desde un registro de modelos remoto. el c\u00f3digo se cargar\u00eda desde un repositorio remoto. Todos estos cambios implicar\u00edan el uso de las credenciales adecuadas (a trav\u00e9s de secretos encriptados e incluso pueden desplegarse autom\u00e1ticamente a trav\u00e9s de flujos de trabajo CI/CD). Pero, por supuesto, hay responsabilidades posteriores como el monitoreo.","title":"Contenedorizaci\u00f3n"},{"location":"Reproducibilidad/Docker/#contenedorizacion","text":"Empaquetar nuestra aplicaci\u00f3n en contenedores reproducibles y escalables.","title":"Contenedorizaci\u00f3n"},{"location":"Reproducibilidad/Docker/#introduccion","text":"El \u00faltimo paso para lograr la reproducibilidad es desplegar nuestro c\u00f3digo versionado y los artefactos en un entorno reproducible. Esto va mucho m\u00e1s all\u00e1 del entorno virtual que configuramos para nuestras aplicaciones Python porque hay especificaciones a nivel de sistema (sistema operativo, paquetes impl\u00edcitos necesarios, etc.) que no estamos capturando. Queremos ser capaces de encapsular todos los requisitos que necesitamos para que no haya dependencias externas que impidan a otra persona reproducir nuestra aplicaci\u00f3n exacta.","title":"Introducci\u00f3n"},{"location":"Reproducibilidad/Docker/#docker","text":"En soluciones para la reproducibilidad a nivel de sistema el motor de contenedores Docker es el m\u00e1s popular por varias ventajas clave: reproducibilidad v\u00eda Dockerfile con instrucciones expl\u00edcitas para desplegar nuestra aplicaci\u00f3n en un sistema espec\u00edfico. aislamiento a trav\u00e9s de los contenedores para no afectar a otras aplicaciones que tambi\u00e9n puedan ejecutarse en el mismo sistema operativo subyacente. y muchas m\u00e1s ventajas como el tama\u00f1o, la velocidad, Docker Hub, etc. Vamos a utilizar Docker para desplegar nuestra aplicaci\u00f3n localmente de forma aislada, reproducible y escalable. Una vez que hagamos esto, cualquier m\u00e1quina con el motor Docker instalado podr\u00e1 reproducir nuestro trabajo.","title":"Docker"},{"location":"Reproducibilidad/Docker/#arquitectura","text":"El motor de contenedores Docker se encarga de poner en marcha los contenedores configurados, que contienen nuestra aplicaci\u00f3n y sus dependencias (binarios, librer\u00edas, etc.). El motor de contenedores es muy eficiente, ya que no necesita crear un sistema operativo distinto para cada aplicaci\u00f3n en contenedor. Esto tambi\u00e9n significa que nuestros contenedores pueden compartir los recursos del sistema a trav\u00e9s del motor Docker.","title":"Arquitectura"},{"location":"Reproducibilidad/Docker/#instalacion","text":"Una vez instalado , podemos iniciar el Docker Desktop que nos permitir\u00e1 crear y desplegar nuestras aplicaciones en contenedores. docker --version","title":"Instalaci\u00f3n"},{"location":"Reproducibilidad/Docker/#imagenes","text":"El primer paso es construir una imagen Docker que tenga la aplicaci\u00f3n y todas sus dependencias especificadas. Podemos crear esta imagen utilizando un Dockerfile que describe un conjunto de instrucciones. Estas instrucciones esencialmente construyen capas de imagen de s\u00f3lo lectura en la parte superior de cada uno para construir nuestra imagen completa. Echemos un vistazo al Dockerfile de nuestra aplicaci\u00f3n y a las capas de imagen que crea.","title":"Im\u00e1genes"},{"location":"Reproducibilidad/Docker/#dockerfile","text":"Empezaremos creando un Dockerfile: touch Dockerfile La primera l\u00ednea que escribiremos en nuestro Dockerfile especifica la imagen base de la que queremos extraer. Queremos usar la imagen base para ejecutar aplicaciones basadas en Python versi\u00f3n 3.9 con la variante slim. Esta variante slim con paquetes m\u00ednimos satisface nuestros requisitos y mantiene el tama\u00f1o de la capa de la imagen bajo. # Base image FROM python:3.9-slim A continuaci\u00f3n vamos a instalar las dependencias de nuestra aplicaci\u00f3n. En primer lugar, vamos a copiar los archivos necesarios de nuestro sistema de archivos local. Una vez que tenemos nuestros archivos, podemos instalar los paquetes necesarios para instalar las dependencias de nuestra aplicaci\u00f3n utilizando el comando RUN. Una vez que hayamos terminado de usar los paquetes, podemos eliminarlos para mantener el tama\u00f1o de nuestra capa de imagen al m\u00ednimo. # Instalar dependencias WORKDIR /mlops COPY setup.py setup.py COPY requirements.txt requirements.txt RUN apt-get update \\ && apt-get install -y --no-install-recommends gcc build-essential \\ && rm -rf /var/lib/apt/lists/* \\ && python3 -m pip install --upgrade pip setuptools wheel \\ && python3 -m pip install -e . --no-cache-dir \\ && python3 -m pip install protobuf == 3 .20.1 --no-cache-dir \\ && apt-get purge -y --auto-remove gcc build-essential A continuaci\u00f3n, estamos listos para copiar los archivos necesarios para ejecutar nuestra aplicaci\u00f3n. # Copy COPY coe_template coe_template COPY app app COPY data data COPY config config COPY stores stores # Extraer activos de S3 RUN dvc init --no-scm RUN dvc remote add -d storage stores/blob RUN dvc pull Como nuestra aplicaci\u00f3n (API) requiere que el puerto 8000 est\u00e9 abierto, necesitamos especificar en nuestro Dockerfile que lo exponga. # Exponer puertos EXPOSE 8000 El \u00faltimo paso es especificar el ejecutable que se ejecutar\u00e1 cuando se construya un contenedor a partir de nuestra imagen. Para nuestra aplicaci\u00f3n, queremos lanzar nuestra API con gunicorn. # Iniciar aplicaci\u00f3n ENTRYPOINT [ \"gunicorn\" , \"-c\" , \"app/gunicorn.py\" , \"-k\" , \"uvicorn.workers.UvicornWorker\" , \"app.api:app\" ]","title":"Dockerfile"},{"location":"Reproducibilidad/Docker/#compilar-imagenes","text":"Una vez que hemos terminado de componer el Dockerfile, estamos listos para construir nuestra imagen utilizando el comando build que nos permite a\u00f1adir una etiqueta y especificar la ubicaci\u00f3n del Dockerfile a utilizar. docker build -t coe_template:latest -f Dockerfile . Podemos inspeccionar todas las im\u00e1genes construidas y sus atributos: docker images Tambi\u00e9n podemos eliminar alguna o todas las im\u00e1genes en funci\u00f3n de su identificaci\u00f3n \u00fanica. docker rmi <IMAGE_ID> # eliminar una imagen docker rmi $( docker images -a -q ) # eliminar todas las im\u00e1genes","title":"Compilar im\u00e1genes"},{"location":"Reproducibilidad/Docker/#ejecutar-contenedores","text":"Una vez que hemos construido nuestra imagen, estamos listos para ejecutar un contenedor usando esa imagen con el comando run que nos permite especificar la imagen, el reenv\u00edo de puertos, etc. docker run -p 8000 :8000 --name coe_template coe_template:latest Una vez que tenemos nuestro contenedor en funcionamiento, podemos utilizar la API gracias al puerto que estamos compartiendo (8000): curl -X 'POST' \\ 'http://localhost:8000/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"texts\": [ { \"text\": \"Transfer learning with transformers for text classification.\" } ] }' Podemos inspeccionar todos los contenedores as\u00ed: docker ps # contenedores en funcionamiento docker ps -a # contenedores detenidos Tambi\u00e9n podemos detener y eliminar cualquiera o todos los contenedores en funci\u00f3n de sus identificadores \u00fanicos: docker stop <CONTAINER_ID> # detener un contenedor en ejecuci\u00f3n docker rm <CONTAINER_ID> # eliminar un contenedor docker stop $( docker ps -a -q ) # detener todos los contenedores docker rm $( docker ps -a -q ) # eliminar todos los contenedores","title":"Ejecutar contenedores"},{"location":"Reproducibilidad/Docker/#depurar","text":"En el caso de que nos encontremos con errores mientras construimos nuestras capas de imagen, una forma muy f\u00e1cil de depurar el problema es ejecutar el contenedor con las capas de imagen que se han construido hasta el momento. Podemos hacer esto incluyendo s\u00f3lo los comandos que se han ejecutado con \u00e9xito hasta ahora en el Dockerfile. Y luego tenemos que reconstruir la imagen (ya que alteramos el Dockerfile) y ejecutar el contenedor: docker build -t coe_template:latest -f Dockerfile . docker run -p 8000 :8000 -it coe_template /bin/bash Una vez que tenemos nuestro contenedor funcionando, podemos utilizar nuestra aplicaci\u00f3n como lo har\u00edamos en nuestra m\u00e1quina local, pero ahora es reproducible en cualquier sistema operativo.","title":"Depurar"},{"location":"Reproducibilidad/Docker/#produccion","text":"Este Dockerfile es com\u00fanmente el artefacto final para desplegar y escalar sus servicios, con algunos cambios: los activos de datos se extraer\u00edan de una ubicaci\u00f3n de almacenamiento remota. los artefactos del modelo se cargar\u00edan desde un registro de modelos remoto. el c\u00f3digo se cargar\u00eda desde un repositorio remoto. Todos estos cambios implicar\u00edan el uso de las credenciales adecuadas (a trav\u00e9s de secretos encriptados e incluso pueden desplegarse autom\u00e1ticamente a trav\u00e9s de flujos de trabajo CI/CD). Pero, por supuesto, hay responsabilidades posteriores como el monitoreo.","title":"Producci\u00f3n"},{"location":"Reproducibilidad/Git/","text":"","title":"Git"},{"location":"Reproducibilidad/Pre-commit/","text":"Pre-commit Usando los hooks pre-commit de git para asegurar las comprobaciones antes del commit. Introducci\u00f3n Antes de realizar un commit en nuestro repositorio local, hay muchos elementos en nuestra lista de tareas, que van desde el estilo, el formato, las pruebas, etc. Y es muy f\u00e1cil olvidarse de algunos de estos pasos. Para ayudarnos a gestionar todos estos pasos importantes, podemos utilizar hooks de pre-commit, que se activar\u00e1n autom\u00e1ticamente cuando intentemos realizar un commit. Instalaci\u00f3n Utilizaremos el framework Pre-commit para ayudarnos a realizar autom\u00e1ticamente comprobaciones importantes mediante hooks cuando hagamos un commit. # Instalar pre-commit python -m pip install pre-commit == 2 .19.0 pre-commit install Y agregaremos esto a nuestro script setup.py en lugar de nuestro archivo requirements.txt porque no es fundamental para las operaciones de aprendizaje autom\u00e1tico. # setup.py setup ( ... extras_require = { \"dev\" : docs_packages + style_packages + test_packages + [ \"pre-commit==2.19.0\" ], \"docs\" : docs_packages , \"test\" : test_packages , }, ) Configuraci\u00f3n Definimos nuestros hooks de pre-commit a trav\u00e9s de un archivo de configuraci\u00f3n .pre-commit-config.yaml . Podemos crear nuestra configuraci\u00f3n yaml desde cero o utilizar la CLI de pre-commit para crear una configuraci\u00f3n de ejemplo a la que podemos a\u00f1adir. # Simple config pre-commit sample-config > .pre-commit-config.yaml cat .pre-commit-config.yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.2.0 hooks : - id : trailing-whitespace - id : end-of-file-fixer - id : check-yaml - id : check-added-large-files Hooks A la hora de crear y utilizar hooks, tenemos varias opciones para elegir: Integrado (Built-in) Dentro de la configuraci\u00f3n de ejemplo, podemos ver que pre-commit ha a\u00f1adido algunos hooks por defecto de su repositorio. Especifica la ubicaci\u00f3n del repositorio, la versi\u00f3n, as\u00ed como los identificadores espec\u00edficos de los hooks a utilizar. # .pre-commit-config.yaml ... - id : check-added-large-files args : [ '--maxkb=1000' ] exclude : \"notebooks\" ... Personalizado Tambi\u00e9n hay muchos hooks personalizados y populares de terceros entre los que podemos elegir. Por ejemplo, si queremos aplicar comprobaciones de formato con Black, podemos aprovechar el hook de precommit de Black. # .pre-commit-config.yaml ... - repo : https://github.com/psf/black rev : 22.6.0 hooks : - id : black args : [] files : . ... Local Tambi\u00e9n podemos crear nuestros propios hooks locales sin configurar un .pre-commit-hooks.yaml separado. Aqu\u00ed estamos definiendo dos hooks pre-commit, test-non-training y clean, para ejecutar algunos comandos que hemos definido en nuestro Makefile. Del mismo modo, podemos ejecutar cualquier comando de entrada con argumentos para crear hooks muy r\u00e1pidamente. # .pre-commit-config.yaml ... - repo : local hooks : - id : test name : test entry : make args : [ \"test\" ] language : system pass_filenames : false - id : clean name : clean entry : make args : [ \"clean\" ] language : system pass_filenames : false Creemos nuestro archivo de configuaci\u00f3n de pre-commits: touch .pre-commit-config.yaml Ver nuestro .pre-commit-config.yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v4.3.0 hooks : - id : trailing-whitespace - id : end-of-file-fixer exclude : \"config/run_id.txt\" - id : check-yaml exclude : \"mkdocs.yml\" - id : check-added-large-files args : [ '--maxkb=1000' ] exclude : \"notebooks\" - id : check-ast - id : check-json - id : check-merge-conflict - id : detect-private-key - repo : https://github.com/psf/black rev : 22.6.0 hooks : - id : black args : [] files : . - repo : https://github.com/pycqa/flake8 rev : 3.9.2 hooks : - id : flake8 - repo : https://github.com/PyCQA/isort rev : 5.10.1 hooks : - id : isort args : [] files : . - repo : https://github.com/asottile/pyupgrade # update python syntax rev : v2.37.3 hooks : - id : pyupgrade args : [ --py36-plus ] - repo : local hooks : - id : test name : test entry : make args : [ \"test\" ] language : system pass_filenames : false - id : clean name : clean entry : make args : [ \"clean\" ] language : system pass_filenames : false Si est\u00e1 usando Windows tiene que realizar un paso adicional, sino d\u00e1ra error Great Expectations al ejecutar por consola, por los problemas con los caracteres especiales. Dentro del virtual enviroment debe ir a la librer\u00eda great_expectations , luego a la carpeta cli . Aqu\u00ed debe abrir checkpoint.py y reemplazar la l\u00ednea 305 por esto: cli_message ( str ( status_line . encode ( 'utf-8' ))) Commit Nuestros hooks de pre-commit se ejecutar\u00e1n autom\u00e1ticamente cuando intentemos hacer un commit. Podremos ver si cada hook pas\u00f3 o fall\u00f3 y hacer cualquier cambio. Si alguno de los hooks ha fallado, tendremos que arreglar el archivo correspondiente o, en muchos casos, el reformateo se producir\u00e1 autom\u00e1ticamente. En el caso de que alguno de los hooks haya fallado, tenemos que a\u00f1adir y comitear de nuevo para asegurarnos de que todos los hooks son aprobados. git add . git commit -m <MENSAJE> Ejecutar Aunque los hooks de precommit est\u00e1n pensados para ejecutarse antes de un commit, podemos activar manualmente todos o algunos hooks en todos o en un conjunto de archivos. # Run pre-commit run --all-files # ejecutar todos los hooks en todos los archivos pre-commit run <HOOK_ID> --all-files # ejecutar un hook en todos los archivos pre-commit run --files <PATH_TO_FILE> # ejecutar todos los hooks en un archivo pre-commit run <HOOK_ID> --files <PATH_TO_FILE> # ejecutar un hook en un archivo Skip No es recomendable omitir la ejecuci\u00f3n de ninguno de los hooks porque est\u00e1n ah\u00ed por una raz\u00f3n. Pero para algunos commits muy urgentes y que salvan el mundo, podemos usar el flag no-verify. # Commit without hooks git commit -m <MENSAJE> --no-verify Actualizar En nuestro archivo de configuraci\u00f3n .pre-commit-config.yaml , hemos tenido que especificar las versiones de cada uno de los repositorios para poder utilizar sus \u00faltimos hooks. Pre-commit tiene un comando CLI de auto-actualizaci\u00f3n que actualizar\u00e1 estas versiones a medida que est\u00e9n disponibles. # Autoupdate pre-commit autoupdate Tambi\u00e9n podemos a\u00f1adir este comando a nuestro Makefile para que se ejecute cuando se cree un entorno de desarrollo para que todo est\u00e9 actualizado. # Makefile .ONESHELL : venv : python3 -m venv venv source venv/bin/activate && \\ python3 -m pip install --upgrade pip setuptools wheel && \\ python3 -m pip install -e \".[dev]\" && \\ pre-commit install && \\ pre-commit autoupdate","title":"Pre-commit"},{"location":"Reproducibilidad/Pre-commit/#pre-commit","text":"Usando los hooks pre-commit de git para asegurar las comprobaciones antes del commit.","title":"Pre-commit"},{"location":"Reproducibilidad/Pre-commit/#introduccion","text":"Antes de realizar un commit en nuestro repositorio local, hay muchos elementos en nuestra lista de tareas, que van desde el estilo, el formato, las pruebas, etc. Y es muy f\u00e1cil olvidarse de algunos de estos pasos. Para ayudarnos a gestionar todos estos pasos importantes, podemos utilizar hooks de pre-commit, que se activar\u00e1n autom\u00e1ticamente cuando intentemos realizar un commit.","title":"Introducci\u00f3n"},{"location":"Reproducibilidad/Pre-commit/#instalacion","text":"Utilizaremos el framework Pre-commit para ayudarnos a realizar autom\u00e1ticamente comprobaciones importantes mediante hooks cuando hagamos un commit. # Instalar pre-commit python -m pip install pre-commit == 2 .19.0 pre-commit install Y agregaremos esto a nuestro script setup.py en lugar de nuestro archivo requirements.txt porque no es fundamental para las operaciones de aprendizaje autom\u00e1tico. # setup.py setup ( ... extras_require = { \"dev\" : docs_packages + style_packages + test_packages + [ \"pre-commit==2.19.0\" ], \"docs\" : docs_packages , \"test\" : test_packages , }, )","title":"Instalaci\u00f3n"},{"location":"Reproducibilidad/Pre-commit/#configuracion","text":"Definimos nuestros hooks de pre-commit a trav\u00e9s de un archivo de configuraci\u00f3n .pre-commit-config.yaml . Podemos crear nuestra configuraci\u00f3n yaml desde cero o utilizar la CLI de pre-commit para crear una configuraci\u00f3n de ejemplo a la que podemos a\u00f1adir. # Simple config pre-commit sample-config > .pre-commit-config.yaml cat .pre-commit-config.yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.2.0 hooks : - id : trailing-whitespace - id : end-of-file-fixer - id : check-yaml - id : check-added-large-files","title":"Configuraci\u00f3n"},{"location":"Reproducibilidad/Pre-commit/#hooks","text":"A la hora de crear y utilizar hooks, tenemos varias opciones para elegir:","title":"Hooks"},{"location":"Reproducibilidad/Pre-commit/#integrado-built-in","text":"Dentro de la configuraci\u00f3n de ejemplo, podemos ver que pre-commit ha a\u00f1adido algunos hooks por defecto de su repositorio. Especifica la ubicaci\u00f3n del repositorio, la versi\u00f3n, as\u00ed como los identificadores espec\u00edficos de los hooks a utilizar. # .pre-commit-config.yaml ... - id : check-added-large-files args : [ '--maxkb=1000' ] exclude : \"notebooks\" ...","title":"Integrado (Built-in)"},{"location":"Reproducibilidad/Pre-commit/#personalizado","text":"Tambi\u00e9n hay muchos hooks personalizados y populares de terceros entre los que podemos elegir. Por ejemplo, si queremos aplicar comprobaciones de formato con Black, podemos aprovechar el hook de precommit de Black. # .pre-commit-config.yaml ... - repo : https://github.com/psf/black rev : 22.6.0 hooks : - id : black args : [] files : . ...","title":"Personalizado"},{"location":"Reproducibilidad/Pre-commit/#local","text":"Tambi\u00e9n podemos crear nuestros propios hooks locales sin configurar un .pre-commit-hooks.yaml separado. Aqu\u00ed estamos definiendo dos hooks pre-commit, test-non-training y clean, para ejecutar algunos comandos que hemos definido en nuestro Makefile. Del mismo modo, podemos ejecutar cualquier comando de entrada con argumentos para crear hooks muy r\u00e1pidamente. # .pre-commit-config.yaml ... - repo : local hooks : - id : test name : test entry : make args : [ \"test\" ] language : system pass_filenames : false - id : clean name : clean entry : make args : [ \"clean\" ] language : system pass_filenames : false Creemos nuestro archivo de configuaci\u00f3n de pre-commits: touch .pre-commit-config.yaml Ver nuestro .pre-commit-config.yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v4.3.0 hooks : - id : trailing-whitespace - id : end-of-file-fixer exclude : \"config/run_id.txt\" - id : check-yaml exclude : \"mkdocs.yml\" - id : check-added-large-files args : [ '--maxkb=1000' ] exclude : \"notebooks\" - id : check-ast - id : check-json - id : check-merge-conflict - id : detect-private-key - repo : https://github.com/psf/black rev : 22.6.0 hooks : - id : black args : [] files : . - repo : https://github.com/pycqa/flake8 rev : 3.9.2 hooks : - id : flake8 - repo : https://github.com/PyCQA/isort rev : 5.10.1 hooks : - id : isort args : [] files : . - repo : https://github.com/asottile/pyupgrade # update python syntax rev : v2.37.3 hooks : - id : pyupgrade args : [ --py36-plus ] - repo : local hooks : - id : test name : test entry : make args : [ \"test\" ] language : system pass_filenames : false - id : clean name : clean entry : make args : [ \"clean\" ] language : system pass_filenames : false Si est\u00e1 usando Windows tiene que realizar un paso adicional, sino d\u00e1ra error Great Expectations al ejecutar por consola, por los problemas con los caracteres especiales. Dentro del virtual enviroment debe ir a la librer\u00eda great_expectations , luego a la carpeta cli . Aqu\u00ed debe abrir checkpoint.py y reemplazar la l\u00ednea 305 por esto: cli_message ( str ( status_line . encode ( 'utf-8' )))","title":"Local"},{"location":"Reproducibilidad/Pre-commit/#commit","text":"Nuestros hooks de pre-commit se ejecutar\u00e1n autom\u00e1ticamente cuando intentemos hacer un commit. Podremos ver si cada hook pas\u00f3 o fall\u00f3 y hacer cualquier cambio. Si alguno de los hooks ha fallado, tendremos que arreglar el archivo correspondiente o, en muchos casos, el reformateo se producir\u00e1 autom\u00e1ticamente. En el caso de que alguno de los hooks haya fallado, tenemos que a\u00f1adir y comitear de nuevo para asegurarnos de que todos los hooks son aprobados. git add . git commit -m <MENSAJE>","title":"Commit"},{"location":"Reproducibilidad/Pre-commit/#ejecutar","text":"Aunque los hooks de precommit est\u00e1n pensados para ejecutarse antes de un commit, podemos activar manualmente todos o algunos hooks en todos o en un conjunto de archivos. # Run pre-commit run --all-files # ejecutar todos los hooks en todos los archivos pre-commit run <HOOK_ID> --all-files # ejecutar un hook en todos los archivos pre-commit run --files <PATH_TO_FILE> # ejecutar todos los hooks en un archivo pre-commit run <HOOK_ID> --files <PATH_TO_FILE> # ejecutar un hook en un archivo","title":"Ejecutar"},{"location":"Reproducibilidad/Pre-commit/#skip","text":"No es recomendable omitir la ejecuci\u00f3n de ninguno de los hooks porque est\u00e1n ah\u00ed por una raz\u00f3n. Pero para algunos commits muy urgentes y que salvan el mundo, podemos usar el flag no-verify. # Commit without hooks git commit -m <MENSAJE> --no-verify","title":"Skip"},{"location":"Reproducibilidad/Pre-commit/#actualizar","text":"En nuestro archivo de configuraci\u00f3n .pre-commit-config.yaml , hemos tenido que especificar las versiones de cada uno de los repositorios para poder utilizar sus \u00faltimos hooks. Pre-commit tiene un comando CLI de auto-actualizaci\u00f3n que actualizar\u00e1 estas versiones a medida que est\u00e9n disponibles. # Autoupdate pre-commit autoupdate Tambi\u00e9n podemos a\u00f1adir este comando a nuestro Makefile para que se ejecute cuando se cree un entorno de desarrollo para que todo est\u00e9 actualizado. # Makefile .ONESHELL : venv : python3 -m venv venv source venv/bin/activate && \\ python3 -m pip install --upgrade pip setuptools wheel && \\ python3 -m pip install -e \".[dev]\" && \\ pre-commit install && \\ pre-commit autoupdate","title":"Actualizar"},{"location":"Reproducibilidad/Versionado/","text":"Versionado de c\u00f3digo, datos y modelos Versionar el c\u00f3digo, los datos y los modelos para garantizar un comportamiento reproducible en los sistemas de ML. Introducci\u00f3n Adem\u00e1s del c\u00f3digo, hay otra clase muy importante de artefactos que necesitamos rastrear y versionar: config, datos y modelos. Es importante que versionemos todo para poder reproducir exactamente la misma aplicaci\u00f3n en cualquier momento. Y vamos a hacer esto mediante el uso de un commit de Git como una instant\u00e1nea del c\u00f3digo, la configuraci\u00f3n, los datos y el modelo utilizados para producir un modelo espec\u00edfico. Estos son los elementos clave que tendremos que incorporar para que nuestra aplicaci\u00f3n sea totalmente reproducible: El repositorio debe almacenar punteros a grandes datos y artefactos del modelo que viven en blob storage. Utilizar commits para almacenar instant\u00e1neas del c\u00f3digo, la configuraci\u00f3n, los datos y el modelo y poder actualizar y revertir las versiones. Exponer las configuraciones para poder ver y comparar los par\u00e1metros. Aplicaci\u00f3n Utilizaremos Data Version Control (DVC) para versionar nuestros conjuntos de datos y ponderaciones del modelo y almacenarlos en un directorio local que actuar\u00e1 como nuestro almacenamiento de blobs. Podr\u00edamos utilizar opciones de almacenamiento de blob remotas como S3, GCP, Azure Blob Storage, Google Drive, DAGsHub, etc., pero vamos a replicar las mismas acciones localmente para poder ver c\u00f3mo se almacenan los datos. Instalaci\u00f3n Empecemos por instalar DVC e inicializarlo para crear un directorio .dvc. # Initialization python -m pip install dvc == 2 .10.2 dvc init Aseg\u00farese de a\u00f1adir este paquete y la versi\u00f3n a nuestro archivo requirements.txt . Almacenamiento remoto Despu\u00e9s de inicializar DVC, podemos establecer d\u00f3nde estar\u00e1 nuestro almacenamiento remoto. Crearemos y utilizaremos el directorio stores/blob como nuestro almacenamiento remoto, pero en un entorno de producci\u00f3n ser\u00eda algo como un cloud storage. Definiremos nuestro almac\u00e9n blob en nuestro archivo config/config.py : # Inside config/config.py BLOB_STORE = Path ( STORES_DIR , \"blob\" ) BLOB_STORE . mkdir ( parents = True , exist_ok = True ) Ejecutaremos el script de configuraci\u00f3n para que se cree este almacenamiento: python config/config.py Necesitamos notificar a DVC sobre esta ubicaci\u00f3n de almacenamiento para que sepa d\u00f3nde guardar los activos de datos: dvc remote add -d storage stores/blob Si queremos utilizar una opci\u00f3n de almacenamiento remoto, as\u00ed es como configurar\u00edamos un bucket de S3 para guardar nuestros datos versionados: # Create bucket: https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html # Add credentials: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html dvc remote modify storage access_key_id ${ AWS_ACCESS_KEY_ID } dvc remote modify storage secret_access_key ${ AWS_SECRET_ACCESS_KEY } dvc remote add -d storage s3://<BUCKET_NAME> A\u00f1adir datos Ahora estamos listos para a\u00f1adir nuestros datos al almacenamiento remoto. Esto a\u00f1adir\u00e1 autom\u00e1ticamente los respectivos activos de datos a un archivo .gitignore (se crear\u00e1 uno nuevo dentro del directorio de datos) y crear\u00e1 archivos de punteros que apuntar\u00e1n a donde los activos de datos est\u00e1n realmente almacenados (nuestro almacenamiento remoto). Pero primero, tenemos que eliminar el directorio de datos de nuestro archivo .gitignore. # Dentro de .gitignore logs/ stores/ # data/ # eliminar o comentar esta l\u00ednea y ahora estamos listos para a\u00f1adir nuestros activos de datos: # Agregar artefactos dvc add data/projects.csv dvc add data/tags.csv dvc add data/labeled_projects.csv Ahora deber\u00edamos ver el archivo data/.gitignore creado autom\u00e1ticamente: # data/.gitignore /projects.csv /tags.csv /labeled_projects.csv y todos los archivos de punteros que se crearon para cada artefacto de datos que a\u00f1adimos. Cada archivo puntero contendr\u00e1 el hash md5, el tama\u00f1o y la ubicaci\u00f3n que comprobaremos en nuestro repositorio git. Push Ahora estamos listos para enviar nuestros artefactos a nuestro almac\u00e9n de blobs: dvc push Si inspeccionamos nuestro almacenamiento (stores/blob), veremos que los datos se almacenan de forma eficiente. En caso de que nos olvidemos de a\u00f1adir nuestros artefactos, podemos a\u00f1adirlo como un pre-commit para que ocurra autom\u00e1ticamente cuando intentemos hacer un commit. Si no hay cambios en nuestros archivos versionados, no pasar\u00e1 nada. # Makefile .PHONY : dvc dvc : dvc add data/projects.csv dvc add data/tags.csv dvc add data/labeled_projects.csv dvc push # .pre-commit-config.yaml - repo : local hooks : - id : dvc name : dvc entry : make args : [ \"dvc\" ] language : system pass_filenames : false Pull Cuando alguien quiere extraer nuestros activos de datos, podemos utilizar el comando pull para obtener de nuestro almacenamiento remoto a nuestros directorios locales. Todo lo que necesitamos es asegurarnos primero de que tenemos los \u00faltimos archivos de puntero (a trav\u00e9s de git pull) y luego hacer pull del almacenamiento remoto. dvc pull","title":"Versionado de c\u00f3digo, datos y modelos"},{"location":"Reproducibilidad/Versionado/#versionado-de-codigo-datos-y-modelos","text":"Versionar el c\u00f3digo, los datos y los modelos para garantizar un comportamiento reproducible en los sistemas de ML.","title":"Versionado de c\u00f3digo, datos y modelos"},{"location":"Reproducibilidad/Versionado/#introduccion","text":"Adem\u00e1s del c\u00f3digo, hay otra clase muy importante de artefactos que necesitamos rastrear y versionar: config, datos y modelos. Es importante que versionemos todo para poder reproducir exactamente la misma aplicaci\u00f3n en cualquier momento. Y vamos a hacer esto mediante el uso de un commit de Git como una instant\u00e1nea del c\u00f3digo, la configuraci\u00f3n, los datos y el modelo utilizados para producir un modelo espec\u00edfico. Estos son los elementos clave que tendremos que incorporar para que nuestra aplicaci\u00f3n sea totalmente reproducible: El repositorio debe almacenar punteros a grandes datos y artefactos del modelo que viven en blob storage. Utilizar commits para almacenar instant\u00e1neas del c\u00f3digo, la configuraci\u00f3n, los datos y el modelo y poder actualizar y revertir las versiones. Exponer las configuraciones para poder ver y comparar los par\u00e1metros.","title":"Introducci\u00f3n"},{"location":"Reproducibilidad/Versionado/#aplicacion","text":"Utilizaremos Data Version Control (DVC) para versionar nuestros conjuntos de datos y ponderaciones del modelo y almacenarlos en un directorio local que actuar\u00e1 como nuestro almacenamiento de blobs. Podr\u00edamos utilizar opciones de almacenamiento de blob remotas como S3, GCP, Azure Blob Storage, Google Drive, DAGsHub, etc., pero vamos a replicar las mismas acciones localmente para poder ver c\u00f3mo se almacenan los datos.","title":"Aplicaci\u00f3n"},{"location":"Reproducibilidad/Versionado/#instalacion","text":"Empecemos por instalar DVC e inicializarlo para crear un directorio .dvc. # Initialization python -m pip install dvc == 2 .10.2 dvc init Aseg\u00farese de a\u00f1adir este paquete y la versi\u00f3n a nuestro archivo requirements.txt .","title":"Instalaci\u00f3n"},{"location":"Reproducibilidad/Versionado/#almacenamiento-remoto","text":"Despu\u00e9s de inicializar DVC, podemos establecer d\u00f3nde estar\u00e1 nuestro almacenamiento remoto. Crearemos y utilizaremos el directorio stores/blob como nuestro almacenamiento remoto, pero en un entorno de producci\u00f3n ser\u00eda algo como un cloud storage. Definiremos nuestro almac\u00e9n blob en nuestro archivo config/config.py : # Inside config/config.py BLOB_STORE = Path ( STORES_DIR , \"blob\" ) BLOB_STORE . mkdir ( parents = True , exist_ok = True ) Ejecutaremos el script de configuraci\u00f3n para que se cree este almacenamiento: python config/config.py Necesitamos notificar a DVC sobre esta ubicaci\u00f3n de almacenamiento para que sepa d\u00f3nde guardar los activos de datos: dvc remote add -d storage stores/blob Si queremos utilizar una opci\u00f3n de almacenamiento remoto, as\u00ed es como configurar\u00edamos un bucket de S3 para guardar nuestros datos versionados: # Create bucket: https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html # Add credentials: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html dvc remote modify storage access_key_id ${ AWS_ACCESS_KEY_ID } dvc remote modify storage secret_access_key ${ AWS_SECRET_ACCESS_KEY } dvc remote add -d storage s3://<BUCKET_NAME>","title":"Almacenamiento remoto"},{"location":"Reproducibilidad/Versionado/#anadir-datos","text":"Ahora estamos listos para a\u00f1adir nuestros datos al almacenamiento remoto. Esto a\u00f1adir\u00e1 autom\u00e1ticamente los respectivos activos de datos a un archivo .gitignore (se crear\u00e1 uno nuevo dentro del directorio de datos) y crear\u00e1 archivos de punteros que apuntar\u00e1n a donde los activos de datos est\u00e1n realmente almacenados (nuestro almacenamiento remoto). Pero primero, tenemos que eliminar el directorio de datos de nuestro archivo .gitignore. # Dentro de .gitignore logs/ stores/ # data/ # eliminar o comentar esta l\u00ednea y ahora estamos listos para a\u00f1adir nuestros activos de datos: # Agregar artefactos dvc add data/projects.csv dvc add data/tags.csv dvc add data/labeled_projects.csv Ahora deber\u00edamos ver el archivo data/.gitignore creado autom\u00e1ticamente: # data/.gitignore /projects.csv /tags.csv /labeled_projects.csv y todos los archivos de punteros que se crearon para cada artefacto de datos que a\u00f1adimos. Cada archivo puntero contendr\u00e1 el hash md5, el tama\u00f1o y la ubicaci\u00f3n que comprobaremos en nuestro repositorio git.","title":"A\u00f1adir datos"},{"location":"Reproducibilidad/Versionado/#push","text":"Ahora estamos listos para enviar nuestros artefactos a nuestro almac\u00e9n de blobs: dvc push Si inspeccionamos nuestro almacenamiento (stores/blob), veremos que los datos se almacenan de forma eficiente. En caso de que nos olvidemos de a\u00f1adir nuestros artefactos, podemos a\u00f1adirlo como un pre-commit para que ocurra autom\u00e1ticamente cuando intentemos hacer un commit. Si no hay cambios en nuestros archivos versionados, no pasar\u00e1 nada. # Makefile .PHONY : dvc dvc : dvc add data/projects.csv dvc add data/tags.csv dvc add data/labeled_projects.csv dvc push # .pre-commit-config.yaml - repo : local hooks : - id : dvc name : dvc entry : make args : [ \"dvc\" ] language : system pass_filenames : false","title":"Push"},{"location":"Reproducibilidad/Versionado/#pull","text":"Cuando alguien quiere extraer nuestros activos de datos, podemos utilizar el comando pull para obtener de nuestro almacenamiento remoto a nuestros directorios locales. Todo lo que necesitamos es asegurarnos primero de que tenemos los \u00faltimos archivos de puntero (a trav\u00e9s de git pull) y luego hacer pull del almacenamiento remoto. dvc pull","title":"Pull"},{"location":"Seguridad/Seguridad/","text":"","title":"Seguridad"}]}